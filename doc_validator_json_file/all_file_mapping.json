{"Documentation Site": " title: Documentation Site slug: /readme  This documentation site is built using Docusaurus 2, a modern static website generator. System Requirements https://docusaurus.io/docs/installation#requirements Installation Follow the CONTRIBUTING_CODE guide in the great_expectations repository to install dev dependencies. Then run the following command from the repository root to install the rest of the dependencies and build documentation locally (including prior versions) and start a development server: console invoke docs Once you've run invoke docs once, you can run invoke docs --start to start the development server without copying and building prior versions. Note that if prior versions change your build won't include those changes until you run invoke docs --clean and invoke docs again. console invoke docs --start To remove versioned code, docs and sidebars run: console invoke docs --clean Linting standard.js is used to lint the project. Please run the linter before committing changes. console invoke docs --lint Build To build a static version of the site, this command generates static content into the build directory. This can be served using any static hosting service. console invoke docs --build Deployment Deployment is handled via Netlify. Other relevant files The following are a few details about other files Docusaurus uses that you may wish to be familiar with.  sidebars.js: JavaScript that specifies the sidebar/navigation used in docs pages src: non-docs pages live here static: static assets used in docs pages (such as CSS) live here docusaurus.config.js: the configuration file for Docusaurus babel.config.js: Babel config file used when building package.json: dependencies and scripts yarn.lock: dependency lock file that ensures reproducibility  sitemap.xml is not in the repo since it is built and uploaded by a netlify plugin during the documentation build process.  Documentation changes checklist  For any pages you have moved or removed, update _redirects to point from the old to the new content location  Versioning To add a new version, follow these steps: (Note: yarn commands should be run from docs/docusaurus/)  It may help to start with a fresh virtualenv and clone of gx. Check out the version from the tag, e.g. git checkout 0.15.50 Make sure dev dependencies are installed pip install -c constraints-dev.txt -e \".[test]\" Install API docs dependencies pip install -r docs/sphinx_api_docs_source/requirements-dev-api-docs.txt Build API docs invoke api-docs from the repo root. Run yarn install from docs/docusaurus/. Temporarily change onBrokenLinks: 'throw' to onBrokenLinks: 'warn' in docusaurus.config.js to allow the build to complete even if there are broken links. Run yarn build from docs/docusaurus/. Create the version e.g. yarn docusaurus docs:version 0.15.50 from docs/docusaurus/. Pull down the version file (see docs/build_docs for the file, currently https://superconductive-public.s3.us-east-2.amazonaws.com/oss_docs_versions.zip) Unzip and add your newly created versioned docs via the following: Copy the version you built in step 4 from inside versioned_docs in your repo to the versioned_docs from the unzipped version file. Copy the version you built in step 4 from inside versioned_sidebars in your repo to the versioned_sidebars from the unzipped version file. Add your version number to versions.json in the unzipped version file. Zip up versioned_docs, versioned_sidebars and versions.json and upload to the s3 bucket (see docs/build_docs for the bucket name). Make sure versioned_docs, versioned_sidebars and versions.json are at the top level of the zip file (not nested in a folder). Once the docs are built again, this zip file will be used for the prior versions. ", "Changelog": " title: Changelog Deprecation policy  Deprecation warnings and the supporting code are maintained for two minor versions.  For example, v0.12 deprecations will only be removed as part of a v0.15 release. This means we have three supported minor versions in the release at any time.  For example: in v0.15 we support v0.15, v0.14, and v0.13.  When v0.16 ships we will support v0.16, v0.15, and v0.14 and will remove support for v0.13. Deprecation warnings include (in the warning message) the version that they were introduced.  For example: \"deprecated as of v0.13\" Deprecation warnings are accompanied by a moniker (as a code comment) indicating when they were deprecated.  For example: # deprecated-v0.13 Changes to methods and parameters due to deprecation are also noted in the relevant docstrings.  0.17.9  [BUGFIX] PR title checker's code should handle apostrophes (#8513) [BUGFIX] Patch add_or_update_expectation_suite with Cloud-backed contexts (#8522) [DOCS] Update Code Example for Viewing a Full Checkpoint Configuration (#8492) [DOCS] Add New Topic for Connecting to In-Memory Source Data Using Spark (#8445) [DOCS] Remove Unused Documentation Style Guide Topic (#8496) [DOCS] Update Links and Content in the Data Context Topic (#8489) [DOCS] Update Connect to filesystem source data (#8483) [DOCS] Add step to install azure-storage-blob when using azure storage blobs\u2026 (#8156) [DOCS] add python version badge to README (#7040) [DOCS] standardizes references to supported Python versions (#8474) [MAINTENANCE] Remove core team attributions from changelog (#8493) [MAINTENANCE] Remove azure-pipeline.yml after migrating to Github Actions (#8494) [MAINTENANCE] Notify on build-n-publish failure. (#8495) [MAINTENANCE] Send slack notification on release (#8497) [MAINTENANCE] Removed unused github action. (#8499) [MAINTENANCE] Change default testing level from WARNING to INFO (#8506) [MAINTENANCE] Flaky decorator added for docs-integration tests that access Cloud resources (#8510) [MAINTENANCE] Mob typing: update list of deprecated, never to be typed files. (#8514) [MAINTENANCE] Better testing for FDS quoted identifier logic (#8509) [MAINTENANCE] Remove Click upper bound constraint. (#8515) [MAINTENANCE] Docs Pipeline Clean up (#8468) [MAINTENANCE] Update flaky test (#8527)  0.17.8  [FEATURE] Add Agent action to list table names in SQL Datasources (#8177) [FEATURE] Add Test Draft Config Workflow to Agent (#8410) [FEATURE] add run checkpoint action to agent (#8449) [FEATURE] DataAssistantResult should include exceptions from rules (#8429) [BUGFIX] Cleanup aws_postgres reference environment (#8439) [BUGFIX] Use consistent name between compose and cli command (#8440) [BUGFIX] Scaffolding .gitignore should not require writeable file system (#8362) (thanks @ivanstillfront) [BUGFIX] Remove unused, overwritten pytest marker. (#8441) [BUGFIX] get_validator throws AttributeError: 'CloudDataContext' object has no attribute 'ge_cloud_mode' (#8433) [BUGFIX] PP-282: fixing update_datasource method bug (#8464) [BUGFIX] Add performance marker to pyproject.toml (#8480) [BUGFIX] Fix Postgres, Trino quoted identifier issues (#8442) [DOCS] Correct Typo (#8451) [DOCS] Admonition and Landing Page Updates (#8432) [DOCS] Remove Expectation Implementations by Backend Topic (#8437) [DOCS] Source Data and Datasource Format Update (#8435) [DOCS] Update the API Reference Landing Page (#8472) [DOCS] Fix Grid Autosizing (#8475) [DOCS] Update Use Great Expectations with Amazon Web Services using Athena (#8406) [DOCS] Fixed typos in Execution Engine glossary entry (#8086) [DOCS] add public_api decorator to TableAsset and QueryAsset (#8470) [MAINTENANCE] Default snippet-check to non verbose mode (#8448) [MAINTENANCE] Move test_dependency_versions to github actions (#8427) [MAINTENANCE] Add pytest-xdist and use by default. (#8436) [MAINTENANCE] Add sleep to allow services to come up before we run tests (#8454) [MAINTENANCE] add test services for mssql, mysql, and trino (#8447) [MAINTENANCE] Removed sqlalchemy_version_compatibility from required test markers. (#8456) [MAINTENANCE] Linting for previously excluded cli, expectations and rule_based_profiler tests (#8422) [MAINTENANCE] Run cloud e2e tests. (#8443) [MAINTENANCE] Start testing clickhouse in github actions. (#8452) [MAINTENANCE] Github Actions - Spark tests (#8460) [MAINTENANCE] Remove external_sqldialect marker as test coverage marker. (#8458) [MAINTENANCE] Pytest Github Action - BigQuery and Postgres (#8417) [MAINTENANCE] Run marker-tests after unit-tests and static-analysis (#8465) [MAINTENANCE] Run all_backend tests along with specific service tests. (#8467) [MAINTENANCE] AWS Glue script using Context Manager to catch FutureWarning (#8466) [MAINTENANCE] Remove old required tests which have been replaced. (#8462) [MAINTENANCE] Run all python versions on scheduled run (#8463) [MAINTENANCE] min-versions wait for unit-tests, static-analysis (#8471) [MAINTENANCE] Remove Azure Dev CI steps (#8473) [MAINTENANCE] Enable B009 Bugbear get-attr-with-constant rule (#8434) [MAINTENANCE] Update ListTableNamesAction to use GxAgentEnvVars (#8450) [MAINTENANCE] Add check that each test only has one required marker. (#8477) [MAINTENANCE] Add test case for run_name_template using env_var (#8461) [MAINTENANCE] Delete azure-pipelines-dev.yml (#8476) [MAINTENANCE] Test out releaser github action (#8478) [MAINTENANCE] Add missing marker to pyproject.toml (#8481) [MAINTENANCE] Slack notifications if non-pr ci github action pipeline fails (#8485) [MAINTENANCE] Run additional matrix steps on any non-pull_request event (#8486) [MAINTENANCE] GX-Release Process update to use GitHub Actions (#8484)  0.17.7  [FEATURE] Add Agent support for Missingness Data Assistant (#8336) [FEATURE] Allow a rule to fail within DataAssistant and still run other rules (#8393) [BUGFIX] fixing minor bug in sqlalchemy_execution_engine.py (#8374) (thanks @SaeedFarahani) [BUGFIX] ensure_json_serializable accounts for pydantic.BaseModel (#8431) [BUGFIX] Table.head() follow-up for Python versions (#8426) [DOCS] Update docs on expectation docstrings (#8405) [DOCS] corrects default value for result_format in documentation (#8419) [DOCS] Remove Custom Expectations Overview Topic (#8391) [DOCS] Update contributing code readme with required markers information. (#8414) [DOCS] Fix markdown link in install_gx.md (#8416) (thanks @jmorakuebler) [DOCS] Docs TOC Reorg (#8421) [DOCS] Fix snippet reference in how_to_use_great_expectations_with_sql.md (#8430) (thanks @jmorakuebler) [MAINTENANCE] Experimental column descriptive metrics repository (#8335) [MAINTENANCE] Mark remaining datasource tests. (#8399) [MAINTENANCE] Pytest Mark - Metrics Folder (#8400) [MAINTENANCE] Add marks to missing expectations tests. (#8401) [MAINTENANCE] Add test for marker coverage and remove integration marker. (#8394) [MAINTENANCE] Add ci test for test marker coverage (#8402) [MAINTENANCE] mypy 1.4.1 (#8226) [MAINTENANCE] snowflake regex (#8403) [MAINTENANCE] Resolve misc typing issues in metrics code (#8411) [MAINTENANCE] Bump certifi from 2022.12.7 to 2023.7.22 in /docs_rtd (#8415) [MAINTENANCE] conditional snowflake-connector-python version bump (#8412) [MAINTENANCE] Refactor Table.head() for sqlalchemy (#8234) [MAINTENANCE] List required markers when verify marker test fails. (#8413) [MAINTENANCE] Bump version of autoupdate GH action (#8425) [MAINTENANCE] Bump Ubuntu version in autoupdate GH action (#8428) [MAINTENANCE] Update error message around context.add_checkpoint when neither name nor checkpoint are provided (#8313) [MAINTENANCE] move Azure docs and public API steps to Github Actions (#8408) [MAINTENANCE] Use constants for GX directory and YAML references (#8420) [MAINTENANCE] Clean up Databricks SQL FDS impl and docs (#8424) [CONTRIB] Pyspark Implementation for expect_column_values_to_be_valid_currency_code (#8418) (thanks @calvingdu) [CONTRIB] Adding docs for missingness data assistant (#8379)  0.17.6  [FEATURE] Register MissingnessDataAssistant (#8337) [BUGFIX] Ensure that Fluent Datasources support database table names in lowercase for Oracle, DB2, and Snowflake (#8327) [BUGFIX] Ensure that GX supports database column names in lowercase and mixcase for Oracle, DB2, and Snowflake (#8345) [BUGFIX] Handle ValueError in agent action (#8369) [BUGFIX] Ensure that SQLAlchemy is installed for SQL Data Source and TableAsset Fluent Data Source Module (#8361) [BUGFIX] DataAssistantResult should not error on get_expectation_suite without name (#8370) [BUGFIX] Exclude appropriate SnowflakeDatasource fields when creating an execution engine (#8371) [DOCS] Improvements on how_to_edit_expectationsuite guide (#8096) (thanks @Ismar11) [DOCS] Update Slack Link to Point to Discourse (#7840) [DOCS] Adds a consolidated overview of using GX as a conceptual guide (#8045) [DOCS] Updated the Execution Engine terms page (#8309) [DOCS] Add docs around Snowflake FDS (#8340) [DOCS] Update link in Data Asset Glossary Topic (#8349) [DOCS] updated docs to remove SimpleCheckpoint (#8352) [DOCS] Remove Outdated SQLAlchemy 2.0 Admonitions (#8357) [DOCS] Update Links to Get started with Great Expectations and Databricks (#8350) [DOCS] Add MetricProviders Conceptual Content (#8175) [DOCS] Add Information for Adding Custom Parameters to Custom Expectations (#8172) [DOCS] Remove index.md Files and Add Redirects (#8332) [MAINTENANCE] Remove trace_docs_deps script in CI (#8305) [MAINTENANCE] GH Actions pytest marker test matrix (#8277) [MAINTENANCE] Pytest Mark - Render folder (#8301) [MAINTENANCE] Fix docs pipeline check changes (#8315) [MAINTENANCE] Add test markers to tests/datasource/fluent/XXX/X.py (#8316) [MAINTENANCE] Pytest Mark - Profiling (#8318) [MAINTENANCE] invoke deps task (#8311) [MAINTENANCE] Fix requirements and CONTRIBUTING_CODE doc (#8312) [MAINTENANCE] gitignore for GCP credentials (#8323) [MAINTENANCE] Pytest Mark - DataContext Part 2 (#8317) [MAINTENANCE] Pytest Mark - ExecutionEngine (#8324) [MAINTENANCE] Pytest Mark - Integration and Expectations (#8319) [MAINTENANCE] Minimum version of numpy bumped to 1.20.3 (#8326) [MAINTENANCE] Disable Airflow provider tests in CI due to external test failures (#8342) [MAINTENANCE] Bump minimum version of numpy==1.21.6 for Python 3.9 (#8341) [MAINTENANCE] Update algolia index (#8273) [MAINTENANCE] Pytest Mark - DataConnector (#8338) [MAINTENANCE] Replace many repetitive slow integration tests for ParameterBuilder components with a few fast unit tests (#8302) [MAINTENANCE] Finish marking fluent datasource tests. (#8347) [MAINTENANCE] Bump word-wrap from 1.2.3 to 1.2.4 in /docs/docusaurus (#8346) [MAINTENANCE] Add test markers to datasource/batch_kwarg_generator tests. (#8348) [MAINTENANCE] Re-mark slow tests. (#8356) [MAINTENANCE] Update standard node package (#8339) [MAINTENANCE] Making tests for column names insensitivity for Oracle, DB2, and Snowflake easier to read (#8358) [MAINTENANCE] Add detect-private-key pre-commit hook (#8363) [MAINTENANCE] Pytest Mark - Top-level test/ (#8359) [MAINTENANCE] Unpin upper bound on Click dependency (#8360) [MAINTENANCE] Revert Click pin removal (#8367) [MAINTENANCE] Bump pygments from 2.7.4 to 2.15.0 in /docs_rtd (#8368) [MAINTENANCE] Fix non-AssertErrors for Expectations during build-gallery process (#8353) [MAINTENANCE] Mark datasource top level tests (#8365) [MAINTENANCE] add invoke ci-tests and  invoke service (#8322) [CONTRIB] Create custom expectation for Chi Square Test (#8314) [CONTRIB] Contributed expect_column_to_have_no_months_missing (#8307) (thanks @HadasManor) [CONTRIB] Add KS test custom expectation (#8344)  0.17.5  [FEATURE] Airflow reference environment (#8257) [FEATURE] Missingness DataAssistant initial implementation (#8268) [FEATURE] DatabricksSQL FDS (#8184) [FEATURE]  single batch missingness data assistant (#8278) [DOCS] Replaces outdated integration guides with redirects to versioned documentation (#8173) [MAINTENANCE] Remove unreferenced docs integration tests (#8228) [MAINTENANCE] Remove duplicate custom checks in PR pipeline (#8265) [MAINTENANCE] Use updated search api key (#8269) [MAINTENANCE] DataAssistantDispatcher should not reach into DataAssistant (#8262) [MAINTENANCE] GH action static analysis and unittest setup (#8272) [MAINTENANCE] remove algolia index action (#8264) [MAINTENANCE] Bump semver from 5.7.1 to 5.7.2 (#8274) [MAINTENANCE] Improvement to contributor documentation (#8043) (thanks @christian-bromann) [MAINTENANCE] Implement unit tests for BatchFilter.parse_batch_slice logic (#8280) [MAINTENANCE] Run GH Action CI job every 3 hours (#8281) [MAINTENANCE] Ensure all tests in tests/checkpoint are marked (#8282) [MAINTENANCE] Pytest Mark - Validator and utils (#8275) [MAINTENANCE] Add marker cli to all cli tests. (#8286) [MAINTENANCE] Reduce maximum allowed unittest duration (#8283) [MAINTENANCE] Add tests for Batch slicing for SparkFilePathDatasource (#8285) [MAINTENANCE] Pytest Mark - RuleBasedProfiler and others (#8288) [MAINTENANCE] Pin jsonschema (#8290) [MAINTENANCE] Remove two slow DataAssistant integration tests (#8287) [MAINTENANCE] Add data docs container to airflow env (#8284) [MAINTENANCE] xfail Cloud E2E test (#8292) [MAINTENANCE] Remove --verbose pytest default (#8298) [MAINTENANCE] Update marker on slow test from unit to filesystem. (#8296) [MAINTENANCE]  Typing Improvements - render/util (#8279) [MAINTENANCE] Add pytest marks to tests/actions (#8299) [MAINTENANCE] Mark tests in tests/core/usage_statistics (#8295) [MAINTENANCE] Mark tests in /tests/data_asset (#8300) [MAINTENANCE] Add markers to tests/core (#8297) [MAINTENANCE] Apply pytest markers to root dir tests/data_context tests (#8293) [MAINTENANCE] Run Github ci action on push (#8303) [MAINTENANCE] Dont use check_for_docs_deps_changes (#8304) [MAINTENANCE] Increase timeout for flaky tests (#8306)  0.17.4  [MAINTENANCE] Protect develop with no-commit-to-branch pre-commit hook (#8254) [MAINTENANCE] Change Pydantic models to utilize by_alias=True (#8252) [MAINTENANCE] Support individual connection args for Snowflake FDS (#8183) [MAINTENANCE] Replace dynamic datasource deletion with single delete method (#8189) [MAINTENANCE] Disable usage statistics when in Cloud-backed environments (#8248)  0.17.3  [FEATURE] Examples of using different store backends in reference environments (#8211) [FEATURE] Add checker to ensure snippets are being used (#8178) [FEATURE] Host data docs for the postgres reference environment (#8221) [FEATURE] Add checker to ensure test files have fixture definition (#8186) [FEATURE] make backend_dependencies required and not optional for doc integration tests (#8216) [FEATURE] View data docs inside snowflake and bigquery reference environments (#8231) [FEATURE] Reference Environment - AWS RDS (#8222) [BUGFIX] Fix yarn install and condition on CI pipeline (#8217) [BUGFIX] Ensure DataAssistantResult.plot_expectations_and_metrics does not raise exceptions when no Metrics or Expectations are available to plot (#8238) [DOCS] update result_format configuration documentation (#8209) [DOCS] Update Install GX (#8206) [DOCS] minor updates to the readme files (#8245) [MAINTENANCE] Arranging call arguments to build_batch_request() utility method to be in consistent order (#8224) [MAINTENANCE] Use python static file server (#8229) [MAINTENANCE] Fix race condition in integration tests using context manager (#8223) [MAINTENANCE] Remove a line ignoring warnings about iteritems (#8227) [MAINTENANCE] Correcting typographical error in test method naming. (#8233) [MAINTENANCE] Reference environment consistency - installed version (#8237) [MAINTENANCE] Reference environment consistency - unpin python version (#8239) [MAINTENANCE] Use data docs container for cloud blob stores (#8240) [MAINTENANCE] Reference Environments: Match the new quickstart (#8242) [MAINTENANCE] Reference Environment - AWS Credentials Clean up (#8230) [MAINTENANCE] Filter altair/jsonschema Deprecation warning (#8244) [MAINTENANCE] Filter jsonschema.RefResolver, ErrorTree warnings in tests (#8246) [MAINTENANCE] Temporarily upper bound Click due to mypy typing issues (#8247) [MAINTENANCE] Update test_deprecation.py in advance of 0.17.3 release (#8251) [MAINTENANCE] enable typechecking in validator.py (#8204) [MAINTENANCE] Refactor validations in Checkpoint to use CheckpointValidationConfig (#8225)  0.17.2  [FEATURE] AWS S3 reference environment (#8166) [FEATURE] Snowflake FDS (#8157) [FEATURE] Fix Bigquery/GCS integration tests (#8149) [FEATURE] Python 3.11 support (#8174) [FEATURE] Fix azure docs tests (#8171) [FEATURE] Reference Environment - BigQuery (#8176) [FEATURE] Reference Environment - Google Cloud Storage (#8197) [FEATURE] Enable updating active stores and CRUD for data docs sites (#8194) [FEATURE] Use PythonScript task type for custom docs ci checks (#8208) [FEATURE] Reference Environment: Azure Blob Storage (#8212) [FEATURE] ExpectDaySumToBeCloseToEquivalentWeekDayMean: User can give weeks_back as argument (#8139) (thanks @HadasManor) [BUGFIX] Allow for create_temp_table to be False when creating a SqlAlchemyBatchData object (#8160) [BUGFIX] Raise more informative error on failure of getting an expectation suite by name (#8170) [BUGFIX] Enable Pandas Column Aggregate Metrics To Support Decimal Numeric Types (#8195) [BUGFIX] Relax numpy version requirements for python 3.10 (#8199) [BUGFIX] Patch additional pytest.deprecated_call around adding datasource with Cloud (#8219) [BUGFIX] Fix yarn install and condition on CI pipeline (#8217) [DOCS] Create New Landing Pages (#7993) [DOCS] Add table listing supported evaluation parameter expressions to the docs (#8124) [DOCS] Reorganize Checkpoint Section (#8182) [DOCS] Getting Started with GX (#8143) [DOCS] Sidebar updates for Getting Started (#8196) [DOCS] Get Started Section Revisions (#8202) [DOCS] Remove Connect to a source data system from TOC (#8193) [MAINTENANCE] Raise error on unsupported versions of python. (#8158) [MAINTENANCE] Reference Environment - Update Snowflake Notebook to Specify Python 3.10 (#8167) [MAINTENANCE] Refactor reference environment CLI for consistency (#8169) [MAINTENANCE] DX-565 remove allow_cross_type_comparison from column_pair_values.a_greater_than_b (#8025) [MAINTENANCE] Python 3.11 SQLAlchemy import time fix (#8180) [MAINTENANCE] Rich comments and docstrings for DataAssistant related modules. (#8185) [MAINTENANCE] Fix Python 3.11 Docker excessive pip backtracking (#8187) [MAINTENANCE] Start cleaning up outdated deprecation warnings (#8135) [MAINTENANCE] Replace Non-Existent Expectation Name With Correct Existing Expectation Name (#8191) [MAINTENANCE] Deal with Python 3.11 deprecations (#8192) [MAINTENANCE] Placing S3, Trino Imports Under Compatibility Pattern (#8198) [MAINTENANCE] Deprecate adding legacy datasources in Cloud-backed environments (#8190) [MAINTENANCE] Add pytest.deprecated_call to misc usages of add_datasource in tests (#8207) [MAINTENANCE] Placing AWS RedShift Imports Under Compatibility Pattern (#8205) [MAINTENANCE] Placing Snowflake Imports Under Compatibility Pattern (#8210) [MAINTENANCE] Placing AWS Athena Imports Under Compatibility Pattern (#8213) [MAINTENANCE] Update notebook link to non-legacy docs (#8215) [MAINTENANCE] Placing BigQuery Imports Under Compatibility Pattern (#8214) [MAINTENANCE] Fix Python 3.11 async expectations tests (#8203)  0.17.1  [FEATURE] Fix spark docs tests (#8131) [FEATURE] Reference Environment - Snowflake (#8148) [FEATURE] Fluent Data Source ABS, GCS, and S3 recursive file discovery (#8118) (thanks @toivomattila) [BUGFIX] Fix config substitution for substrings (#8145) [BUGFIX] Fix 'great_expectations[cloud]' extra install (#8151) [BUGFIX] Persist SQLDatasource splitters on creation (#8164) [DOCS] Enable docs versioning for 0.16.16 (#8125) [DOCS] Update set_based_column_map_expectation.py (#8142) [DOCS] Fix broken links and reenable link checking (#8146) [DOCS] Add version info to markdown links for earlier versions (#8155) [MAINTENANCE] update location of link in data docs footer (#8130) [MAINTENANCE] invoke show-automerges (#8127) [MAINTENANCE] Lint assets dir (#8123) [MAINTENANCE] test/integration linting (#8132) [MAINTENANCE] minimal Anonymizer type-checking (#8106) [MAINTENANCE] Use Available Enums Instead of Strings for Metric Name Extensions (#8137) [MAINTENANCE] Add env var to avoid out of memory error when building docs (#8144) [MAINTENANCE] Update single cloud onboarding script to use fluent datasources (#8114) [MAINTENANCE] Add --bash command for postgres reference env (#8154) [MAINTENANCE] Add default table_name to TableAsset if omitted (#8152) [CONTRIB] expect_queried_column_pair_values_to_be_both_filled_or_null (#7949) (thanks @eden-o)  0.17.0  [FEATURE] Agent prints stack trace on error (#8092) [FEATURE] Clickhouse Integration (#7719) (thanks @Plozano94) [FEATURE] Pandas 2.0.0 and Sqlalchemy 2.0.0 compatibility (#7633) [FEATURE] Better pandas query (#8101) [FEATURE] Add Pandas FWFAsset - fixed width file (#8119) [BUGFIX] Fix conditional for pyspark compatibility (#8108) [BUGFIX] respect result format bool only for validators and checkpoints (#8111) [BUGFIX] Robust Handling Of Column Types And Empty DataFrames For DataBricks/Spark Environment (#8115) [BUGFIX] Fix GXCloudStoreBackend updates by name (#8116) [BUGFIX] Patch bad mock in GCS test (#8128) [DOCS] Update Quickstart guide to be Cloud-compatible (#8036) [DOCS] Temporarily hardcode quickstart snippet due to substitution error (#8091) [DOCS] Update prefect gx tutorial (#8009) (thanks @discdiver) [DOCS] Fix line breaks in quickstart (#8098) [DOCS] Remove Remaining CLI Admonitions (#8070) [DOCS] Remove examples of specifying a test_backends list of dicts (#7816) [MAINTENANCE] Add tests for file and Cloud-backed quickstart workflows (#8037) [MAINTENANCE] Update anonymous usage statistics payloads with hashed MAC address (#8078) [MAINTENANCE] Check if PR is a fork in some docs_integration stages (#8090) [MAINTENANCE] Update how_to_host_and_share_data_docs_on_gcs (#8067) [MAINTENANCE] Add persistent OSS user identifier to anonymized usage statistics payloads (#8089) [MAINTENANCE] Update MSSQL docker container and instructions for Apple Silicon Macs (#8093) [MAINTENANCE] Remove hardcoded Data Source._EXCLUDED_EXEC_ENG_ARGS (#8100) [MAINTENANCE] Update print_diagnostic_checklist (#8018) [MAINTENANCE] Type hint cleanup in usage statistics (#8105) [MAINTENANCE] remove unused noqa comments (#8107) [MAINTENANCE] Remove the mysql-mac-m1 and starburst directories from assets/docker (#8104) [MAINTENANCE] Typing Data Assistant Result (#8110) [MAINTENANCE] Ensure that new usage statistics schema changes are backwards compatible (#8109) [MAINTENANCE] Cleanup generate_expectation_tests (#8019) [MAINTENANCE] Update build_in_memory_runtime_context to accept which datasources to include (#8017) [MAINTENANCE] Pandas and SqlAlchemy 2.0 follow-up (#8112) [MAINTENANCE] Misc/docs integration uncommented (#8076) [MAINTENANCE] Fixes for test_dependency_versions pipeline (#8122)  0.16.16  [FEATURE] Update mySQL docker compose to specify platform (#8046) [FEATURE] Update GX Cloud on job status (#8047) [FEATURE] DX-441 put how_to_connect_to_in_memory_data_using_pandas un\u2026 (#8057) [BUGFIX] Set SQLALCHEMY_WARN_20 in Dockerfile (#7931) [BUGFIX] Ensure CloudDataContext Add Checkpoint flow returns Checkpoint with cloud-updated values (#8062) [BUGFIX] Erroneous Code Duplication and Lack of Type Hints in Expectation Parent Class (#8066) [BUGFIX] Fix broken Cloud tests around Checkpoint (#8083) [BUGFIX] fix issue-7954 (#7963) (thanks @jkingdon-ms) [BUGFIX] Fix Update Checkpoint for Cloud (#8084) [BUGFIX] Mock correct method in test (#8087) [DOCS] Cloud - Getting started uses Fluent Datasources (#8035) [DOCS] Update Links in How to use auto-initializing Expectations (#8054) [DOCS] Update How to create Expectations Interactively in Python (#8052) [DOCS] Add guidance for \"DataFrameAsset.build_batch_request()\" in \"https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/fluent/in_memory/how_to_connect_to_in_memory_data_using_pandas\". (#8069) [DOCS] Update set_based_column_map_expectation_template.py (#8068) [DOCS] Remove mention of CLI in evaluation_parameters terminology page. (#8075) [MAINTENANCE] Ruff rule DTZ - prevent naive datetimes (#8050) [MAINTENANCE] Pandas 1.5 schema updates (#8039) [MAINTENANCE] Fix CI tests_schemas unit test (#8055) [MAINTENANCE] Fix CI round 2 (#8056) [MAINTENANCE] Minor stylistic clean up of \"DataAssistant.build_metric_multi_batch_parameter_builder()\" method usage (#8059) [MAINTENANCE] Enable Ruff - pylint rules (#8058) [MAINTENANCE] type-checking core/batch.py (#8024) [MAINTENANCE] Improve code elegance and fix informational string (#8061) [MAINTENANCE] Misc refactor of Store CRUD (#8064) [MAINTENANCE] Fix pandas model schema differences across python versions (#8065) [MAINTENANCE] Updating CLI messages to point to new docs (#8072) [MAINTENANCE] Update GitHub templates for issues/PRs (#8073) [MAINTENANCE] Update default action list in Checkpoint based on user environment (#8074) [MAINTENANCE] Ruff 0.0.271 update (#8077) [MAINTENANCE] Update PR template to reference invoke for linting (#8079) [MAINTENANCE] Delete LegacyCheckpoint (#8082) [MAINTENANCE] Add public_api to read_parquet (#8060) [MAINTENANCE] Postgres starter reference environment (#8031)  0.16.15  [FEATURE] Checker for correct Python code snippets in documentation. (#8000) [FEATURE] Add scaffolding for gx-agent (#7837) [FEATURE] Cloud - generate unique asset names for pandas_default assets (#8020) [FEATURE] Connect to Cloud for agent config (#8006) [FEATURE] Add AbstractDataContext.view_validation_result (#8033) [FEATURE] Add Onboarding Data Assistant agent action (#7882) [FEATURE] put 'How to connect to postgresql data' under test (#7988) [BUGFIX] Remove perpetually failing test -- in preparation for release. (#8040) [BUGFIX] meta_notes rendered inline can fail schema validation for a valid string-only configuration (#8044) [BUGFIX] Ensure That DataFrame for Fluent Data Source Pandas/Spark DataFrame Data Assets is specified only in one API method (#8032) [DOCS] Add Windows Support Admonition (#7991) [DOCS] Update how_to_edit_an_existing_expectationsuite.md (#8007) [DOCS] Adds redirects for removed UserConfigurableProfiler documentation (#8026) [DOCS] Create New Expectation Classes Conceptual Guide (#8004) [DOCS] Check simple spelling and word order error using ChatGPT-based program (#8029) [DOCS] Update Quickstart to Reflect User Feedback (#8027) [DOCS] Cloud - Getting started uses Fluent Datasources (#8035) [MAINTENANCE] Update teams.yml (#8011) [MAINTENANCE] Add sensible default values to Checkpoint APIs (#7992) [MAINTENANCE] Removing xfails that we're added to get release out. (#8014) [MAINTENANCE] Remove reference to Superconductive from CITATION.cff (#8016) [MAINTENANCE] Deleting References to UserConfigurableProfiler Documentation (#7983) [MAINTENANCE] mypy 1.3 + azure deps in type-checking step (#8012) [MAINTENANCE] Bump tornado from 6.1.0 to 6.3.2 in /docs_rtd (#8003) [MAINTENANCE] Add support for Validator in Checkpoint CRUD (#7999) [MAINTENANCE] Remove unused .dockerignore (#8030) [MAINTENANCE] Fluent Data Source Update: Credentials Doc update (#7968) [MAINTENANCE] Persist connection for backends that need a connection to keep temporary tables alive (#7607)  0.16.14  [FEATURE] Add \"batch.columns()\" convenience method to Fluent DataAsset implementation. (#7926) [FEATURE] Implementing Python code snippets under test for \"https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/fluent/filesystem/how_to_connect_to_one_or_more_files_using_spark\" (#7927) [FEATURE] Implementing Python code snippets under test for \"https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/fluent/filesystem/how_to_quickly_connect_to_a_single_file_with_pandas\" (#7938) [FEATURE] Implementing Python code snippets under test for \"https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/fluent/cloud/how_to_connect_to_data_on_s3_using_pandas\" (#7941) [FEATURE] Implementing Python code snippets under test for \"https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/fluent/cloud/how_to_connect_to_data_on_s3_using_spark\" (#7943) [FEATURE] Implementing Python code snippets under test for \"https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/fluent/database/how_to_connect_to_sqlite_data\" (#7947) [FEATURE] Implementing Python code snippets under test for \"https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/fluent/cloud/how_to_connect_to_data_on_gcs_using_pandas\" (#7959) [FEATURE] Implementing Python code snippets under test for \"https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/fluent/cloud/how_to_connect_to_data_on_gcs_using_spark\" (#7964) [FEATURE] add ssm parameter support for config secrets (#7940) (thanks @Isaacwhyuenac) [FEATURE] Implementing Python code snippets under test for \"https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/fluent/cloud/how_to_connect_to_data_on_azure_blob_storage_using_pandas\" (#7965) [FEATURE] Implementing Python code snippets under test for \"https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/fluent/cloud/how_to_connect_to_data_on_azure_blob_storage_using_spark\" (#7967) [FEATURE] Implementing Python code snippets under test for \"https://docs.greatexpectations.io/docs/guides/setup/configuring_data_contexts/instantiating_data_contexts/how_to_instantiate_a_specific_filesystem_data_context\" (#7984) [FEATURE] Implementing Python code snippets under test for \"https://docs.greatexpectations.io/docs/guides/setup/configuring_data_contexts/initializing_data_contexts/how_to_initialize_a_filesystem_data_context_in_python\" (#7985) [FEATURE] Put how_to_connect_to_sql_data fluent docs under test (#7956) [FEATURE] Implementing Python code snippets under test for \"https://docs.greatexpectations.io/docs/guides/setup/configuring_data_contexts/initializing_data_contexts/how_to_initialize_a_filesystem_data_context_in_python\" and \"https://docs.greatexpectations.io/docs/guides/setup/configuring_data_contexts/how_to_convert_an_ephemeral_data_context_to_a_filesystem_data_context\". (#7986) [FEATURE] putting fluent doc how_to_connect_to_a_sql_table under test (#7966) [FEATURE] put fluent doc \"How to connect to sql data using a query\" under test (#7987) [FEATURE] Checker for correct Python code snippets in documentation. (#7996) [FEATURE] NotImplementedErrors for all FDS methods when accessed from BDS (#8002) [BUGFIX] Correcting instructions for Contributor Package installation (#7936) [BUGFIX] Return qualified name when calling TableAsset.as_selectable() (#7942) (thanks @calabozo) [BUGFIX] fix add_or_update_expectation_suite update path (#7911) [BUGFIX] Cloud - fix add_or_update_*() when using name as keyword arg (#7952) [BUGFIX] Change GXSqlDialect.AWSATHENA to awsathena (#7950) (thanks @calabozo) [BUGFIX] Setting maximum on typing-extension package version (#7970) [BUGFIX] Resolve Issue 7335 (#7339) (thanks @richardohara) [BUGFIX] Cloud - Fix context.sources.update_*() POST instead of PUT calls (#7989) [BUGFIX] meta_notes rendered inline can fail schema validation for a valid configuration (#7995) [BUGFIX] xfail two cloud tests that are blocking release 0.16.14 (#8008) [DOCS] Technical tags in Versioned Docs reference correct version (#7935) [DOCS] Fix docs deploy (#7958) [DOCS] Add small doc change (#7957)  [DOCS] Fix issues with technical tags links in earlier versions (#7961) [DOCS] Temporarily pin typing-extensions for API docs (#7977) [DOCS] Add optional Slack step to Cloud Quickstart (#7955) [DOCS] Add another small fix to doc (#7960) [DOCS] Versioning: Convert technical tag imports starting with /docs/ to relative paths (#7981) [DOCS] Edit Bigquery connection path (#7982) [DOCS] FDS Topic Quality Review (#7944) [DOCS] Updates the migration guide to direct to the last version of GX to support the V2 processes and API surface (#7976) [DOCS] Removes Data Connector references (#7930) [DOCS] add in-memory add expectation suite (#7973) (thanks @tb102122) [DOCS] Removes CLI documentation from current docs version (#7975) [DOCS] Delete CODE_OF_CONDUCT.md (#7625)  [MAINTENANCE] Fixing pytest: renderer assertion (#7928) [MAINTENANCE] Update teams.yml (#7934) [MAINTENANCE] Remove the -rs flag from the ci pytest invocations. (#7937) [MAINTENANCE] Update GXCloudIdentifier to return nullable attrs instead of empty strings (#7895) [MAINTENANCE] Better error message when calling \"Fluent\" methods from wrong Data Source type (#7929) [MAINTENANCE] bump python minimum version to 3.8 (#7916) [MAINTENANCE] Update ruff to 0.0.269 and target python 3.8 (#7945) [MAINTENANCE] black 23.3 (#7946) [MAINTENANCE] ignore black formatting and ruff auto-fix revisions (#7953) [MAINTENANCE] Refactor documentation integration tests into their proper categories according to their respective backend. (#7978) [MAINTENANCE] Bump requests from 2.25.1 to 2.31.0 in /docs_rtd (#7969) [MAINTENANCE] Update Cloud Tests for release this week + Revert pin on typing-extension (#7980) [MAINTENANCE] Use Session for all api.greatexpectations.io calls (#7919) [MAINTENANCE] Revert \"[FEATURE] Checker for correct Python code snippets in documentation.\" (#7998) [MAINTENANCE] Fix async Cloud tests (#8005)  0.16.13  [FEATURE] Spark file reader support for fluent datasources (#7844) [FEATURE] Spark directory asset types (#7873) [FEATURE] Add Spark DeltaAsset type (#7872) [FEATURE] Add DirectoryDeltaAsset (#7877) [FEATURE] Implementing Python code snippets under test for \"https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/fluent/data_assets/how_to_organize_batches_in_a_file_based_data_asset\" (#7907) [FEATURE] Implementing Python code snippets under test for \"https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/fluent/data_assets/how_to_organize_batches_in_a_sql_based_data_asset\" (#7909) [FEATURE] Implementing Python code snippets under test for \"https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/fluent/filesystem/how_to_connect_to_one_or_more_files_using_pandas\" (#7922) [FEATURE] DataProfilerStructuredDataAssistant Float Rule (#7842) (thanks @micdavis) [BUGFIX] Fix inability to extend SimpleCheckpoint -- and several additional enhancements and clean up (#7879) [BUGFIX] Delete ExpectationSuite by name in GX Cloud (#7881) [BUGFIX] optional dataframe on datasources (#7862) [BUGFIX] Fix sparkDF cannot compute mean for DecimalType (#7867) [BUGFIX] Fix Cloud FDS add_or_update_* methods (#7908) [BUGFIX] fix PandasAzureBlobStorageDatasource config substitution (#7914) [BUGFIX] Fix remaining FDS config substitution issues (#7917) [DOCS] removes remaining Block-config Data Source guides (#7870) [DOCS] Update \"How to use Great Expectations with Databricks\" (#7762) [DOCS] CLI Edits (#7865) [DOCS] More doc updates to remove the CLI (#7874) [DOCS] Create New Templates for How-To, Tutorial, Conceptual, and Reference Documentation (#7855) [DOCS] Add Links to Tutorial Templates in the README (#7884) [DOCS] removes block-config docs for source data systems in the cloud (#7871) [DOCS] Continuing CLI Update (#7876) [DOCS] Removes the SQL block config guides. (#7886) [DOCS] Remove jq dependency for building docs (#7893) [DOCS] Less verbose logging during docs build (#7894) [DOCS] Corrections to Document \"How to request data from a Data Asset\" with proper FDS implementation and integration test. (#7896) [DOCS] Remove or Modify References to the CLI (#7875) [DOCS] Updated contribution docs (#7880) [DOCS] Glossary Update (#7900) [DOCS] Removes filesystem and core block config docs (#7913) [DOCS] FDS Deployment Pattern Redshift (#7868) [DOCS] Document and simplify local docs build (#7892) [DOCS] How to Edit Existing ExpectationSuite (#7859) [DOCS] CLI Clean-up (#7904) [MAINTENANCE] FDS Documentation - Creating ExpectationSuite with Domain Knowledge (#7852) [MAINTENANCE] Refactor directory data asset (#7878) [MAINTENANCE] partial checkpoint type checking (#6914) [MAINTENANCE] type-checking checkpoint/actions.py (#7899) [MAINTENANCE] Trigger docs_integration on develop (#7902) [MAINTENANCE] Finish type-checking checkpoint (#7901) [MAINTENANCE] Update How to set up GX to work with data in Azure Blob Storage (#7910) [MAINTENANCE] CloudDataContext ExpectationsStore can only request one Suite at a time (#7905) [MAINTENANCE] Removing engine-specific tests that assert generic behavior (#7918) [MAINTENANCE] Add docs/*.py to GXChanged for linting (#7924) [MAINTENANCE] Fixes Glue tests on vanilla pytest (#7925)  0.16.12  [FEATURE] Plumbing of validation_result_url from cloud response (#7809) [FEATURE] Splitters work with Spark Fluent Datasources (#7832) [FEATURE] Update get_context to scaffold project structure for file-backed usecases (#7693) [BUGFIX] Azure Package Presence/Absence Tests Strengthening (#7818) [BUGFIX] Handle \"persist\" directive in \"SparkDFExecutionEngine\" properly. (#7830) [BUGFIX] Adding support for Fluent Batch Requests to context.get_validator (#7808) [BUGFIX] FDS - Deletes not immediately reflected in great_expectations.yml (#7843) [BUGFIX] batching_regex tags are now correctly rendered in docs (#7845) [BUGFIX] Fix link checker and add to mypy type checking (#7857) [BUGFIX] expect_day_count_to_be_close_to_equivalent_week_day_mean (#7782) (thanks @HadasManor) [BUGFIX] Docs-Tests: Connection.connect() was causing Snowflake and BigQuery Tests to Fail (#7863) [DOCS] Prerequisites Cleanup (#7811) [DOCS] Update docs for how_to_initialize_a_filesystem_data_context_in_python (#7831) [DOCS] Updating Checkpoint terms page (#7722) [DOCS] Update how to create a checkpoint with Test YAML config (#7835) [DOCS] Removing datasource centric test_yaml_config doc (#7836) [DOCS] Creating a Checkpoint from an In-Memory Dataframe (#7701) [DOCS] Review and Revise Great Expectations Quickstart (#7727) [MAINTENANCE] FDS - Datasources can rebuild their own asset data_connectors (#7826) [MAINTENANCE] Enable Spark-S3 Integration tests on Azure CI/CD (#7819) [MAINTENANCE] Clean up: Remove duplicated fixture and utilize deeper filtering mechanism for configuration assertions. (#7825) [MAINTENANCE] Enable S3/Spark Connecting To Your Data tests (#7828) [MAINTENANCE] New PR template (#7710) [MAINTENANCE] ruff .0.262 -> .0.265 (#7829) [MAINTENANCE] Boto import pattern established (#7796) [MAINTENANCE] Prevent TCH001 warnings for pydantic model annotations (#7846) [MAINTENANCE] Pin altair (#7849) [MAINTENANCE] Adding docs link checker to invoke (#7841) [MAINTENANCE] Clean up version checker message formatting (#7838) [MAINTENANCE] Bump nbconvert version (#7847) [MAINTENANCE] Return empty set instead of None (#7797) [MAINTENANCE] Improve misconfigured sampler error message (#7858) [MAINTENANCE] Fixing path formatting for DataConnector of Fluent SparkAzureBlobStorageDatasource and correction of the SQLAlchemy compatibility usage in TableHead metric (#7860) [MAINTENANCE] S3 Spark Integration Guide - Rendering Fix (#7864)  0.16.11  [FEATURE] Add tests for Checkpoint utilizing SQLAlchemy style Fluent Datasources. (#7759) [FEATURE] Spark parquet reader support for fluent datasources (#7754) [FEATURE] Add tests for SimpleCheckpoint utilizing Fluent Datasources with Pandas, Spark, and SQLAlchemy test cases. (#7778) [FEATURE] Spark read directory of files as a single batch for CSV (#7777) [FEATURE] Enable passing \"spark_config\" through to \"SparkDFExecutionEngine\" constructor as arguments to \"add_spark*()\" Fluent Datasources methods. (#7810) [BUGFIX] Patch faulty version checker logic (#7783) [BUGFIX] Fix FDS Sqlite config round tripping (#7791) [BUGFIX] Correct import errors in Azure Blob Storage tests and make Azure Glob Storage and Google Cloud Storage tests more elegant. (#7795) [BUGFIX] Add pytest decorator to fixture (#7803) [BUGFIX] fix class name (#7734) (thanks @tb102122) [BUGFIX] Repair handling of regular expressions partitioning for cloud file storage environments utilizing prefix directive. (#7798) [BUGFIX] AWS Docs reference clash (#7817) [BUGFIX] Cloud - Fix FDS Asset has no attribute _data_connector (#7813) [BUGFIX] Upper bound pyathena due to breaking API in V3 (#7821) [DOCS] FDS Deployment Pattern - Google Cloud:  BigQuery and GCS (#7741) [DOCS] Remove temporary pin for ipython (#7784) [DOCS] Add CLI Admonition (#7765) [DOCS] Link Update (#7788) [DOCS] Remove Redundant Introduction Headings (#7747) [DOCS] Remove Prerequisites from Admonitions (#7786) [DOCS] Link Updates (#7781) [DOCS] FDS Deployment Pattern - AWS: Spark and S3 (#7775) [MAINTENANCE] Add check to CloudDataContext to ensure using latest PyPI version (#7753) [MAINTENANCE] Cache the latest great_expectations version (#7785) [MAINTENANCE] Enable flake8-bugbear rules (#7776) [MAINTENANCE] Cleanup of Fluent BatchRequest type and immutability constraints (#7769) [MAINTENANCE] CLI warnings for suite new command (#7787) [MAINTENANCE] Update pip instal extras and use AWS_ env vars (#7793) [MAINTENANCE] Test DirectoryCSVAsset with both str and pathlib.Path (#7801) [MAINTENANCE] Lint test/data_context (#7767) [MAINTENANCE] FDS Documentation Update - S3 Pandas reference fixes (#7789) [MAINTENANCE] Update all pytest calls in CI to show reason skipped (#7806) [MAINTENANCE] Dont run runme_script_runner_tests stage on forks (#7807) [MAINTENANCE] Lint tests/checkpoint & tests/execution_engine (#7804) [MAINTENANCE] docs-integration re-start (#7735) [MAINTENANCE] Remove runme fixtures/stages and enable docs-integration to run automatically (#7812) [MAINTENANCE] Fix linting error. (#7820) [MAINTENANCE] Fix pin count. (#7823)  0.16.10  [FEATURE] Add tests for Checkpoint utilizing Pandas and Spark style Fluent Datasources. (#7740) [FEATURE] Fluent BatchRequest slicing (#7706) [BUGFIX] Patch Expectation registry issue introduced in 0.16.9 (#7771) [DOCS] Remove, relocate, consolidate, and edit Contributing content (#7669) [DOCS] Temporarily pin ipython for python 3.8 before building api docs (#7764) [DOCS] Update Links in Configure Topics (#7760) [DOCS] Link Updates (#7768) [MAINTENANCE] FDS Deployment Guide - Pandas S3 fix reference (#7755) [MAINTENANCE] IPython Python 3.8 upper bound (#7763) [MAINTENANCE] breakup mypy ci steps (#7761) [MAINTENANCE] fix async type-check step (#7772) [MAINTENANCE] Bump Python version in static_type_check stage of async CI (#7773)  0.16.9  [FEATURE] Implementing Fluent Datasources Support for Checkpoint (#7697) [FEATURE] FDS persist DataAsset to YAML file immediately on creation (#7705) [FEATURE] Cloud support FDS deletes (#7682) [FEATURE] Persist Cloud DataAssets on creation (#7748) [FEATURE] Add tests for Checkpoint utilizing Pandas and Spark style Fluent Datasources. (#7740) [BUGFIX] Render Correct Fonts in Data Assistant Plot graphs (#7676) [BUGFIX] fix rendering data asset name in microsoft teams notification (#7675) [BUGFIX] Register core Expectations JIT in Validator workflows (#7683) [BUGFIX] Data Context Data Source CRUD support for Fluent Datasources (#7660) [BUGFIX] Replace renamed fixture (#7711) [BUGFIX] Add missing pyspark reference (#7684) (thanks @willfeltman) [DOCS] Add fluent datasources and yaml configuration warning message (#7673) [DOCS] D/ /fluent connect to data overview (#7671) [DOCS] Update fluent In Progress Cautionary Note (#7681) [DOCS] Remove version reference (#7644) [DOCS] Removing in-progress from docs confirmed as up-to-date (#7686) [DOCS] Updated Data Source terms page (#7692) [DOCS] Removing json schema profiler documentation (#7694) [DOCS] Removing CLI-based suite edit workflow (#7689) [DOCS] Updated onboarding data assistant docs test script to Fluent-style (#7695) [DOCS] Update for fluent datasources: Expectations that span multiple batches evaluation params (#7668) [DOCS] Add fluent docs and test create and edit expectations with profiler (#7696) [DOCS] Quick docstring update for list_datasources (#7699) [DOCS] Retiring the CLI (#7700) [DOCS] Updating the Rule-Based Profiler doc to Fluent (#7698) [DOCS] Update Batch Request glossary entry. (#7716) [DOCS] Removed guide for no YML, redirect to EphemeralDataContext (#7702) [DOCS] Update for fluent datasources: Dynamically load evaluation params from a database (#7717) [DOCS] Update batch glossary docs. (#7726) [DOCS] Update for fluent datasources: Create a new Checkpoint (#7729) [DOCS] Temporarily revert update_expectation_suite call in GX Cloud quickstart (#7736) [DOCS] Light update to How to add validations data or suites to a Checkpoint (#7703) [DOCS] Updating cross-table comparison guide with Fluent Datasources (#7691) [DOCS] Better output from invoke public-api report (#7730) [DOCS] Removed unneeded calls to update datasource in docs. (#7739) [DOCS] FDS Deployment Pattern - AWS S3 Pandas (#7718) [DOCS] Update pypyi page urls (#7752) [MAINTENANCE] Correcting minor typographical errors and type hints issues in Checkpoint and Test Checkpoint Modules (#7665) [MAINTENANCE] Only attempt docs-integration pipeline when manually triggered (#7674) [MAINTENANCE] Clean up Checkpoint test method names and usage of batch_request_dict fixture (#7670) [MAINTENANCE] Correct typo in Checkpoint test method fixture (#7677) [MAINTENANCE] Enable remaining CloudDataContext ExpectationSuite CRUD (#7646) [MAINTENANCE] list_datasources should return FDS configs as well (#7667) [MAINTENANCE] Exit with error when attempting to delete a fluent style datasource using the CLI (#7687) [MAINTENANCE] Add warning to datasource list command if fluent datasources are detected (#7690) [MAINTENANCE] ruff update 0.0.262 (#7707) [MAINTENANCE] Adding black to invoke lint (#7715) [MAINTENANCE] Use same version of mypy in contrib tool (#7724) [MAINTENANCE] Update a Fluent Data Source related fixture name to better reflect its capabilities (#7725) [MAINTENANCE] Add CLI warnings when adding a checkpoint with fluent datasources (#7685) [MAINTENANCE] Iterate over the regex_pattern characters too in (#7720) [MAINTENANCE] Minor stylistic cleanup (#7732) [MAINTENANCE] fix get available data assets names for fds (#7723) [MAINTENANCE] add warning messages when using CLI to edit an expectaiton suite if fluent datasources are present (#7714) [MAINTENANCE] add warning to datasource new CLI command (#7709) [MAINTENANCE] Add split/join logic to build_gallery process (#7572) [MAINTENANCE] Use invoke public-api in main CI pipeline (#7746) [MAINTENANCE] Add remaining public_api decorators for core fluent datasources (#7749) [MAINTENANCE] FDS update schemas - fixes CI (#7751) [MAINTENANCE] FDS Deployment Guide - Pandas S3 fix reference (#7755)  0.16.8  [FEATURE] add Fluent Datasources to CloudDataContext (#7570) [BUGFIX] fix marshmallow schema for SQLAlchemy connect_args passthrough (#7614) [BUGFIX] MapCondition Memory Inefficiencies in Spark (#7626) [BUGFIX] Fix capitalone_dataprofiler_expectations imports (#7658) [BUGFIX] CloudDataContext creates great_expectations.yml when adding a Fluent datasource (#7657) [BUGFIX] Correct GX configuration structure that incorporates both V3 and Fluent Datasources (#7661) [BUGFIX] Patch broken include_rendered_content test in advance of 0.16.8 release (#7663) [DOCS] Corrects Step Numbering in How to instantiate a specific Filesystem Data Context (#7612) [DOCS] Corrects Heading Issue in How to host and share Data Docs on Azure Blob Storage (#7620) [DOCS] Update overview.md (#7627) [DOCS] Updates the \"Interactive Mode\" guide for creating Expectations (#7624) [DOCS] Updates the language in the banner linking the legacy site to the current docs. (#7636) [DOCS] Improve expect_column_values_to_be_of_type docstring (#7632) [DOCS] Corrects a typo found in the navigation section of the legacy docs (#7643) [DOCS] Add lakeFS to list of data version control tools (#7642) (thanks @rmoff) [DOCS] Standardize language around GX Cloud access tokens (#7621) [DOCS] Added IAM user and IAM assume role doc (#7634) (thanks @Reactor11) [DOCS] update to location of cloud callout in the OSS Quickstart (#7616) [MAINTENANCE] Update teams.yml (#7623) [MAINTENANCE] Utilize NotImported for SQLAlchemy, Google Cloud Services, Azure Blob Storage, and Spark import usage (#7617) [MAINTENANCE] Remove stray cloud test marker. (#7639) [MAINTENANCE] Upgrade mypy to 1.2.0 (#7645) [MAINTENANCE] Static type checking with python 3.8 (#7637) [MAINTENANCE] Static type checking with python 3.8 followup (#7647) [MAINTENANCE] The 'sklearn' PyPI package is deprecated, use 'scikit-learn' (#7651) [MAINTENANCE] numpy.typing only available after v1.20 (#7654) [MAINTENANCE] Update NotImported mechanism to use scoped compatibility modules (#7635) [MAINTENANCE] Uncap altair version, and bump minimum version to 4.2.1. Also uncap urllib3 version, and bump minimum version to 1.26 (#7650)  0.16.7  [FEATURE] Added AssumeRole Feature (#7547) (thanks @Reactor11) [BUGFIX] Fix Fluent Spark DataConnectors on config load (#7560) [BUGFIX] dataset_name made optional parameter for Expectations (#7603) [BUGFIX] Misc gallery bugfixes (#7611) [BUGFIX] Remove spark from bic Expectations since it never worked for them (#7619) [DOCS] Use current minor version number in drop down instead of \"Current\" (#7581) [DOCS] Adds deprecation policy to changelog page (#7585) [DOCS] Use the actual version after release (#7583) [DOCS] Update some docs_rtd requirements so the venv can be created successfully (#7580) [DOCS] Add Cloud quickstart (#7441) [DOCS] Updates how the GX Cloud Beta is referenced in the Quickstart guide. (#7594) [DOCS]  Corrects typo in code block within in-memory Pandas guide (#7600) [DOCS] Updates to Contributing through GitHub (#7601) [DOCS] Correct expectation documentation for expect_column_max_to_be_between (#7597) [DOCS] Add scripts under test for \"How to create and edit Expectations with instant feedback from a sample Batch of data\" (#7615) [DOCS] Corrects Step Numbering in How to instantiate a specific Filesystem Data Context (#7612) [DOCS] Corrects Heading Issue in How to host and share Data Docs on Azure Blob Storage (#7620) [MAINTENANCE] Warning non integer slice on row for SQLAlchemy 2.0 Compatibility (#7501) [MAINTENANCE] Warning MetaData.bind argument deprecated for SQLAlchemy 2.0 Compatibility (#7502) [MAINTENANCE] Capitalize \"If\" in rendering of conditional Expectations (#7588) [MAINTENANCE] Remove pip pins in CI and in contributing_setup.md (#7587) [MAINTENANCE] Remove ignore of warning deprecated api features detected sqlalchemy 2 (#7584) [MAINTENANCE] Fix sqlalchemy 2.0 incompatible warnings (#7589) [MAINTENANCE] Increase minimum scipy version package to 1.6.0 to take advantage of available capabilities. (#7591) [MAINTENANCE] Remove s3fs dependency and upper bound for boto3 (#7598) [MAINTENANCE] Move Fluent Datasources Sorters into TYPE_CHECKING block (#7602) [MAINTENANCE] Bump terser from 5.10.0 to 5.16.8 in /docs/docusaurus (#7486) (thanks @dependabot[bot]) [MAINTENANCE] Bump cookiecutter from 1.7.3 to 2.1.1 in /contrib/cli (#7510) (thanks @dependabot[bot]) [MAINTENANCE] Polish and ratchet requirements pins and upper bounds (#7604) [MAINTENANCE] small documentation updates (#7606) [MAINTENANCE] SqlAlchemy 2 Compatibility - engine.execute() (#7469) [MAINTENANCE]  Deprecate ColumnExpectation in favor of ColumnAggregateExpectation (#7609) [MAINTENANCE] Deprecate TableExpectation in favor of BatchExpectation (#7610) [MAINTENANCE] Explicitly test relevant modules in Sqlalchemy compatibility pipeline (#7613) [MAINTENANCE] Fluent Datasources: Eliminate redundant Data Source name and DataAsset name from dictionary and JSON configuration (#7573) [CONTRIB] add check to calculate difference between 2 dates in month (#7576) (thanks @tb102122) [CONTRIB] Expect Column Values to be Valid UUID - Added SqlAlchemyExecutionEngine support (#7592) (thanks @asafla)  0.16.6  [FEATURE] Fluent DataAsset batch_metadata config variables (#7513) [FEATURE] Add batch metadata to spark add_*_asset methods (#7534) [BUGFIX] Fluent Data Source load from config fixes for remaining Pandas Datasources (#7442) [BUGFIX] Address pandas==2.0.0 test failures (#7553) [BUGFIX] Render prescriptive ExpectationConfigurations with evaluation parameters inline (#7552) [BUGFIX] Release Pipeline Fix (#7575) [DOCS] Update GX version in _data.jsx component (#7549) [DOCS] Adds guides on using Ephemeral Data Contexts and updates Quickstart Next Steps (#7500) [DOCS] Fixes broken code block and incorrectly numbered steps in \"How to organize Batches in a SQL-based Data Asset\" (#7533) [DOCS] Update nav to match gx.io site (#7557) [DOCS] Corrects step numbers in \"How to organize Batches in a file-based Data Asset\" (#7559) [DOCS] Delete SLACK_GUIDELINES.md (#7566) [DOCS] Update syntax highlighting of code blocks in GX Cloud Getting Started guide (#7563) [DOCS] Fix code snippets for earlier versions (#7554) [DOCS]  Fix typo in docs (#7568) [DOCS] Moar typo fix (#7569) [DOCS] removes the original getting started tutorial pages and redirects to the quickstart guide (#7548) [DOCS] Fix integral typo (#7578) [DOCS] Prepare earlier versions using develop (#7567) [DOCS] Use orange in docs logs (#7579) [DOCS] Add GX Cloud Onboarding Script (#7517) [MAINTENANCE] release prep for 0.16.5 (#7545) [MAINTENANCE] Test Pandas 2.0 prerelease in CI/CD (#7343) [MAINTENANCE] Add noqa directives for existing sqlalchemy imports (#7564) [MAINTENANCE] Add ruff rule for sqlalchemy imports (#7562) [MAINTENANCE] adding a footer to data docs with a link to the cloud page (#7532) [MAINTENANCE] Harden tests for CloudDataContext always include_rendered_content (#7558) [MAINTENANCE] FluentDatasources - Quickstart Snippets converted to Named Snippets (#7550) [MAINTENANCE] Simplify GXCloudStoreBackend._has_key check (#7561) [MAINTENANCE] Temporarily Pin pandas<2.0.0 for compatibility (#7571) [MAINTENANCE] SqlAlchemy 2.0 Compatibility - branched connection + bind argument now required (#7529) [MAINTENANCE]  Add missing docstrings to fluent sql_datasource splitter methods. (#7577)  0.16.5  [FEATURE] Add batch metadata to sql datasources. (#7499) [BUGFIX] Fix issue running quickstart (#7539) [DOCS] doc 508 Updates footer links on docs pages (#7521) [DOCS] DSB-64 removes outdated v2/v3 references from the docs (#7519) [DOCS] Update CODEOWNERS (#7528) [DOCS] Quickstart code under test (#7542) [MAINTENANCE] SqlAlchemy2 Compatibility - Row.keys() (#7520) [MAINTENANCE] Refactoring of CapitalOne Metrics and Profiler-Based DataAssistant for Enhanced Code Elegance (#7522) [MAINTENANCE] SqlAlchemy 2 Compatibility - Autoload Parameter deprecation (#7526) [MAINTENANCE] Bump notebook from 6.4.1 to 6.4.12 in /docs_rtd (#7511) [MAINTENANCE] Break out unit tests to own stage. (#7530) [MAINTENANCE] Bump wheel from 0.37.1 to 0.38.1 in /contrib/cli (#7493) [MAINTENANCE] Simplifying CapitalOne DataProfilerColumnDomainBuilder Using Default \"profile_path\" Argument (#7535) [MAINTENANCE] : Clean up ununsed imports (#7537) [MAINTENANCE] Fix Type-Checking steps (#7536) [MAINTENANCE] Disable UserConfigurableProfiler tests relying on deprecated V2 functionality (#7541) [MAINTENANCE] : replace ColumnMetricProvider with ColumnAggregateMetricProvider (#7538) [MAINTENANCE] Exclude files from deprecation warning check (#7544)  0.16.4  [FEATURE] Add package, contributors and metrics filter in Algolia script for expectation (#7000) (thanks @kod-er) [FEATURE] BatchMetadata for all fluent DataAssets (#7392) [FEATURE] Introducing CapitalOne DataProfilerColumnDomainBuilder as well as multiple improvements to CapitalOne codebase and testability. (#7498) [BUGFIX] Repair SparkSession initialization behavior to disallow restarting, unless explicitly instructed through configuration (#7444) [BUGFIX] Skip dialect specific tests if no flag passed or flag not available (#7443) [BUGFIX] Ensure that MetricStore Records \"data_asset_name\" Properly (#7458) [BUGFIX] Fix incorrect type hint and correct typographical errors in DomainBuilder docstrings and fill in missing docstrings (#7467) [BUGFIX] Reset Metrics Registry in order to keep state of Metrics Registry test cases Runs mutually consistent (#7473) [BUGFIX] Corrected typographical errors in two docstrings (#7506) [BUGFIX] Typo in min versions install (#7516) [DOCS] New ADR proposal for patch version support (#7451) [DOCS] Remove outdated instructions for building documentation (#7457) [DOCS] Updating the docs to gx_dev (#7455) [DOCS] Fixes typo in code snippet for \"How to organize Batches in a file-based Data Asset\" guide (#7465) [DOCS] Postgresql drivername fix for SQLAlchemy compatibility. Closes #7464 (#7466) (thanks @Itzblend) [DOCS] Corrects code snippets (#7470) [DOCS] Add a note about installing version of pyspark that matches Spark version (#7483) [DOCS] adding cloud language to quickstart (#7484) [DOCS] doc-409 Corrects links in databricks guide (#7485) [DOCS] Doc-472 Corrects numbering of steps in guide (#7478) [DOCS] DOC-474 Updates URL for usage statistics page (#7477) [DOCS] Testing ADR (#7495) [DOCS] corrects typo in GCS setup guide (#7514) [MAINTENANCE] Case whens argument change for SQLAlchemy 2.0 compatibility (#7416) [MAINTENANCE] mypy 1.1.1 update (#7452) [MAINTENANCE] Remove v2 api CLI (#7440) [MAINTENANCE] Bump http-cache-semantics from 4.1.0 to 4.1.1 in /docs/docusaurus (#7447) [MAINTENANCE] Updating language per issue 3572 (#7456) [MAINTENANCE] Remove v2 api expectations tests (#7439) [MAINTENANCE] Move docs_link_checker.py and create new docs-specific pipeline (#7422) [MAINTENANCE] select call style change for SQLAlchemy 2 compatibility (#7378) [MAINTENANCE] Decruft map_metric_provider.py (#7460) [MAINTENANCE] Add tests for _register_metric_functions in the MetricProvider class hierarchy. (#7459) [MAINTENANCE] SqlAlchemy2 Compatibility - implicit autocommit (#7400) [MAINTENANCE] Bump ua-parser-js from 0.7.31 to 0.7.34 in /docs/docusaurus (#7474) [MAINTENANCE] Remove SQLAlchemyDataset/Data Source (#7471) [MAINTENANCE] Bump json5 from 1.0.1 to 1.0.2 in /docs/docusaurus (#7475) [MAINTENANCE] Bump @sideway/formula from 3.0.0 to 3.0.1 in /docs/docusaurus (#7487) [MAINTENANCE] Bump ipython from 7.31.1 to 8.10.0 in /docs_rtd (#7491) [MAINTENANCE] Bump cross-fetch from 3.1.4 to 3.1.5 in /docs/docusaurus (#7488) [MAINTENANCE] Deprecated API features detected warning for SQLAlchemy 2.0 compatibility (#7490) [MAINTENANCE] type-checking implementation files (#7454) [MAINTENANCE] Small Refactor of ColumnDomainBuilder for code elegance and computational performance improvements (#7492) [MAINTENANCE] Lower pydantic requirement to v1.9.2 or greater (#7482) [MAINTENANCE] Connection.connect warning for SQLAlchemy 2.0 compatibility (#7489) [MAINTENANCE] Pass PandasDatasource batch_metadata as kwargs to remove possibility of None on DataAsset model (#7503) [MAINTENANCE] Make dataset_name a parameter for Expectations tests or create name from Expectation name, which ensures only limited number of tables created. (#7476) [MAINTENANCE] Fix sqlalchemy warnings for pandas + sql fluent datasources (#7504) [MAINTENANCE] Bump gitpython from 3.1.7 to 3.1.30 in /docs_rtd (#7494) [MAINTENANCE] Bump certifi from 2020.6.20 to 2022.12.7 in /docs_rtd (#7497) [MAINTENANCE] Bump jupyter-core from 4.6.3 to 4.11.2 in /docs_rtd (#7496) [MAINTENANCE] Bump nbconvert from 5.6.1 to 6.5.1 in /docs_rtd (#7508) [MAINTENANCE] Bump numpy from 1.21.0 to 1.22.0 in /docs_rtd (#7509) [MAINTENANCE] Revert PR 7490 (#7515) [MAINTENANCE] Use YAMLHandler in tests and docs (#7507) [MAINTENANCE] Dedicated airflow 2.2.0 async test (#7518) [MAINTENANCE] Remove airflow2 min depdency test. (#7524) [CONTRIB] - Add new column expectation not be null and empty (#7449) (thanks @tmilitino)  0.16.3  [BUGFIX] Fix LegacyRow import. (#7446)  0.16.2  [FEATURE] Develop PandasDBFSDatasource (as part of Fluent Datasources) (#7372) [FEATURE] Make SQL datasources add asset methods public. (#7387) [FEATURE] Develop SparkDBFSDatasource (as part of Fluent Datasources) (#7380) [FEATURE] add optional id to Fluent Datasources and DataAsset schemas (#7334) [FEATURE] Fluent SQLDatasource accepts arbitrary kwargs (#7394) [FEATURE] Fluent SQLDatasource create_temp_table (#7407) [FEATURE] F/great 1463/add updates with datasource obj (#7401) [FEATURE] Fluent SparkDataframeDatasource with DataframeDataAsset (#7425) [BUGFIX] Add FluentBatchRequest early exit to convert_to_json_serializable (#7381) [BUGFIX] Handle non-string fluent batch request options in convert_to_json_serializable (#7386) [BUGFIX] Fix bug with case sensitive execution env (#7393) [BUGFIX] Fixing typographical errors and argument omissions (#7398) [BUGFIX] Remove Query ID from exception for query.template_values metric (#7373) (thanks @itaise) [BUGFIX] Fluent PandasFilesytemDatasources data_connector fixes (#7414) [DOCS] Add back GX logo to README (#7391) [DOCS] DOC-473: Adds shared components for fluent and state management updates (#7404) [DOCS] DOC-473 Adds guide \"How to set up GX to work with SQL databases\" (#7409) [DOCS] DOC-473 Adds guide \"How to set up GX to work with data on GCS\" (#7408) [DOCS] Pending doc updates for Data Context state management and fluent Data Source configuration (#7301) [DOCS] Put data_context.md code examples under test (#7417) [MAINTENANCE] Make sure sqlalchemy 2.0 warnings are emitted when running pipelines (#7379) [MAINTENANCE] Make sure all existing warnings are ignored in full CI pipeline (#7389) [MAINTENANCE] Add PR title checker GitHub Action (#7365) [MAINTENANCE] Re-enable warnings as errors (#7383) [MAINTENANCE] Test against minimum SQLAlchemy versions (#7396) [MAINTENANCE] : split up map_metric_provider.py (#7402) [MAINTENANCE] Consolidate Cloud tutorials (#7395) [MAINTENANCE] Change for connection.execute() for SQLAlchemy 2 compatibility (#7384) [MAINTENANCE] Bump SQLAlchemy lower bound to 1.4.0 (#7413) [MAINTENANCE] Validate ExpectationConfiguration before adding to suite (#7366) [MAINTENANCE] Convert to python built in warning categories (#7415) [MAINTENANCE] Move NotImported to optional_imports.py and start using it. (#7421) [MAINTENANCE] Bump minimist from 1.2.5 to 1.2.8 in /docs/docusaurus (#7419) [MAINTENANCE] PandasFilesystemDatasource stubs and Schema corrections (#7428) [MAINTENANCE] Bump loader-utils from 2.0.2 to 2.0.4 in /docs/docusaurus (#7429) [MAINTENANCE] Bump webpack from 5.74.0 to 5.76.3 in /docs/docusaurus (#7430) [MAINTENANCE] Fix some failing tests when running pytest with no args locally (#7426) [MAINTENANCE] only provide py.typed files for fully typed sub-packages (#7438) [MAINTENANCE] Some fluent datasource methods should be private (#7437) [CONTRIB] Adding support for date for the row condition parser (#7359) (thanks @maayaniti) [CONTRIB] Limit results for two expectations (#7403) (thanks @itaise) [CONTRIB] [MAINTENANCE] Custom query expectation and editing query.template_values metric (#7390) (thanks @mantasmy)  0.16.1  [FEATURE] Fluent CRUD operation stubs (#7347) [FEATURE] Implement DataBricks (DBFS) DataConnector for Fluent Datasources needs (#7355) [BUGFIX] BigQuery performance uses updated add_or_update_expectation_suite() method (#7325) [BUGFIX] Ensure Correct JSON and Dictionary Dump Output of Serialized Fluent Objects (#7336) [BUGFIX] Fluent Datasources - empty \"assets\" key on serialization (#7341) [BUGFIX] Use Path().glob Instead of Path(). Issue #7239 (#7327) (thanks @richardohara) [BUGFIX] Patch misc errors in advance of v0.16.1 release (#7371) [BUGFIX] #ephemeral_data_asset broken data docs links (#7367) [BUGFIX] Fluent batch_request_options ignore None values (#7368) [BUGFIX] Add FluentBatchRequest early exit to convert_to_json_serializable (#7381) [DOCS] Adding ADR (#7314) [DOCS] Add Cloud onboarding tutorial (#7333) [DOCS] Fix onboarding cloud tutorial (#7348) [DOCS] SQL Alchemy 2 Warnigns (#7292) [DOCS] Update Cloud Getting Started Guide (#7364) [MAINTENANCE] In Fluent Datasources: Configuration Serialization Methods Do Not Need to be Public (#7332) [MAINTENANCE] Add crud methods to context.sources (#7328) [MAINTENANCE] Split out mysql and multi db docs integration tests into a separate job (#7331) [MAINTENANCE] [MAINTENANCE ] Include all .pyi files in package data (#7340) [MAINTENANCE] Separate docs_integrations tests by backend (#7342) [MAINTENANCE] Parallelize integration tests - move test definitions (#7345) [MAINTENANCE] Update test around unneeded deprecation warning threshold (#7351) [MAINTENANCE] Remove deprecated code from Validator (#7353) [MAINTENANCE] ruff 0.0.255 (#7349) [MAINTENANCE] add sqla deps to type-checking CI step (#7350) [MAINTENANCE] Remove deprecated code around generator_asset (#7356) [MAINTENANCE] Revert accidental changes pushed to develop (#7363) [MAINTENANCE] D/sql alchemy 2 warnings (#7362) [MAINTENANCE] Warnings are errors + Suppress warnings in preparation for supporting SQLAlchemy 2.x (#7352) [MAINTENANCE] Clean up miscellaneous deprecated code (#7358) [MAINTENANCE] Rename misc internal uses of ge_cloud_id (#7360) [MAINTENANCE] Patch deprecation warning in EVR.__eq__ (#7374) [MAINTENANCE] remove imprecise wording in the new datasource notebook (#7369) [MAINTENANCE] Add additional warning ignores to pyproject.toml (#7375) [MAINTENANCE] Enable snowflake V3 Expectations Tests (#7370) [MAINTENANCE] Temporarily disable warnings as errors for v0.16.1 release (#7382)  0.16.0  [FEATURE] Provide S3 access/operations to new PandasDatasource (#7197) [FEATURE] F/great 1393/add more zep column splitters (#7203) [FEATURE] Fluent datasources have separate asset registries (#7193) [FEATURE] Improve Trino types support (#6588) (thanks @ms32035) [FEATURE] F/great 1393/add sql multi col splitter (#7236) [FEATURE] EphemeralDataContext.convert_to_file_context() (#7175) [FEATURE] New Datasources: Support S3 access for SparkDatasource (#7232) [FEATURE] Implementation of Microsoft Azure Blob Storage DataConnector (#7246) [FEATURE] Implementation of Google Cloud Storage DataConnector (#7249) [FEATURE] Develop PandasGoogleCloudStorageDatasource (as part of New Datasources) (#7253) [FEATURE] Develop PandasAzureBlobStorageDatasource (as part of New Datasources) (#7254) [FEATURE] Develop SparkGoogleCloudStorageDatasource SparkAzureBlobStorageDatasource (as part of New Datasources) (#7255) [FEATURE] Enable creating Checkpoint and SimpleCheckpoint with Validator (#7275) [FEATURE] Editor support for Fluent Datasources dynamic .add_<DS_TYPE>() methods (#7273) [FEATURE] M/great 1708/move fluent datasources out of experimental (#7294) [FEATURE] PandasDatasource in-memory DataFrameAsset (#7280) [FEATURE] Fluent Datasources - ConfigStr config substitution support (#7309) [BUGFIX] Ensure that Marshmallow UUIDs are converted to strings before usage (#7219) [BUGFIX] docs-integration tests failing after CRUD updates (#7220) [BUGFIX] Adding exception logging to store-related failures (#7202) (thanks @ciguaran) [BUGFIX] Patch outdated Azure relative paths (#7247) [BUGFIX] params_with_json_schema is undefined in ExpectSelectColumnValuesToBeUniqueWithinRecord._prescriptive_renderer (#7261) [BUGFIX] Patch Cloud workflow for add_or_update_datasource (#7242) [BUGFIX] Stop modifying input batch request when getting a batch list. (#7269) [BUGFIX] Single dynamic pandas model failures don't result in all models failing when used (#7277) [BUGFIX] Fix typographical error in type used in \"isinstance()\" comparison in BatchManager (was generating \"must be formal [...] Batch\" warning erroneously) (#7291) [BUGFIX] Correct a typographical error in specific path-forming template name for SparkDatasource-Microsoft Azure Blob Storage combination (#7302) [BUGFIX] Use timezone-aware datetimes in checkpoint run information by default. (#7244) [BUGFIX] row_conditions now allow whitespace (#7313) [BUGFIX] Scope PandasDatasource reader method types and serialization (1 of 2) (#7306) [BUGFIX] AssertError Duing Great Expectations Installation (#7285) (thanks @richardohara) [BUGFIX] Provide Error Checking for \"expectation_suite_name\" in Checkpoint Constructor and Run Methods (#7320) [BUGFIX] Scope PandasDatasource reader method types and serialization (2 of 2) (#7318) [BUGFIX] raise error if no identifier provided to get checkpoint/expectation suite (#7321) [BUGFIX] Fluent Datasources: Ensure that Data Source and DataAsset Model Serialization Does Not Include \"name\" Field (#7323) [DOCS] Versioning instructions and fix references for versioning (#7221) [DOCS] Fix links to images in welcome page (#7222) [DOCS] Add API docs build to versioning instructions (#7230) [DOCS] Move readme_assets under docs/ (#7231) [DOCS] Fix hyperlinks in docs (#7264) [DOCS] Scan yaml examples for class names that should be part of the public API (#7267) [DOCS] Update location to run yarn commands (#7276) [DOCS] Fix reference to M1 Mac installation guide (#7279) [DOCS] Remove additional warnings from API docs build. (#7282) [DOCS] Fix blog links in parse_strings_as_datetimes warnings (#7288) [DOCS] Fix unsupported chars in API docs (#7310) [DOCS] Add instructions to cli for creating data assets (#7326) [MAINTENANCE] No line number snippets checker (#7216) [MAINTENANCE] Re-enable snippet checker (#7217) [MAINTENANCE] Refactor New SparkDatasource in order to prepare for adding support for Cloud Files Storage environments (#7223) [MAINTENANCE] Repo Cleanup: Move docs related files under docs dir (#7227) [MAINTENANCE] Time series expectations (#7182) [MAINTENANCE] Contrib  time series generators (#7235) [MAINTENANCE] ci & local type-checking parity - pyspark (#7237) [MAINTENANCE] Fix CI dev pipeline - skip orphaned schema check on min pandas version (#7241) [MAINTENANCE] Make a copy of azure pipeline definition files in new locations (#7233) [MAINTENANCE] Complete migration of Azure CI/CD config to ci directory (#7245) [MAINTENANCE] ruff v0.0.253 - enable PYI rules (#7243) [MAINTENANCE] Optionally allow None values in RenderedAtomicContent (#7240) [MAINTENANCE] PandasDatasource read_DataAsset methods return a Validator (#7226) [MAINTENANCE] Rename default pandas ephemeral data asset (#7251) [MAINTENANCE] Fix linting errors in contrib/ (#7259) [MAINTENANCE] Keep repo root consistent (#7260) [MAINTENANCE] Threshold for public API report (#7262) [MAINTENANCE] Update the signature of Fluent datasource factory methods (#7266) [MAINTENANCE] Move some scripts into ci folder (#7271) [MAINTENANCE] _PandasDataAsset positional argument support (#7258) [MAINTENANCE] Refactor to utilize common DataConnector instantiation and Connection Test Error Message templates for New FilePath Datasources (#7268) [MAINTENANCE] Merge PandasDatasource read method signatures with associated _PandasDataAssets (#7272) [MAINTENANCE] Use ruff linting on scripts folder (#7270) [MAINTENANCE] Fix for docs build warnings (#7229) [MAINTENANCE] Add maybe imports (#7274) [MAINTENANCE] Proper mocking of AbstractDataContext for testing (#7281) [MAINTENANCE] Warn if sqlalchemy version is >= 2.0.0 (#7283) [MAINTENANCE] Remove Prefix \"Batch\" from \"BatchSorter\" and \"BatchSortersDefinition\" (in New Datasources) (#7284) [MAINTENANCE] Call the correct sqlalchemy method to eliminate deprecation warning (#7293) [MAINTENANCE] Fluent Datasources - initial snippet tests (#7278) [MAINTENANCE] add ruff PTH use-pathlib # noqa comments (#7290) [MAINTENANCE] Enable SparkDatasource add_asset() methods to accept \"header\" and \"infer_schema\" arguments; fix typos in docstrings. (#7296) [MAINTENANCE] enable ruff use-pathlib PTH rules (#7297) [MAINTENANCE] Replace fluent data connector names with a constant (#7299) [MAINTENANCE] Apply ruff path rules for contrib (#7300) [MAINTENANCE] Rename column splitter to splitter. (#7303) [MAINTENANCE] Run airflow operator tests in GX pipeline (#7298) [MAINTENANCE] Add build docs to block PR pipeline (#7307) [MAINTENANCE] Linting for tests/datasource (#7308) [MAINTENANCE] Fix test_build_docs stage (#7311) [MAINTENANCE] Elide Duplication of \"name\" Key from Fluent Data Source and DataAsset Configuration (#7312) [MAINTENANCE] Remove nonexistent pypi ref in README. Add WIP warning in README. (#7304) [MAINTENANCE] Fix a bug in the hourly generator. Add hourly functionality to genera\u2026 (#7305) [MAINTENANCE] Update allowed deprecation warning threshold in advance of 0.16.0 (#7317) [MAINTENANCE] Fluent Data Source stubs for dynamic Pandas add_asset methods (#7315) [MAINTENANCE] Stub files for Fluent PandasDatasource and py.typed (#7322) [MAINTENANCE] row-condition also includes tabs in conditional expectation (#7324) [CONTRIB] -add athena credential support (#7186) (thanks @tmilitino) [CONTRIB] expect_multicolumn_sum_values_to_be_equal_to_single_column (#7224) (thanks @swittchawa) [CONTRIB] [BUGFIX] Update custom multi-column expectation logic and default kwargs (#7252) (thanks @yussaaa) [CONTRIB] Feature/expect column values to match thai (#7238) (thanks @swittchawa)  0.15.50  [FEATURE] Utilize DataConnector in service of new DataAsset implementations (#7094) [FEATURE] F/great 1393/add initial non datetime sql splitters (#7183) [FEATURE] Experimental PandasDatasource with single-batch _PandasDataAssets (#7173) [FEATURE] Experimental filesystem DataAssets path in batch request options and batch metadata (#7129) [FEATURE] Default PandasDatasource (#7196) [BUGFIX] : Allow CLI to work with RuntimeDataConnector (#7187) (thanks @luke321321) [BUGFIX] Patch GX Cloud validator.save_expectation_suite() workflow (#7189) [BUGFIX] Dynamic pandas asset model field substitution (#7212) [DOCS] Use named snippets part 5 (#7181) [DOCS] Use named snippets part 4 (#7176) [DOCS] Use named snippets part 6 (#7171) [DOCS] Use named snippets part 7 (#7192) [DOCS] Use named snippets part 9 (#7195) [DOCS] Use named snippets part 11 (#7201) [DOCS] Use named snippets part 8 (#7194) [DOCS] Use named snippets part 10 (#7199) [DOCS] Fix broken link in anonymous_usage_statistics.md (#7211) (thanks @Erin-GX) [DOCS] Use named snippets part 12 (#7214) [MAINTENANCE] Update changelog when updated release 0.15.49 (#7179) [MAINTENANCE] Raise store config ClassInstantiationError from original DataContextError (#7174) [MAINTENANCE] New Datasources: Delineate \"SparkFilesystemDatasource\" (instead of \"SparkDatasource\") -- to be congruent with \"PandasFilesystemDatasource\" (#7178) [MAINTENANCE] Small Refactor to Enable PandasDatsource for multiple Storage Environments (#7190) [MAINTENANCE] Replace regex with batching_regex for fluent filesystem-like datasources (#7207) [MAINTENANCE] Misc DataContext state management & API cleanup (#7215)  0.15.49  [FEATURE] Enable customization of candidate Regular Expression patterns when running OnboardingDataAssistant (#7104) [FEATURE] Enable gx.get_context() to work without any inputs (#7074) [FEATURE] Add datasource arg to DataContext Datasource CRUD (#7070) [FEATURE] Update zep to use sqlalchemy_data_splitter.py (#7151) [FEATURE] ZEP - Dynamically define add_<ASSET_TYPE>_asset() methods if needed (#7121) [FEATURE] add expectation_column_values_to_be_continuous (#5861) (thanks @jmoskovc) [BUGFIX] Rename experimental get_batch_request to build_batch_request (#7107) [BUGFIX] Remove version from versions.json (#7109) [BUGFIX] Properly Enable/Disable Spark Integration Tests Depending on pyspark Installation for New Datasources (#7132) [BUGFIX] Copy previous versions after checking out the current commit (#7142) [BUGFIX] TupleAzureBlobStoreBackend no longer gives warning when obfuscating connection string (#7139) [BUGFIX] Patch inconsistent ordering within GCP test asserts (#7130) [BUGFIX] Parse pandas version correctly for development builds (#7147) (thanks @jtilly) [BUGFIX] Patch broken rendered content Cloud tests (#7155) [BUGFIX] pydantic>=1.10.4 - ImportError: cannot import name dataclass_transform (#7163) [BUGFIX] ID/PK Spark and Sql fall back when unexpected_index_column_names have not been defined (#7150) [BUGFIX] Patch broken Cloud test blocking 0.15.49 release (#7177) [DOCS] Add CRUD API matrix to AbstractDataContext docstring (#7079) [DOCS] Build API docs using latest released version (#7067) [DOCS] Add displayHTML method to view Data Docs (#7125) (thanks @swittchawa) [DOCS] Use named snippets part 1 (#7131) [DOCS] : fix capitalization of Slack (#7136) (thanks @JoelGritter) [DOCS] Remove sitemap.xml (#7141) [DOCS] doc-464 consolidating and standardizing snippets (#7154) [DOCS] Use named snippets part 2 (#7143) [DOCS] Use named snippets part 3 (#7169) (thanks @jmoskovc) [MAINTENANCE] Remove Extra Character from ID/PK Example README (#7098) [MAINTENANCE] Rename experimental get_batch_request to build_batch_request (#7095) [MAINTENANCE] Fix incorrect label on \"How to configure a SQL Data Source\" docs page (#7106) [MAINTENANCE] Update dependency on pydantic (#7111) [MAINTENANCE] Move experimental base_directory from _FilesystemDataAsset to PandasDatasource and SparkDatasource (#7078) [MAINTENANCE] Use secret to store algolia api key (#7115) [MAINTENANCE] Fluent Datasources - don't register \"private\" Data Source classes (#7124) [MAINTENANCE] ZEP - Realign pandas asset args for Datasource level base_directory (#7123) [MAINTENANCE] format notebooks with black (#7054) [MAINTENANCE] mypy v1.0.0 (#7138) [MAINTENANCE] Output Consistent Data Format from \"table.head\" Metric for every ExecutionEngine (#7134) [MAINTENANCE] ruff 0.0.246 update (#7137) [MAINTENANCE] Refactor sql splitter to take selectable instead of str. (#7133) [MAINTENANCE] Update V3 DataConnector utilities to support New Datasources (ZEP) (#7144) [MAINTENANCE] Change all instances of create_expectation_suite to add_expectation_suite in tests, docs, and source code (#7117) [MAINTENANCE] Clean up pathlib.Path() usage in DataConnector utilities and restore tighter formatting in great_expectations/util.py (#7149) [MAINTENANCE] Clean up mypy violations in CardinalityChecker (#7146) [MAINTENANCE] Remove unused dockerfile (#7152) [MAINTENANCE] Delete cli v012 tests. (#7159) [MAINTENANCE] ZEP - update asset factories method signatures from asset models (#7096) [MAINTENANCE] Bump minimum version of pytest (#7164) [MAINTENANCE] Clean up additional deprecation warnings from outdated CRUD API (#7156) [MAINTENANCE] Experimental PandasDatasource, single-batch _PandasDataAssets, related schemas (#7158) [MAINTENANCE] Removing path for --v2-api upgrade and informative message (#7170) [CONTRIB] Add experimental expectation to check column values after split (#7120) (thanks @ace-racer) [CONTRIB] added new Expectations  - India_zip_code expectation and not_to_be_future_date expectation (#6086) (thanks @prachijain136) [CONTRIB] Update the rendered text for min and max values to be clearer. (#7166)  0.15.48  [FEATURE] Place FilesystemDataAsset into separate module (its functionality is used by both PandasDatasource and SparkDatasource) (#7025) [FEATURE] Add SQL query data asset for new experimental datasources (#6999) [FEATURE] Experimental DataAsset test_connection (#7019) [FEATURE] ZEP - generate pandas assets (#7044) [FEATURE] Experimental Splitter connection testing (#7051) [FEATURE] ID/PK ColumnPairExpectations and MultiColumnMapExpectations - Spark (#7001) [FEATURE] Add expectation_suite arg to DataContext ExpectationSuite CRUD (#7059) [FEATURE] ID/PK ColumnPairExpectations and MultiColumnMapExpectations - SQL (#7046) [FEATURE] Introducing General-Purpose Wrapper for Regular Expressions Parsing and incorporating it in \"_FilesystemDataAsset\" (#7062) [FEATURE] Add checkpoint arg to DataContext Checkpoint CRUD (#7066) [FEATURE] Add profiler arg to DataContext Profiler CRUD (#7060) [FEATURE] Add API action (#6902) (thanks @itaise) [BUGFIX] ID/PK - Rendering ColumnPair and MultiColumn Expectations in DataDocs (#7041) [BUGFIX] ColumnPairExpectation tests need to consider 2 possible GroupBy results (#7045) [BUGFIX] zep - always serialize type field (#7056) [BUGFIX] ZEP - html asset generation on pandas 1.1 (#7068) [BUGFIX] fix ZEP pandas min tests (#7084) [BUGFIX] Skip all ZEP pandas datasource tests for min pandas (#7091) [DOCS] Add new DataContext CRUD to public API (#7058) [DOCS] DOC-461 remove unlinked link (#7083) [DOCS] Adding algolia click events (#7085) [DOCS] Versioning for documentation (#7033) [MAINTENANCE] linting for /contrib (#7005) [MAINTENANCE] Deprecate old DataContext CRUD methods (#7031) [MAINTENANCE] Simplify logic for add_or_update (#7035) [MAINTENANCE] ZEP - test type-checking (#7028) [MAINTENANCE] ZEP put schemas under source control (#6988) [MAINTENANCE] Add id as a param to any CRUD methods with ge_cloud_id (#7036) [MAINTENANCE] Minor Cleanup (#7047) [MAINTENANCE] replace isort with ruff sorting rules (#6907) [MAINTENANCE] Standardize Checkpoint CRUD (#6962) [MAINTENANCE] ruff 0.0.241 (#7048) [MAINTENANCE] finish linting great_expectations (#7050) [MAINTENANCE] fix - FutureWarning: pandas.Float64Index is deprecated when importing great_expectations (#7055) [MAINTENANCE] Move path mapping out of ExecutionEngine into DataConnector (#7065) [MAINTENANCE] Add ruff TCH001 noqa annotations (#7072) [MAINTENANCE] Enable ruff TCH rules (#7073) [MAINTENANCE] Cache dependency installation during build of Dockerfile.tests (#7071) [MAINTENANCE] Fix most typing errors in DataAssistantResult (#7010) [MAINTENANCE] add ge_dev + gx_dev to .gitignore (#6860) [MAINTENANCE] Refactor Filesystem DataAsset into FilePath DataAsset and Filesystem DataAsset (later inherits former) (#7075) [MAINTENANCE] Refactor Store and StoreBackend to leverage new CRUD methods (#7081) [MAINTENANCE] Delete gx_venv from project root (#7080) [MAINTENANCE] correct typo in data assistants portion of the getting started tutorial (#7088) [MAINTENANCE] ID/PK - CompoundColumnsUnique is filtered only for SQL (#7087) [MAINTENANCE] Minor clean up to make DataConnector method names less confusing. (#7089) [MAINTENANCE] Refactor DataConnector Path Format Utilities For Better Encapsulation (#7092) [MAINTENANCE] Temporarily xfail ID/PK tests due to Pandas min version conflicts (#7097) [MAINTENANCE] Remove Extra Character from ID/PK Example README (#7098) [CONTRIB] Row condition parser sqlalchemy: adding support for != operator & adding support all operators for string (#7053) (thanks @maayaniti)  0.15.47  [FEATURE] ZEP - dynamic pandas asset schema definitions (#6780) [FEATURE] ID/PK ColumnPairExpectations and MultiColumnMapExpectations - Pandas (#6941) [FEATURE] Experimental Datasource and DataAsset connection testing (#6844) [FEATURE] Implement Experimental SparkDatasource with CSVDataAsset (#6981) [FEATURE] Place FilesystemDataAsset into separate module (its functionality is used by both PandasDatasource and SparkDatasource) (#7025) [BUGFIX] Snowflake/Oracle/DB2 <--> SQLAlchemy table and column names case insensitivity representation (#6951) [BUGFIX] try except import of pandas types (#6983) [BUGFIX] fix jsonschema - altair conflict (#6984) [BUGFIX] Temporarily disable items with issues rendering (#6997) [BUGFIX] Fix Renderer Configuration for expectation expect_column_values_to_not_be_in_set #6963 (#6990) (thanks @jmcorreia) [BUGFIX] Patch logic error in new add_or_update methods (#7021) [BUGFIX] Pandas ID/PK - bugfix for column name and update tests (#7015) [DOCS] Regex-Based, Set-Based, Query-Based, & Actions Docstrings (#6863) [DOCS] Documentation for classes and methods within ExecutionEngine class hierarchy (#6936) [DOCS] Enable use of code blocks in Returns: section (#6946) [DOCS] Add missing data connectors and data contexts (#6945) [DOCS] DOC-280: How to use GX with AWS S3 and Spark (#6782) [DOCS] Adding docstrings per the list (#6931) [DOCS] Docstrings for DataContext child classes and DataAssistantResult.to_json_dict (#6956) [DOCS] batch docstring (#6939) [DOCS] BatchDefinition (#6940) [DOCS] Add metric provider to public api report (#6958) [DOCS] BatchRequest (#6943) [DOCS] head (#6944) [DOCS] Add public_api. Docstring is fine already (#6955) [DOCS] Add public API docstring for RuleBasedProfiler (#6947) [DOCS] Adding docstrings for metric providers (#6960) [DOCS] Add docstrings for several data connectors (#6949) [DOCS] Adds docstring to class configured data connector classes (#6961) [DOCS] Add public API docstring for validate_configuration on expect_column_value_z_scores_to_be_less_than and expect_column_values_to_match_json_schema (#6873) [DOCS] Expectations Class DocStrings (#6950) [DOCS] D/dx 237/tal docstrings (#6959) [DOCS] Add delete_checkpoint to public API (#6965) [DOCS] Use markdown style code blocks (#6970) [DOCS] DataContext and CheckpointConfig DocString (#6911) [DOCS] Either Documentation tag style acceptable (#6974) [DOCS] DocStrings for Column, Query, & Table Metric Providers & register_metric (#6971) [DOCS] render utils (#6975) [DOCS] Add public API docstring for UserConfigurableProfiler (#6904) [DOCS] Add docstring for ExpectationValidationResult (#6968) [DOCS] Add some json serialization docstrings. (#6880) [DOCS] DOC-285 new guide: how to use self initializing expectations (#5205) [DOCS] DOC-286 how to add support for the auto initializing framework to a custom expectation (#5300) [DOCS] ExpectationConfiguration, get_success_kwargs and validate api docs (#6982) [DOCS] rule_based_profiler_result (#6977) [DOCS] metric_value, metric_partial (#6978) [DOCS] Actions, Checkpoint, ExpectationSuiteValidationResult, RunIdentifier related docstrings (#6986) [DOCS] API docs support self referential links (#6998) [DOCS] Add rendering docstrings (#6992) [DOCS] Expectations related DocStrings (#6994) [DOCS] MetricConfiguration DocString (#6996) [DOCS] Updates typo in prerequisites section (#7004) (thanks @ruankie) [DOCS] Update API docs landing page (#6972) [DOCS] Remove BaseDataContext and DataContext from the public API (#7008) [DOCS] Fix setup instructions for email validation (#7007) (thanks @ruankie) [DOCS] DOC-348 corrects typos in the aws+athena guide intro and congratulations sections (#6989) [DOCS] DOC-420 updates to screenshots (#7012) [DOCS] DOC-416 How to use GX with AWS using Redshift (#6985) [DOCS] Fix metric provider and reorganize sidebar (#7022) [DOCS] Typo - Update api_reference.md (#7024) [DOCS] Nest sidebar by shortest import path (#7032) [MAINTENANCE] Parameterized tests for ID/PK at ColumnMapExpectation level (#6925) [MAINTENANCE] ruff -> 0.0.236 (#6948) [MAINTENANCE] docstring for expect_column_values_to_not_match_regex_list's validate_configuration (#6877) [MAINTENANCE] Remove handrolled linters/checkers from scripts/ and CI (#6964) [MAINTENANCE] Remove refs to old scripts from invoke calls (#6967) [MAINTENANCE] Fix some linting issues (#6973) [MAINTENANCE] Fix variable name error associated with adding typing and docstrings (#6980) [MAINTENANCE] Add test to ensure that all types in the DataContext hierarchy emit expected usage stats (#6915) [MAINTENANCE] Standardize Datasource CRUD (#6892) [MAINTENANCE] Typing histogram_single_batch_parameter_builder (#6916) [MAINTENANCE] Add add_expectation_suite to DataContext CRUD (#6926) [MAINTENANCE] ColumnExpectation, render_evaluation_parameter_string and validate method (#6995) [MAINTENANCE] Add update_expectation_suite and add_or_update_expectation_suite to DataContext CRUD (#6987) [MAINTENANCE] Standardize RuleBasedProfiler CRUD (#6991) [MAINTENANCE] Make Pandas installation with Python 3.10 less restrictive (#7013) [MAINTENANCE] ZEP - postgres test typing (#7023) [MAINTENANCE] [BUGFIX ] ZEP - pandas serde fix (#7009) [MAINTENANCE] add preview image for twitter and other social preview images (#7027) [MAINTENANCE] Update update_ methods in DataContext to return persisted object (#7034) [MAINTENANCE] ZEP - use parameter that exists on min pandas version (#7037) [MAINTENANCE] xfail ZEP async spark tests for release (#7038) [CONTRIB] expect_multicolumn_values_not_to_be_all_null (#6912) (thanks @yussaaa) [CONTRIB] Adding support for punctuation in column_value for the row_condition parser (#7018) (thanks @maayaniti)  0.15.46  [BUGFIX] Disable RendererConfiguration constraint to support legacy renderer fallback behavior (#6938) [DOCS] Remove the great_expectations path prefix for API docs (#6934) [DOCS] Updates Custom Expectation docs w/ code snippets (#6365) [DOCS] Regex-Based, Set-Based, Query-Based, & Actions Docstrings (#6863) [DOCS] Documentation for classes and methods within ExecutionEngine class hierarchy (#6936)  0.15.45  [FEATURE] Experimental datasources batch.head() (#6765) [FEATURE] Add Validation Result URL to Checkpoint Result (#6908) [BUGFIX] Fix issues rendering code blocks in API docs (#6917) [BUGFIX] Fix list_keys method for TupleS3StoreBackend (#6901) (thanks @enagovitsyn) [BUGFIX] Fix rendering issue with api docs (#6924) [BUGFIX] Render bar graph with boolean values (#6910) (thanks @tmilitino) [BUGFIX] Capital one contrib/micdavis/import hotfix (#6922) (thanks @micdavis) [DOCS] Adding docstring for Checkpoint.self_check() (#6841) [DOCS] AbstractDataContext.add_store docstring (#6851) [DOCS] Doc Strings for ExpectationSuite Display Methods (#6856) [DOCS] DataAssistantResult.get_expectation_suite() docstring (#6862) [DOCS] Misc docstrings around DataAssistant (#6866) [DOCS] enable running invoke docstrings on select modules (#6868) [DOCS] Adds docstring for expect_column_distinct_values_to_contain_set (#6855) [DOCS] Documentation Strings for Metric Domain Types and Metric Function Types (#6872) [DOCS] added docstrings for the public API (#6884) [DOCS] Add public API docstring for expect_column_values_to_be_unique validate_configuration (#6897) [DOCS] Miscellaneous docstrings for DataContext and utils (#6852) [DOCS] Add public API docstring for expect_column_values_to_be_of_type validate_configuration (#6896) [DOCS] Add public API docstring for JsonSchemaProflier.validate (#6900) [DOCS] Exclude DataAssistantRunner.run() (#6919) [DOCS] StoreValidationResultAction, StoreEvaluationParametersAction and StoreMetricsAction api docs (#6879) [DOCS] Add public API docstring for expect_column_values_to_be_dateutil_parseable validate_configuration (#6864) [DOCS] YAML docs (#6861) [DOCS] Add public API docstring for expect_column_values_to_be_decreasing validate_configuration (#6865) [DOCS] Docstrings for Checkpoint and related classes (#6882) [DOCS] Add public API docstring for expect_table_row_count_to_be_between validate_configuration (#6883) (thanks @lockettks) [DOCS] Validator.get_expectation_suite() docstring (#6886) [DOCS] Fix Checkpoint docstring whitespace (#6927) [DOCS] DataAssistantResult docstring (#6887) [DOCS] Add public API docstring for expect_column_values_to_be_in_set validate_configuration (#6890) [DOCS] Add public API docstring for expect_column_values_to_be_in_type_list and expect_column_values_to_be_increasing validate_configuration (#6891) [DOCS] Deprecate util.render_evaluation_parameter_string function (#6894) [DOCS] Add public API docstring for Profiler.validate (#6898) [DOCS] Add public API docstring for expect_column_values_to_be_between validate_configuration (#6858) [DOCS] Add public API docstring for expect_column_values_to_be_in_json_parseable validate_configuration (#6893) [DOCS] Add public API docstring for expect_column_values_to_be_null validate_configuration (#6895) [DOCS] Update docstrings for some of actions.py (#6853) [DOCS] /typo correction (#6920) (thanks @mingyyy) [DOCS] DOC-417 How to use GX with AWS using Athena (#6828) [DOCS] Adding docstrings (#6854) [MAINTENANCE] Update teams.yml (#6839) [MAINTENANCE] invoke 2.0 and schema task (for zep types) (#6836) [MAINTENANCE] Build hierarchy in sidebars for API docs (#6842) [MAINTENANCE] Change public_api task name to avoid confusion (#6843) [MAINTENANCE] Add the fragment back to internal references (#6845) [MAINTENANCE] Clean up public_api excludes (#6846) [MAINTENANCE] Fix the error message for invalid batch request options (#6848) [MAINTENANCE] Standardize Store CRUD (#6826) [MAINTENANCE] Fix scripts not found error in invoke (#6867) [MAINTENANCE] Fix argument name typo (#6850) (thanks @KirillUlich) [MAINTENANCE] more clearly specifies range of supported python versions (#6870) [MAINTENANCE] add validate_configuration docstring (#6857) [MAINTENANCE] docstring for expect_column_values_to_not_be_null#validate_configuration (#6859) [MAINTENANCE] Standardize project config CRUD (#6837) [MAINTENANCE] update docstring in validator.py and checkpoint_result.py (#6875) [MAINTENANCE] updated docstring on validate configuration (#6871) [MAINTENANCE] Exclude unit tests from comprehensive stage of dev CI (#6903) [MAINTENANCE] Refactor file_relative_path util (#6778) [MAINTENANCE] switch to ruff linter (#6888) [MAINTENANCE] Use docusaurus style code block in api docs (#6906) [MAINTENANCE] metrics linting (#6889) [MAINTENANCE] Add exception message to RenderedAtomicContent failure renderer (#6795) [MAINTENANCE] Remove CloudNotificationAction (#6881) [MAINTENANCE] Use ruff linter for docstring linting (#6913) [MAINTENANCE] Add validate_configuration method docstrings (#6899) [MAINTENANCE] docstring for expect_column_values_to_not_match_like_pattern_list's validate_configuration (#6874) [MAINTENANCE] docstring for expect_column_values_to_not_match_like_pattern validate_configuration (#6876) [MAINTENANCE] docstring for expect_compound_columns_to_be_unique validate_configuration (#6878) [MAINTENANCE] Add docstrings for Validator and its save_expectation_suite and validate methods (#6885) [MAINTENANCE] Type Hints Correction in New Datasources; Additional DocStrings (#6918)  0.15.44  [FEATURE] Add pandas datasource sorter by refactoring into DataAsset (#6787) [FEATURE] ID/PK Demo Files (#6833) [BUGFIX] Fix missing not operator ~ (#6808) [BUGFIX] Implemented lowercase function to check what type of file endswith (#6810) (thanks @tmilitino) [BUGFIX] : expect_day_count_to_be_close_to_equivalent_week_day_mean (#6811) (thanks @HadasManor) [BUGFIX] Pandas ID/PK query was causing DataDocs error (#6832) [DOCS] Link to gh issue #4152 for ruamel.yaml (#6799) (thanks @jamesmyatt) [DOCS] ExpectationSuite and remove_expectation api docs (#6785) [DOCS] Add GitHub PR links to changelogs (#6818) [DOCS] Update yarn-snippet-check to only target specific source code dirs (#6825) [DOCS] Adding docstring for ExpectationSuite.add_expectation (#6829) [DOCS] DOC-394: Fix broken redirect links (#6835) [MAINTENANCE] Enable more backends for some contrib expectations (#6775) [MAINTENANCE] Change execution_engine_type from method to property. (#6788) [MAINTENANCE] More backends for expect_yesterday_count_compared_to_avg_equivalent_days_of_week (#6790) [MAINTENANCE] Update gallery pipeline to only have one scheduled run per day (early AM) (#6791) [MAINTENANCE] Convert the validation results to JSON serializable (#6776) (thanks @lu-lz) [MAINTENANCE] Propagate \"runtime_configuration\" argument throughout Validator flow (#6767) [MAINTENANCE] Only include relevant diagnostics info in gallery JSON (#6797) [MAINTENANCE] Clean up public api report part 1 (#6784) [MAINTENANCE] Clean up public api report part 2 (#6792) [MAINTENANCE] Shift daily gallery run by 6 hours (#6802) [MAINTENANCE] Misc docstrings in AbstractDataContext (#6801) [MAINTENANCE] Add checkpoint and datadoc integration test for zep pandas datasource. (#6793) [MAINTENANCE] Use environment variables for expectation gallery data paths (#6805) [MAINTENANCE] Suppress 2 kl_divergence datasets for bigquery that took 90 minutes to insert (#6807) [MAINTENANCE] Improve type hints in ExecutionEngine.resolve_metrics() flow and delete unnecessary checks (#6804) [MAINTENANCE] Fixes for column_values_to_be_between tests (#6809) [MAINTENANCE] Clean up public api report part 3 (#6803) [MAINTENANCE] Add docstring for AbstractDataContext.add_checkpoint (#6728) [MAINTENANCE] Use Enum classes for all metric name suffixes (#6819) [MAINTENANCE] Use shortened_dotted_paths in API docs (#6820) [MAINTENANCE] Update batch request option validation error message (#6821) [MAINTENANCE] Add docstring to DataAsset.add_sorters (#6822) [MAINTENANCE] Misc type cleanup withincheckpoint/ and validator/ (#6817) [MAINTENANCE] Update algolia indexing (#6827) [MAINTENANCE] When running our test suite, suppress warnings result_format configuration in Expectations and Validators (#6823) [MAINTENANCE] ZEP - lower logging levels from INFO -> DEBUG (#6830) [MAINTENANCE] Use shortened dotted paths in api docs (#6831) [MAINTENANCE] Remove outdated refs to Superconductive (#6816) [CONTRIB] Improve contrib schwifty expectations (#6812) (thanks @mkopec87)  0.15.43  [FEATURE] ZEP - Synchronize & save XDatasources (#6717) [FEATURE] Official Python 3.10 support (#6763) [FEATURE] F/great 1313/zep pandas poc (#6745) [FEATURE] Add GX Cloud hyperlink to slack notification (#6740) [FEATURE] Get ExpectationSuite, Checkpoint by name (#6774) [FEATURE] API docs (#6766) [BUGFIX] - Implementing deep copy of runtime_configuration variable (#6682) (thanks @tmilitino) [BUGFIX] Patch broken test_data_context_ge_cloud_mode_with_incomplete_cloud_config_should_throw_error (#6741) [BUGFIX] reformatting setup.py (#6756) [BUGFIX] Fix observed value (#6759) (thanks @itaise) [BUGFIX] fix comment stripping when saving a zep configuration (#6783) [DOCS] DOC-414: Remove guide for use of outdated docker images (#6718) [DOCS] Convert docs snippets to named snippets (#6735) [DOCS] Update documentation to reference get_context (#6738) [DOCS] Convert remaining snippets to named snippets (#6736) [DOCS] Convert line number references to named references in docs (#6748) [DOCS] Doc-280 AWS golden path with S3 cloud storage and Pandas (#6618) [DOCS] Preparation for building api docs (#6737) [DOCS] Change prefix reference for tutorial folder/directory (#6751) (thanks @medeirosthiago) [DOCS] Fix line-links in 4th step's 5th and 6th block (#6752) (thanks @OnkarMadli) [DOCS] - fixed code reference in documentation (#6732) (thanks @tmilitino) [DOCS] validator.head docstring (#6762) [MAINTENANCE] Update docstrings for experimental SQL datasources. (#6714) [MAINTENANCE] update cli DataContext types (#6703) [MAINTENANCE] Fix missing exclamation marks in API docs admonitions (#6721) [MAINTENANCE] ID/PK Tests at Expectations-level with Warnings caught (#6713) [MAINTENANCE] Refactor tests to leverage get_context instead of BaseDataContext (#6720) [MAINTENANCE] Update remaining atomic prescriptive templates (1 of 2) (#6696) [MAINTENANCE] Refactor tests to leverage get_context instead of DataContext (#6723) [MAINTENANCE] Update remaining atomic prescriptive templates (2 of 2) (#6724) [MAINTENANCE] execution_engine  typing (#6730) [MAINTENANCE] core/expectation_ type checking (#6731) [MAINTENANCE] Remove printing of entire snippet map in remark-named-snippet hook (#6749) [MAINTENANCE] Rename all instances of ge_exceptions to gx_exceptions (#6742) [MAINTENANCE] Remove base_data_context mark in tests (#6750) [MAINTENANCE] Consolidate different Metric Types definition Enums (#6746) [MAINTENANCE] exclude scripts directory from package (#6744) (thanks @cburroughs) [MAINTENANCE] Force cryptography version installed where snowflake runs to be v38.0.4 (#6755) [MAINTENANCE] ID/PK - Adding semi-colon to SQL Query output (#6743) [MAINTENANCE] ID/PK result_format documentation update (#6716) [MAINTENANCE] Consistent use of Metric Name Enum values (#6757) [MAINTENANCE] Fix min version test requirements install on azure async pipeline. (#6753) [MAINTENANCE] expectations linting & bug fixes (#6739) [MAINTENANCE] Add Python 3.10 to async Azure pipeline (#6760) [MAINTENANCE] Unset cloud vars when running pytest from a mac (#6747) [MAINTENANCE] Update CheckChanges for dev pipeline (#6768) [MAINTENANCE] Update contrib pipeline to track package changes (#6770) [MAINTENANCE] Update docstring for AbstractDataContext.add_batch_kwargs_generator (#6727) [MAINTENANCE] Update docstring for add_datasource (#6729) [MAINTENANCE] Remove chunk of code in build_gallery.py to skip processing some Expectations (#6772) [MAINTENANCE] Update docstring for AbstractDataContext.build_data_docs (#6726) [MAINTENANCE] Move constraints-test dir within azure dir to clean up project root (#6764) [MAINTENANCE] Misc cleanup of SerializableDataContext (#6777) [MAINTENANCE] Removed some 30 type hint violations (#6771) [MAINTENANCE] Adjust gallery schedule and timeout (#6781) [MAINTENANCE] type-checking - metrics/util.py (#6754) [MAINTENANCE] Update requirements.txt to reflect 3.10 support (#6786) [MAINTENANCE] Enable more backends for some contrib expectations (#6775) [CONTRIB] added condition to ExpectQueriedColumnListToBeUnique (#6702) (thanks @maayaniti) [CONTRIB] Implement Spark backend for several expectations (#6683) (thanks @mkopec87) [CONTRIB] Improve Spark backend support for contrib query based expectations (#6733) (thanks @mkopec87) [CONTRIB] Refactor ExpectColumnValuesToBeHexadecimal expectation to be RegexBased (#6734) (thanks @mkopec87) [CONTRIB] Fix regex based expectations for spark (#6725) (thanks @mkopec87)  0.15.42  [FEATURE] ZEP - PG BatchSorter loading + dumping (#6580) [FEATURE] Ensure result_format accessed is through Checkpoint, and warns users if Expectation or Validator-level (#6562) [FEATURE] ZEP - PG SqlYearMonthSplitter serialization + deserialization (#6595) [FEATURE] ID/PK - unexpected_index_list updated to include actual unexpected value, and EVR to include unexpected_index_column_names (#6586) [FEATURE] Atomic rendering of Evaluation Parameters (#6232) [FEATURE] Add zep datasource data assistant e2e tests. (#6612) [FEATURE] F/great 1400/sql datasource (#6623) [FEATURE] Accept a pathlib.Path context_root_dir (#6613) [FEATURE] Atomic rendering of meta notes (#6627) [FEATURE] Docstring linter for public api (#6638) [FEATURE]  ZEP - Load/dump new style config from DataContext (#6631) [FEATURE] ID/PK Rendering in DataDocs (#6637) [FEATURE] Use docstring linter for public api to catch missing parameters (#6642) [FEATURE] ZEP - load from shared config (#6655) [FEATURE] Added new expectation: ExpectYesterdayCountComparedToAvgEquivalentDaysOfWeek\u2026 (#6622) (thanks @HadasManor) [FEATURE] Add sqlite datasource (#6657) [FEATURE] ExpectDaySumToBeCloseToEquivalentWeekDayMean (#6664) (thanks @HadasManor) [FEATURE] ID/PK flag return_unexpected_index_query that allows QUERY output to be suppressed (#6660) [FEATURE] F/great 1400/postgres constr (#6674) [FEATURE] Ensure Pandas ID/PK can use named indices (#6656) [FEATURE] Support to include ID/PK in validation result for each row - Spark (#6676) [FEATURE] ID/PK Pandas query returned as all unexpected indices (#6692) [BUGFIX] Support non-string datetime evaluation parameters (#6571) [BUGFIX] Use v3.3.6 or higher of google-cloud-bigquery (with shapely bugfix) (#6590) [BUGFIX] Remove rendered header from Cloud-rendering tests (#6597) [BUGFIX] Patch broken Validator test after DataContext refactor (#6605) [BUGFIX] Column value counts metric dtype inference due to numpy deprecation of object dtype (#6604) [BUGFIX] delete_datasource() was getting passed in incorrect parameter for Spark Docs test (#6601) [BUGFIX] Stop overwriting query with static string in RuntimeBatchRequests for SQL (#6614) [BUGFIX] Simplify metric results processing and improve detection of Decimal types in columns (#6610) [BUGFIX] Do not round output of proportion computing metrics (#6619) [BUGFIX] Add sqrt on connect to sqlite database. (#6635) [BUGFIX] RendererConfiguration min_value and max_value with datetimes (#6632) [BUGFIX] Fix a typo in contrib queried expectation (and add type hints to its implementation) (#6650) [BUGFIX] Avoid key collisions between Rule-Based Profiler terminal literals and MetricConfiguration value keys (#6675) [BUGFIX] Add connect args to execution engine schema (#6663) (thanks @itaise) [BUGFIX] Increase minimum numpy versions for Python 3.8 and 3.9 due to support in latest release of scipy (#6704) [BUGFIX] Format current branch name properly for tag branches in docker tests (#6711) [DOCS] Patch broken snippet around Validation Actions (#6606) [DOCS] Fix formatting of 0.15.35 and 0.15.36 in changelogs (#6603) [DOCS] Convert broken snippet to named snippets (#6611) [DOCS] - add anonymous_usage_statistics configutation in documentation (#6626) (thanks @tmilitino) [DOCS] fixing wrong line reference on docs (#6599) (thanks @wagneralbjr) [DOCS] Sidebar changes for Integrations and how-tos (#6649) [DOCS] edit term(data_conext, checkpoints)-link in with airflow (#6646) (thanks @jx2lee) [DOCS] DOC-400 remove an outdated video link (#6667) [DOCS] doc-356 importable prerequisite box with correct theme applied and default entries (#6666) [MAINTENANCE] mypy config update (#6589) [MAINTENANCE] Small refactor of ExecutionEngine.resolve_metrics() for better code readability (and miscellaneous additional clean up) (#6587) [MAINTENANCE] Remove ExplorerDataContext (#6592) [MAINTENANCE] Leverage RendererConfiguration in existing prescriptive templates (2 of 3) (#6488) [MAINTENANCE] Leverage RendererConfiguration in existing prescriptive templates (3 of 3) (#6530) [MAINTENANCE] Add docs snippet checker to dev CI (#6594) [MAINTENANCE] Utilize a StrEnum for ConfigPeer modes (#6596) [MAINTENANCE] Refactor BaseDataContext and DataContext into factory functions (#6531) [MAINTENANCE] flake8 coverage - render, juptyer_ux, checkpoint sub-packages (#6607) [MAINTENANCE] filter RemovedInMarshmallow4 warnings (#6602) [MAINTENANCE] Generate public API candidates (#6600) [MAINTENANCE] partial cli + usage_stats typing (#6335) [MAINTENANCE] update pip installs in pipelines (#6609) [MAINTENANCE] create cache as part of --ci type-checking step (#6621) [MAINTENANCE] Add row_condition logic to RendererConfiguration and remove from atomic renderer implementations (#6616) [MAINTENANCE] install pydantic in develop pipeline (#6624) [MAINTENANCE] Fix develop static type-check stage (#6628) [MAINTENANCE] Unexpected Counts table in DataDocs able to show counts for sampled values (#6634) [MAINTENANCE] CI - install from requirements-types.txt (#6639) [MAINTENANCE] Add docstring linter for public api to CI (#6641) [MAINTENANCE] Batch ID must incorporate batch_spec_passthrough.  Instantiate Validator with DataContext in test modules.  Query metrics/expectations types cleanup. (#6636) [MAINTENANCE] Skip postgres tests in spark. (#6643) [MAINTENANCE] Enrich RendererConfiguration primitive types (#6629) [MAINTENANCE] M/great 1225/async builds branch (#6644) [MAINTENANCE] Comment out calling _disable_progress_bars in build_gallery.py (#6648) [MAINTENANCE] Update generate_expectation_tests func to only log an ERROR message if there is an error (#6651) [MAINTENANCE] Use the correct test for positive_test__exact_min_and_max on trino (#6652) [MAINTENANCE] Update evaluate_json_test_v3_api func to use the debug_logger with useful info when exception occurs (#6653) [MAINTENANCE] Make only column y data_alt different in col_pair_equal tests (#6661) [MAINTENANCE] Add separate tests for exact stdev for redshift and snowflake (#6658) [MAINTENANCE] Add redshift/snowflake casting for sample data on expect_column_values_to_be_of_type (#6659) [MAINTENANCE] Add redshift/snowflake casting for sample data on expect_column_values_to_be_in_type_list (#6668) [MAINTENANCE] Make only column y data_alt different in col_pair_in_set tests (#6670) [MAINTENANCE] Only spark v2 for special test in not_be_in_set (#6669) [MAINTENANCE] Don't attempt any regex Expectation tests with snowflake (#6672) [MAINTENANCE] Clean up returns style and type hints in CardinalityChecker utility (#6677) [MAINTENANCE] begin flake8 linting on tests (#6679) [MAINTENANCE] Clean up packaging & installation pipeline (#6687) [MAINTENANCE] Misc updates to dev Azure pipeline (#6686) [MAINTENANCE] mypy typing for core/util.py (#6617) [MAINTENANCE] Update get_context return type (#6684) [MAINTENANCE] Get datetime tests working for trino/snowflake/spark in values_to_be_in_set (#6671) [MAINTENANCE] Cleanup typing errors. (#6691) [MAINTENANCE] Remove unused Metric and BatchMetric classes and consolidate ValidationMetricIdentifier with other identifiers (#6693) [MAINTENANCE] Refactor BaseDataContext to leverage get_context (#6689) [MAINTENANCE] expect day count to be close to equivalent week day mean (#6680) (thanks @HadasManor) [MAINTENANCE] ID/PK squashed tests re-added (#6694) [MAINTENANCE] initial type checking for rules_based_profiler (#6681) [MAINTENANCE] Improve type checking for Expectations with atomic renderers leveraging RendererConfiguration (#6633) [MAINTENANCE] Add deprecated Cloud variables to _CloudConfigurationProvider.get_values() output (#6708) [MAINTENANCE] Autobuild markdown stubs (#6700) [MAINTENANCE] API docs styling (#6712)  0.15.41  [FEATURE] enable mostly for expect_compound_columns_to_be_unique (#6533) (thanks @kimfrie) [BUGFIX] Return unique list of batch_definitions (#6579) (thanks @tanelk) [BUGFIX] convert_to_json_serializable does not accept numpy datetime (#6553) (thanks @tmilitino) [DOCS] Clean up misc snippet violations (#6582) [MAINTENANCE] Update json schema validation on usage stats to filter based on format. (#6502) [MAINTENANCE] Renaming Metric Name Suffixes using Enum Values for Better Code Readability (#6575) [MAINTENANCE] M/great 1433/cloud tests to async (#6543) [MAINTENANCE] Add static type checking to rule.py and rule_based_profiler_result.py (#6573) [MAINTENANCE] Update most contrib Expectation docstrings to be consistent and decently formatted for gallery (#6577) [MAINTENANCE] Update changelogs to reflect PyPI yanks (0.15.37-0.15.39) (#6581) [MAINTENANCE] Refactor ExecutionEngine.resolve_metrics() for better code readability (#6578) [MAINTENANCE] adding googletag manager to docusaurus (#6584) [MAINTENANCE] typo in method name (#6585)  0.15.40  [FEATURE] F/great 1397/zep checkpoints (#6525) [FEATURE] Add integration test for zep sqlalchemy datasource with renderering. (#6564) [BUGFIX] Patch additional deprecated call to GXCloudIdentifier.ge_cloud_id attr (#6555) [BUGFIX] Patch packaging_and_installation Azure pipeline test failures (#6559) [BUGFIX] Fix dependency issues to reenable RTD builds (#6560) [BUGFIX] Add missing raise statement in RuntimeDataConnector logic (#6569) [DOCS] doc 383: bring sql datasource configuration examples under test (#6466) [MAINTENANCE] Add error handling to docs snippet checker (#6556) [MAINTENANCE] ID/PK tests at Checkpoint-level (#6539) [MAINTENANCE] Improve DataAssistant Parameter Builder Naming/Sanitization Mechanism and Enhance TableDomainBuilder (#6554) [MAINTENANCE] Simplify computational graph assembly from metric configurations (#6563) [MAINTENANCE] RTD Mobile header brand adjustment (#6557) [MAINTENANCE] Use MetricsCalculator methods for ValidationGraph construction and resolution operations in Validator (#6566) [MAINTENANCE] Cast type in execution_environment.py to bypass flaky mypy warnings (#6572) [MAINTENANCE] Additional patch for mypy issue in execution_environment.py (#6574) [MAINTENANCE] Clean up GX rename artifacts (#6561) [CONTRIB] fix observed value in custom expectation (#6515) (thanks @itaise)  0.15.39 - YANKED  [BUGFIX] Patch faulty GX Cloud arg resolution logic (#6542) [BUGFIX] Fix resolution of cloud variables. (#6546) [DOCS] Fix line numbers in snippets part 2 (#6537) [DOCS] Convert nested snippets to named snippets (#6541) [DOCS] Simplify snippet checker logic to catch stray tags in CI (#6538) [MAINTENANCE] v2 Docs link (#6534) [MAINTENANCE] Fix logic around cloud_mode and ge_cloud_mode. (#6550)  0.15.38 - YANKED  [BUGFIX] Patch broken Cloud E2E test around Datasource CRUD (#6520) [BUGFIX] Patch outdated ge_cloud_id attribute call in ValidationOperator (#6529) [BUGFIX] Revert refactor to Datasource instantiation logic in DataContext (#6535) [BUGFIX] Patch faulty GX Cloud arg resolution logic (#6542) [DOCS] Fix line numbers in snippets (#6536) [DOCS] Fix line numbers in snippets part 2 (#6537) [DOCS] Convert nested snippets to named snippets (#6541) [MAINTENANCE] Update Data Assistant plot images (#6521) [MAINTENANCE] Clean up type hints and make test generation more elegant (#6523) [MAINTENANCE] Clean up Datasource instantiation logic in DataContext (#6517) [MAINTENANCE] Update Domain computation in MetricConfiguration (#6528) [MAINTENANCE] v2 Docs link (#6534)  0.15.37 - YANKED  [FEATURE] Support to include ID/PK in validation result for each row - SQL (#6448) [FEATURE] Build process and example API docs (part 1) (#6474) [FEATURE] Add temp_table_schema_name support for BigQuery (#6303) (thanks @BobbyRyterski) [FEATURE] Decorators for API docs (part 2) (#6497) [FEATURE] Decorators for API docs (part 3) (#6504) [BUGFIX] Support slack channel name with webhook also (#6481) (thanks @Kozehh) [BUGFIX] Airflow operator package conflict for jsonschema (#6495) [BUGFIX] Validator uses proper arguments to show progress bar at Metrics resolution-level (#6510) (thanks @tommy-watts-depop) [DOCS] Schedule Algolia Crawler daily at midnight (#6323) [DOCS] fix(gh-6512): fix rendering of Batch definition (#6513) (thanks @JoelGritter) [MAINTENANCE] Add pretty representations for zep pydantic models (#6472) [MAINTENANCE] Misc updates to PR template (#6479) [MAINTENANCE] Minor cleanup for better code readability (#6478) [MAINTENANCE] Move zep method from datasource to data asset. (#6477) [MAINTENANCE] Staging for build gallery (#6480) [MAINTENANCE] Reformat core expectation docstrings (#6423) [MAINTENANCE] Move \"Domain\" to \"great_expectations/core\" to avoid circular imports; also add MetricConfiguration tests; and other clean up. (#6484) [MAINTENANCE] Query the database for datetime column splitter defaults (#6482) [MAINTENANCE] Placing metrics test db under try-except  (#6489) [MAINTENANCE] Clean up tests for more formal Batch and Validator instantiation (#6491) [MAINTENANCE] Rename ge to gx across the codebase (#6487) [MAINTENANCE] Upgrade CodeSee workflow to version 2 (#6498) (thanks @codesee-maps[bot]) [MAINTENANCE]  Rename GE to GX across codebase (GREAT-1352) (#6494) [MAINTENANCE] Resolve mypy issues in cli/docs.py (#6500) [MAINTENANCE] Increase timeout to 15 minutes for the 2 jobs in manual-staging-json-to-prod pipeline (#6509) [MAINTENANCE] Update Data Assistant plot color scheme and fonts (#6496) [MAINTENANCE] Update RendererConfiguration to pydantic model (#6452) [MAINTENANCE] Message for how to install Great Expectations in Cloud Composer by pinning packages (#6492) [MAINTENANCE] Leverage RendererConfiguration in existing prescriptive templates (1 of 3) (#6460) [MAINTENANCE] Clean up teams.yml (#6511) [MAINTENANCE] Make Generated Integration Tests More Robust Using BatchDefinition and InMemoryDataContext In Validator and ExecutionEngine Instantiation (#6505) [MAINTENANCE] DO NOT MERGE UNTIL DEC 8: [MAINTENANCE] Brand changes in docs (#6427) [MAINTENANCE] fixed typo in nav (#6518) [MAINTENANCE] Clean up GX Cloud environment variable usage (GREAT-1352) (#6501) [MAINTENANCE] Update Data Assistant plot images (#6521) [CONTRIB] Add uniqueness expectation (#6473) (thanks @itaise)  0.15.36  [BUGFIX] Contrib Expectation tracebacks (#6471) [BUGFIX] Add additional error checking to ExpectationAnonymizer (#6467) [MAINTENANCE] Add docstring for context.sources.add_postgres (#6459) [MAINTENANCE] fixing type hints in metrics utils module (#6469) [MAINTENANCE] Moving tutorials to great-expectations repo (#6464)  0.15.35  [FEATURE] add multiple input metric (#6373) (thanks @CarstenFrommhold) [FEATURE] add multiple column metric (#6372) (thanks @CarstenFrommhold) [FEATURE]: DataProfilerUnstructuredDataAssistant Integration (#6400) (thanks @micdavis) [FEATURE] add new metric - query template values (#5994) (thanks @itaise) [FEATURE] ZEP Config serialize as YAML (#6398) [BUGFIX] Patch issue with call to ExpectationAnonymizer to ensure DataContext init events are captured (#6458) [BUGFIX] Support Table and Column Names Case Non-Sensitivity Relationship Between Snowflake, Oracle, DB2, etc. DBMSs (Upper Case) and SQLAlchemy (Lower Case) Representations (#6450) [BUGFIX] Metrics return value no longer returns None for unexpected_index_list - Sql and Spark (#6392) [BUGFIX] Fix for mssql tests that depend on datetime to string conversion (#6449) [BUGFIX] issue-4295-fix-issue (#6164) (thanks @YevgeniyaLee) [BUGFIX] updated capitalone setup.py file (#6410) (thanks @micdavis) [BUGFIX] Patch key-generation issue with DataContext.save_profiler() (#6405) [DOCS] add configuration of anonymous_usage_statistics for documentation (#6293) (thanks @milithino) [DOCS] add boto3 explanations on document (#6407) (thanks @tiruka) [MAINTENANCE] [CONTRIB] Multicolumns sum equal to single column (#6446) (thanks @asafla) [MAINTENANCE] [CONTRIB] add expectation - check gaps in SCD tables (#6433) (thanks @itaise) [MAINTENANCE] [CONTRIB] Add no days missing expectation (#6432) (thanks @itaise) [MAINTENANCE] [CONTRIB] Feature/add two tables expectation (#6429) (thanks @itaise) [MAINTENANCE] [CONTRIB] Add number of unique values expectation (#6425) (thanks @itaise) [MAINTENANCE] Add sorters to zep postgres datasource. (#6456) [MAINTENANCE] Bump ubuntu version in CI (#6457) [MAINTENANCE] Remove anticipatory multi-language support from renderers (#6426) [MAINTENANCE] Remove yaml user_flow_scripts (#6454) [MAINTENANCE] Additional sqlite database fixture for taxi_data - All 2020 data in single table (#6455) [MAINTENANCE] Clean Up Variable Names In Test Modules, Type Hints, and Minor Refactoring For Better Code Elegance/Readability (#6444) [MAINTENANCE] Update and Simplify Pandas tests for MapMetrics (#6443) [MAINTENANCE] Add metadata to experimental datasource Batch class (#6442) [MAINTENANCE] Small refactor (#6422) [MAINTENANCE] Sorting batch IDs and typehints clean up (#6421) [MAINTENANCE] Clean Up Type Hints and Minor Refactoring For Better Code Elegance/Readability (#6418) [MAINTENANCE] Implement RendererConfiguration (#6412) [MAINTENANCE] Cleanup For Better Code Elegance/Readability (#6406) [MAINTENANCE] ZEP - GxConfig cleanup (#6404) [MAINTENANCE] Migrate remaining methods from BaseDataContext (#6403) [MAINTENANCE] Migrate additional CRUD methods from BaseDataContext to AbstractDataContext (#6395) [MAINTENANCE] ZEP add yaml methods to all experimental models (#6401) [MAINTENANCE] Remove call to verify_library_dependent_modules for pybigquery (#6394) [MAINTENANCE] Make \"IDDict.to_id()\" serialization more efficient. (#6389)  0.15.34  [BUGFIX] Ensure packaging_and_installation CI tests against latest tag (#6386) [BUGFIX] Fixed missing comma in pydantic constraints (#6391) (thanks @awburgess) [BUGFIX] fix pydantic dev req file entries (#6396) [DOCS] DOC-379 bring spark datasource configuration example scripts under test (#6362) [MAINTENANCE] Handle both ExpectationConfiguration and ExpectationValidationResult in default Atomic renderers and cleanup include_column_name (#6380) [MAINTENANCE] Add type annotations to all existing atomic renderer signatures (#6385) [MAINTENANCE] move zep -> experimental package (#6378) [MAINTENANCE] Migrate additional methods from BaseDataContext to other parts of context hierarchy (#6388)  0.15.33  [FEATURE] POC ZEP Config Loading (#6320) [BUGFIX] Fix issue with misaligned indentation in docs snippets (#6339) [BUGFIX] Use requirements.txt file when installing linting/static check dependencies in CI (#6368) [BUGFIX] Patch nested snippet indentation issues within remark-named-snippets plugin (#6376) [BUGFIX] Ensure packaging_and_installation CI tests against latest tag (#6386) [DOCS] DOC-308 update CLI command in docs when working with RBPs instead of Data Assistants (#6222) [DOCS] DOC-366 updates to docs in support of branding updates (#5766) [DOCS] Add yarn snippet-check command (#6351) [MAINTENANCE] Add missing one-line docstrings and try to make the others consistent (#6340) [MAINTENANCE] Refactor variable aggregation/substitution logic into ConfigurationProvider hierarchy (#6321) [MAINTENANCE] In ExecutionEngine: Make variable names and usage more descriptive of their purpose. (#6342) [MAINTENANCE] Move Cloud-specific enums to cloud_constants.py (#6349) [MAINTENANCE] Refactor out termcolor dependency (#6348) [MAINTENANCE] Zep PostgresDatasource returns a list of batches. (#6341) [MAINTENANCE] Refactor usage_stats_opt_out method in DataContext (#5339) [MAINTENANCE] Fix computed metrics type hint in ExecutionEngine.resolve_metrics() method (#6347) [MAINTENANCE] Subject: Support to include ID/PK in validation result for each row t\u2026 (#5876) (thanks @abekfenn) [MAINTENANCE] Pin mypy to 0.990 (#6361) [MAINTENANCE] Misc cleanup of GX Cloud helpers (#6352) [MAINTENANCE] Update column_reflection_fallback to also use schema name for Trino (#6350) [MAINTENANCE] Bump version of mypy in contrib CLI (#6370) [MAINTENANCE] Move config variable substitution logic into ConfigurationProvider (#6345) [MAINTENANCE] Removes comment in code that was causing confusion to some users. (#6366) [MAINTENANCE] minor metrics typing (#6374) [MAINTENANCE] Make ConfigurationProvider and ConfigurationSubstitutor private (#6375) [MAINTENANCE] Rename GeCloudStoreBackend to GXCloudStoreBackend (#6377) [MAINTENANCE] Cleanup Metrics and ExecutionEngine methods (#6371) [MAINTENANCE] F/great 1314/integrate zep in core (#6358) [MAINTENANCE] Loosen pydantic version requirement (#6384)  0.15.32  [BUGFIX] Patch broken CloudNotificationAction tests (#6327) [BUGFIX] add create_temp_table flag to ExecutionEngineConfigSchema (#6331) (thanks @tommy-watts-depop) [BUGFIX] MapMetrics now return partial_unexpected values for SUMMARY format (#6334) [DOCS] Re-writes \"how to implement custom notifications\" as \"How to get Data Docs URLs for use in custom Validation Actions\" (#6281) [DOCS] Removes deprecated expectation notebook exploration doc (#6298) [DOCS] Removes a number of unused & deprecated docs (#6300) [DOCS] Prioritizes Onboarding Data Assistant in ToC (#6302) [DOCS] Add ZenML into integration table in Readme (#6144) (thanks @dnth) [DOCS] add pypi release badge (#6324) [MAINTENANCE] Remove unneeded BaseDataContext.get_batch_list (#6291) [MAINTENANCE] Clean up implicit Optional errors flagged by mypy (#6319) [MAINTENANCE] Add manual prod flags to core Expectations (#6278) [MAINTENANCE] Fallback to isnot method if is_not is not available (old sqlalchemy) (#6318) [MAINTENANCE] Add ZEP postgres datasource. (#6274) [MAINTENANCE] Delete \"metric_dependencies\" from MetricConfiguration constructor arguments (#6305) [MAINTENANCE] Clean up DataContext (#6304) [MAINTENANCE] Deprecate save_changes flag on Datasource CRUD (#6258) [MAINTENANCE] Deprecate great_expectations.render.types package (#6315) [MAINTENANCE] Update range of allowable sqlalchemy versions (#6328) [MAINTENANCE] Fixing checkpoint types (#6325) [MAINTENANCE] Fix column_reflection_fallback for Trino and minor logging/testing improvements (#6218) [MAINTENANCE] Change the number of expected Expectations in the 'quick check' stage of build_gallery pipeline (#6333)  0.15.31  [BUGFIX] Include all requirement files in the sdist (#6292) (thanks @xhochy) [DOCS] Updates outdated batch_request snippet in Terms (#6283) [DOCS] Update Conditional Expectations doc w/ current availability  (#6279) [DOCS] Remove outdated Data Discovery page and all references (#6288) [DOCS] Remove reference/evaluation_parameters page and all references (#6294) [DOCS] Removing deprecated Custom Metrics doc (#6282) [DOCS] Re-writes \"how to implement custom notifications\" as \"How to get Data Docs URLs for use in custom Validation Actions\" (#6281) [DOCS] Removes deprecated expectation notebook exploration doc (#6298) [MAINTENANCE] Move RuleState into rule directory. (#6284)  0.15.30  [FEATURE] Add zep datasources to data context. (#6255) [BUGFIX] Iterate through GeCloudIdentifiers to find the suite ID from the name (#6243) [BUGFIX] Update default base url for cloud API (#6176) [BUGFIX] Pin termcolor to below 2.1.0 due to breaking changes in lib's TTY parsing logic (#6257) [BUGFIX] InferredAssetSqlDataConnector include_schema_name introspection of identical table names in different schemas (#6166) [BUGFIX] Fixdocs-integration tests, and temporarily pin sqlalchemy (#6268) [BUGFIX] Fix serialization for contrib packages (#6266) [BUGFIX] Ensure that Datasource credentials are not persisted to Cloud/disk (#6254) [DOCS] Updates package contribution references (#5885) [MAINTENANCE] Maintenance/great 1103/great 1318/alexsherstinsky/validation graph/refactor validation graph usage 2022 10 20 248 (#6228) [MAINTENANCE] Refactor instances of noqa: F821 Flake8 directive (#6220) [MAINTENANCE] Logo URI ref in data_docs (#6246) [MAINTENANCE] fix typos in docstrings (#6247) [MAINTENANCE] Isolate Trino/MSSQL/MySQL tests in dev CI (#6231) [MAINTENANCE] Split up compatability and comprehensive stages in dev CI to improve performance (#6245) [MAINTENANCE] ZEP POC - Asset Type Registration (#6194) [MAINTENANCE] Add Trino CLI support and bump Trino version (#6215) (thanks @hovaesco) [MAINTENANCE] Delete unneeded Rule attribute property (#6264) [MAINTENANCE] Small clean-up of Marshmallow warnings (missing parameter changed to load_default as of 3.13) (#6213) [MAINTENANCE] Move .png files out of project root (#6249) [MAINTENANCE] Cleanup expectation.py attributes (#6265) [MAINTENANCE] Further parallelize test runs in dev CI (#6267) [MAINTENANCE] GCP Integration Pipeline fix (#6259) [MAINTENANCE] mypy warn_unused_ignores (#6270) [MAINTENANCE] ZEP - Data Source base class (#6263) [MAINTENANCE] Reverting marshmallow version bump (#6271) [MAINTENANCE] type hints cleanup in Rule-Based Profiler (#6272) [MAINTENANCE] Remove unused f-strings (#6248) [MAINTENANCE] Make ParameterBuilder.resolve_evaluation_dependencies() into instance (rather than utility) method (#6273) [MAINTENANCE] Test definition for ExpectColumnValueZScoresToBeLessThan (#6229) [MAINTENANCE] Make RuleState constructor argument ordering consistent with standard pattern. (#6275) [MAINTENANCE] [REQUEST] Please allow Rachel to unblock blockers (#6253)  0.15.29  [FEATURE] Add support to AWS Glue Data Catalog (#5123) (thanks @lccasagrande) [FEATURE] / Added pairwise expectation 'expect_column_pair_values_to_be_in_set' (#6097) (thanks @Arnavkar) [BUGFIX] Adjust condition in RenderedAtomicValueSchema.clean_null_attrs (#6168) [BUGFIX] Add py to dev dependencies to circumvent compatability issues with pytest==7.2.0 (#6202) [BUGFIX] Fix test_package_dependencies.py to include py lib (#6204) [BUGFIX] Fix logic in ExpectationDiagnostics._check_renderer_methods method (#6208) [BUGFIX] Patch issue with empty config variables file raising TypeError (#6216) [BUGFIX] Release patch for Azure env vars (#6233) [BUGFIX] Cloud Data Context should overwrite existing suites based on ge_cloud_id instead of name (#6234) [BUGFIX] Add env vars to Pytest min versions Azure stage (#6239) [DOCS] doc-297: update the create Expectations overview page for Data Assistants (#6212) [DOCS] DOC-378: bring example scripts for pandas configuration guide under test (#6141) [MAINTENANCE] Add unit test for MetricsCalculator.get_metric() Method -- as an example template (#6179) [MAINTENANCE] ZEP MetaDatasource POC (#6178) [MAINTENANCE] Update scope_check in Azure CI to trigger on changed .py source code files (#6185) [MAINTENANCE] Move test_yaml_config to a separate class (#5487) [MAINTENANCE] Changed profiler to Data Assistant in CLI, docs, and tests (#6189) [MAINTENANCE] Update default GE_USAGE_STATISTICS_URL in test docker image. (#6192) [MAINTENANCE] Re-add a renamed test definition file (#6182) [MAINTENANCE] Refactor method parse_evaluation_parameter (#6191) [MAINTENANCE] Migrate methods from BaseDataContext to AbstractDataContext (#6188) [MAINTENANCE] Rename cfe to v3_api (#6190) [MAINTENANCE] Test Trino doc examples with test_script_runner.py (#6198) [MAINTENANCE] Cleanup of Regex ParameterBuilder (#6196) [MAINTENANCE] Apply static type checking to expectation.py (#6173) [MAINTENANCE] Remove version matrix from dev CI pipeline to improve performance (#6203) [MAINTENANCE] Rename CloudMigrator.retry_unsuccessful_validations (#6206) [MAINTENANCE] Add validate_configuration method to expect_table_row_count_to_equal_other_table (#6209) [MAINTENANCE] Replace deprecated iteritems with items (#6205) [MAINTENANCE] Add instructions for setting up the test_ci database (#6211) [MAINTENANCE] Add E2E tests for Cloud-backed Datasource CRUD (#6186) [MAINTENANCE] Execution Engine linting & partial typing (#6210) [MAINTENANCE] Test definition for ExpectColumnValuesToBeJsonParsable, including a fix for Spark (#6207) [MAINTENANCE] Port over usage statistics enabled methods from BaseDataContext to AbstractDataContext (#6201) [MAINTENANCE] Remove temporary dependency on py (#6217) [MAINTENANCE] Adding type hints to DataAssistant implementations (#6224) [MAINTENANCE] Remove AWS config file dependencies and use existing env vars in CI/CD (#6227) [MAINTENANCE] Make UsageStatsEvents a StrEnum (#6225) [MAINTENANCE] Move all requirements-dev*.txt files to separate dir (#6223) [MAINTENANCE] Maintenance/great 1103/great 1318/alexsherstinsky/validation graph/refactor validation graph usage 2022 10 20 248 (#6228)  0.15.28  [FEATURE] Initial zep datasource protocol. (#6153) [FEATURE] Introduce BatchManager to manage Batch objects used by Validator and BatchData used by ExecutionEngine (#6156) [FEATURE] Add support for Vertica dialect (#6145) (thanks @viplazylmht) [FEATURE] Introduce MetricsCalculator and Refactor Redundant Code out of Validator (#6165) [BUGFIX] SQLAlchemy selectable Bug fix (#6159) (thanks @tommy-watts-depop) [BUGFIX] Parameterize usage stats endpoint in test dockerfile. (#6169) [BUGFIX] B/great 1305/usage stats endpoint (#6170) [BUGFIX] Ensure that spaces are recognized in named snippets (#6172) [DOCS] Clarify wording for interactive mode in databricks (#6154) [DOCS] fix source activate command (#6161) (thanks @JGrzywacz) [DOCS] Update version in runtime.txt to fix breaking Netlify builds (#6181) [DOCS] Clean up snippets and line number validation in docs (#6142) [MAINTENANCE] Add Enums for renderer types (#6112) [MAINTENANCE] Minor cleanup in preparation for Validator refactoring into separate concerns (#6155) [MAINTENANCE] add the internal GE_DATA_CONTEXT_ID env var to the docker file (#6122) [MAINTENANCE] Rollback setting GE_DATA_CONTEXT_ID in docker image. (#6163) [MAINTENANCE] disable ge_cloud_mode when specified, detect misconfiguration (#6162) [MAINTENANCE] Re-add missing Expectations to gallery and include package names (#6171) [MAINTENANCE] Use from __future__ import annotations to clean up type hints (#6127) [MAINTENANCE] Make sure that quick stage check returns 0 if there are no problems (#6177) [MAINTENANCE] Remove SQL for expect_column_discrete_entropy_to_be_between (#6180)  0.15.27  [FEATURE] Add logging/warnings to GX Cloud migration process (#6106) [FEATURE] Introduction of updated gx.get_context() method that returns correct DataContext-type (#6104) [FEATURE] Contribute StatisticsDataAssistant and GrowthNumericDataAssistant (both experimental)  (#6115) [BUGFIX] add OBJECT_TYPE_NAMES to the JsonSchemaProfiler - issue #6109 (#6110) (thanks @OphelieC) [BUGFIX] Fix example Set-Based Column Map Expectation template import (#6134) [BUGFIX] Regression due to GESqlDialect Enum for Hive (#6149) [DOCS] Support for named snippets in documentation (#6087) [MAINTENANCE] Clean up test_migrate=True Cloud migrator output (#6119) [MAINTENANCE] Creation of Hackathon Packages (#4587) [MAINTENANCE] Rename GCP Integration Pipeline (#6121) [MAINTENANCE] Change log levels used in CloudMigrator (#6125) [MAINTENANCE] Bump version of sqlalchemy-redshift from 0.7.7 to 0.8.8 (#6082) [MAINTENANCE] self_check linting & initial type-checking (#6126) [MAINTENANCE] Update per Clickhouse multiple same aliases Bug (#6128) (thanks @adammrozik) [MAINTENANCE] Only update existing rendered_content if rendering does not fail with new InlineRenderer failure message (#6091)  0.15.26  [FEATURE] Enable sending of ConfigurationBundle payload in HTTP request to Cloud backend (#6083) [FEATURE] Send user validation results to Cloud backend during migration (#6102) [BUGFIX] Fix bigquery crash when using \"in\" with a boolean column (#6071) [BUGFIX] Fix serialization error when rendering kl_divergence (#6084) (thanks @roblim) [BUGFIX] Enable top-level parameters in Data Assistants accessed via dispatcher (#6077) [BUGFIX] Patch issue around DataContext.save_datasource not sending class_name in result payload (#6108) [DOCS] DOC-377 add missing dictionary in configured asset datasource portion of Pandas and Spark configuration guides (#6081) [DOCS] DOC-376 finalize definition for Data Assistants in technical terms (#6080) [DOCS] Update docs-integration test due to new whole_table splitter behavior (#6103) [DOCS] How to create a Custom Multicolumn Map Expectation (#6101) [MAINTENANCE] Patch broken Cloud E2E test (#6079) [MAINTENANCE] Bundle data context config and other artifacts for migration (#6068) [MAINTENANCE] Add datasources to ConfigurationBundle (#6092) [MAINTENANCE] Remove unused config files from root of GX repo (#6090) [MAINTENANCE] Add data_context_id property to ConfigurationBundle (#6094) [MAINTENANCE] Move all Cloud migrator logic to separate directory (#6100) [MAINTENANCE] Update aloglia scripts for new fields and replica indices (#6049) (thanks @winrp17) [MAINTENANCE] initial Data Source typings (#6099) [MAINTENANCE] Data context migrate to cloud event (#6095) [MAINTENANCE] Bundling tests with empty context configs (#6107) [MAINTENANCE] Fixing a typo (#6113)  0.15.25  [FEATURE] Since value set in expectation kwargs is list of strings, do not emit expect_column_values_to_be_in_set for datetime valued columns (#6046) [FEATURE] add failed expectations list to slack message (#5812) (thanks @itaise) [FEATURE] Enable only ExactNumericRangeEstimator and QuantilesNumericRangeEstimator in \"datetime_columns_rule\" of OnboardingDataAssistant (#6063) [BUGFIX] numpy typing behind if TYPE_CHECKING (#6076) [DOCS] Update \"How to create an Expectation Suite with the Onboarding Data Assistant\" (#6050) [DOCS] How to get one or more Batches of data from a configured Data Source (#6043) [DOCS] DOC-298 Data Assistant technical term page (#6057) [DOCS] Update OnboardingDataAssistant documentation (#6059) [MAINTENANCE] Clean up of DataAssistant tests that depend on Jupyter notebooks (#6039) [MAINTENANCE] AbstractDataContext.datasource_save() test simplifications (#6052) [MAINTENANCE] Rough architecture for cloud migration tool (#6054) [MAINTENANCE] Include git commit info when building docker image. (#6060) [MAINTENANCE] Allow CloudDataContext to retrieve and initialize its own project config (#6006) [MAINTENANCE] Removing Jupyter notebook-based tests for DataAssistants (#6062) [MAINTENANCE] pinned dremio, fixed linting (#6067) [MAINTENANCE] usage-stats, & utils.py typing (#5925) [MAINTENANCE] Refactor external HTTP request logic into a Session factory function (#6007) [MAINTENANCE] Remove tag validity stage from release pipeline (#6069) [MAINTENANCE] Remove unused test fixtures from test suite (#6058) [MAINTENANCE] Remove outdated release files (#6074)  0.15.24  [FEATURE] context.save_datasource (#6009) [BUGFIX] Standardize ConfiguredAssetSqlDataConnector config in datasource new CLI workflow (#6044) [DOCS] DOC-371 update the getting started tutorial for data assistants (#6024) [DOCS] DOCS-369 sql data connector configuration guide (#6002) [MAINTENANCE] Remove outdated entry from release schedule JSON (#6032) [MAINTENANCE] Clean up Spark schema tests to have proper names (#6033)  0.15.23  [FEATURE] do not require expectation_suite_name in DataAssistantResult.show_expectations_by...() methods (#5976) [FEATURE] Refactor PartitionParameterBuilder into dedicated ValueCountsParameterBuilder and HistogramParameterBuilder (#5975) [FEATURE] Implement default sorting for batches based on selected splitter method (#5924) [FEATURE] Make OnboardingDataAssistant default profiler in CLI SUITE NEW (#6012) [FEATURE] Enable omission of rounding of decimals in NumericMetricRangeMultiBatchParameterBuilder (#6017) [FEATURE] Enable non-default sorters for ConfiguredAssetSqlDataConnector (#5993) [FEATURE] Data Assistant plot method indication of total metrics and expectations count (#6016) [BUGFIX] Addresses issue with ExpectCompoundColumnsToBeUnique renderer (#5970) [BUGFIX] Fix failing run_profiler_notebook test (#5983) [BUGFIX] Handle case when only one unique \"column.histogram\" bin value is found (#5987) [BUGFIX] Update get_validator test assertions due to change in fixture batches (#5989) [BUGFIX] Fix use of column.partition metric in HistogramSingleBatchParameterBuilder to more accurately handle errors (#5990) [BUGFIX] Make Spark implementation of \"column.value_counts\" metric more robust to None/NaN column values (#5996) [BUGFIX] Filter out np.nan values (just like None values) as part of ColumnValueCounts._spark() implementation (#5998) [BUGFIX] Handle case when only one unique \"column.histogram\" bin value is found with proper type casting (#6001) [BUGFIX] ColumnMedian._sqlalchemy() needs to handle case of single-value column (#6011) [BUGFIX] Patch broken save_expectation_suite behavior with Cloud-backed DataContext (#6004) [BUGFIX] Clean quantitative metrics DataFrames in Data Assistant plotting (#6023) [BUGFIX] Defer pprint in ExpectationSuite.show_expectations_by_expectation_type() due to Jupyter rate limit (#6026) [BUGFIX] Use UTC TimeZone (rather than Local Time Zone) for Rule-Based Profiler DateTime Conversions (#6028) [DOCS] Update snippet refs in \"How to create an Expectation Suite with the Onboarding Data Assistant\" (#6014) [MAINTENANCE] Randomize the non-comprehensive tests (#5968) [MAINTENANCE] DatasourceStore refactoring (#5941) [MAINTENANCE] Expectation suite init unit tests + types (#5957) [MAINTENANCE] Expectation suite new unit tests for add_citation (#5966) [MAINTENANCE] Updated release schedule (#5977) [MAINTENANCE] Unit tests for CheckpointStore (#5967) [MAINTENANCE] Enhance unit tests for ExpectationSuite.isEquivalentTo (#5979) [MAINTENANCE] Remove unused fixtures from test suite (#5965) [MAINTENANCE] Update to MultiBatch Notebook to include Configured - Sql (#5945) [MAINTENANCE] Update to MultiBatch Notebook to include Inferred - Sql  (#5958) [MAINTENANCE] Add reverse assertion for isEquivalentTo tests (#5982) [MAINTENANCE] Unit test enhancements ExpectationSuite.eq() (#5984) [MAINTENANCE] Refactor DataContext.__init__ to move Cloud-specific logic to CloudDataContext (#5981) [MAINTENANCE] Set up cloud integration tests with Azure Pipelines (#5995) [MAINTENANCE] Example of splitter_method at Asset and DataConnector level (#6000) [MAINTENANCE] Replace splitter_method strings with SplitterMethod Enum and leverage GESqlDialect Enum where applicable (#5980) [MAINTENANCE] Ensure that DataContext.add_datasource works with nested DataConnector ids (#5992) [MAINTENANCE] Remove cloud integration tests from azure-pipelines.yml (#5997) [MAINTENANCE] Unit tests for GeCloudStoreBackend (#5999) [MAINTENANCE] Parameterize pg hostname in jupyter notebooks (#6005) [MAINTENANCE] Unit tests for Validator (#5988) [MAINTENANCE] Add unit tests for SimpleSqlalchemyDatasource (#6008) [MAINTENANCE] Remove dgtest from dev pipeline (#6003) [MAINTENANCE] Remove deprecated account_id from GX Cloud integrations (#6010) [MAINTENANCE] Added perf considerations to onboarding assistant notebook (#6022) [MAINTENANCE] Redshift specific temp table code path (#6021) [MAINTENANCE] Update datasource new workflow to enable ConfiguredAssetDataConnector usage with SQL-backed Datasources (#6019)  0.15.22  [FEATURE] Allowing schema  to be passed in as batch_spec_passthrough in Spark (#5900) [FEATURE] DataAssistants Example Notebook - Spark (#5919) [FEATURE] Improve slack error condition (#5818) (thanks @itaise) [BUGFIX] Ensure that ParameterBuilder implementations in Rule Based Profiler properly handle SQL DECIMAL type (#5896) [BUGFIX] Making an all-NULL column handling in RuleBasedProfiler more robust (#5937) [BUGFIX] Don't include abstract Expectation classes in _retrieve_expectations_from_module (#5947) [BUGFIX] Data Assistant plotting with zero expectations produced (#5934) [BUGFIX] prefix and suffix asset names are only relevant for InferredSqlAlchemyDataConnector (#5950) [BUGFIX] Prevent \"division by zero\" errors in Rule-Based Profiler calculations when Batch has zero rows (#5960) [BUGFIX] Spark column.distinct_values no longer returns entire table distinct values (#5969) [DOCS] DOC-368 spelling correction (#5912) [MAINTENANCE] Mark all tests within tests/data_context/stores dir (#5913) [MAINTENANCE] Cleanup to allow docker test target to run tests in random order (#5915) [MAINTENANCE] Use datasource config in add_datasource support methods (#5901) [MAINTENANCE] Cleanup up some new datasource sql data connector tests. (#5918) [MAINTENANCE] Unit tests for data_context/store (#5923) [MAINTENANCE] Mark all tests within tests/validator (#5926) [MAINTENANCE]  Certify InferredAssetSqlDataConnector and ConfiguredAssetSqlDataConnector (#5847) [MAINTENANCE] Mark DBFS tests with @pytest.mark.integration (#5931) [MAINTENANCE] Reset globals modified in tests (#5936) [MAINTENANCE] Move Store test utils from source code to tests (#5932) [MAINTENANCE] Mark tests within tests/rule_based_profiler (#5930) [MAINTENANCE] Add missing import for ConfigurationIdentifier (#5943) [MAINTENANCE] Update to OnboardingDataAssistant Notebook - Sql (#5939) [MAINTENANCE] Run comprehensive tests in a random order (#5942) [MAINTENANCE] Unit tests for ConfigurationStore (#5948) [MAINTENANCE] Add a dev-tools requirements option (#5944) [MAINTENANCE] Run spark and onboarding data assistant test in their own jobs. (#5951) [MAINTENANCE] Unit tests for ValidationGraph and related classes (#5954) [MAINTENANCE] More unit tests for Stores  (#5953) [MAINTENANCE] Add x-fails to flaky Cloud tests for purposes of 0.15.22 (#5964) [MAINTENANCE] Bump Marshmallow upper bound to work with Airflow operator (#5952) [MAINTENANCE] Use DataContext to ignore progress bars (#5959)  0.15.21  [FEATURE] Add include_rendered_content to get_expectation_suite and get_validation_result (#5853) [FEATURE] Add tags as an optional setting for the OpsGenieAlertAction (#5855) (thanks @stevewb1993) [BUGFIX] Ensure that delete_expectation_suite returns proper boolean result (#5878) [BUGFIX] many small bugfixes (#5881) [BUGFIX] Fix typo in default value of \"ignore_row_if\" kwarg for MulticolumnMapExpectation (#5860) (thanks @mkopec87) [BUGFIX] Patch issue with checkpoint_identifier within Checkpoint.run workflow (#5894) [BUGFIX] Ensure that DataContext.add_checkpoint() updates existing objects in GX Cloud (#5895) [DOCS] DOC-364 how to configure a spark datasource (#5840) [MAINTENANCE] Unit Tests Pipeline step (#5838) [MAINTENANCE] Unit tests to ensure coverage over Datasource caching in DataContext (#5839) [MAINTENANCE] Add entries to release schedule (#5833) [MAINTENANCE] Properly label DataAssistant tests with @pytest.mark.integration (#5845) [MAINTENANCE] Add additional unit tests around Datasource caching (#5844) [MAINTENANCE] Mark miscellaneous tests with @pytest.mark.unit (#5846) [MAINTENANCE] datasource, data_context, core typing, lint fixes (#5824) [MAINTENANCE] add --ignore-suppress and --ignore-only-for to build_gallery.py with bugfixes (#5802) [MAINTENANCE] Remove pyparsing pin for <3.0 (#5849) [MAINTENANCE] Finer type exclude (#5848) [MAINTENANCE] use id instead id_  (#5775) [MAINTENANCE] Add data connector names in datasource config (#5778) [MAINTENANCE] init tests for dict and json serializers (#5854) [MAINTENANCE] Remove Partitioning and Quantiles metrics computations from DateTime Rule of OnboardingDataAssistant (#5862) [MAINTENANCE] Update ExpectationSuite CRUD on DataContext to recognize Cloud ids (#5836) [MAINTENANCE] Handle Pandas warnings in Data Assistant plots (#5863) [MAINTENANCE] Misc cleanup of test_expectation_suite_crud.py (#5868) [MAINTENANCE] Remove vendored marshmallow__shade (#5866) [MAINTENANCE] don't force using the stand alone mock (#5871) [MAINTENANCE] Update expectation_gallery pipeline (#5874) [MAINTENANCE] run unit-tests on a target package (#5869) [MAINTENANCE] add pytest-timeout (#5857) [MAINTENANCE] Label tests in tests/core with @pytest.mark.unit and @pytest.mark.integration (#5879) [MAINTENANCE] new invoke test flags (#5880) [MAINTENANCE] JSON Serialize RowCondition and MetricBundle computation result to enable IDDict.to_id() for SparkDFExecutionEngine (#5883) [MAINTENANCE] increase the pytest-timeout timeout value during unit-testing step (#5884) [MAINTENANCE] Add @pytest.mark.slow throughout test suite (#5882) [MAINTENANCE] Add test_expectation_suite_send_usage_message (#5886) [MAINTENANCE] Mark existing tests as unit or integration (#5890) [MAINTENANCE] Convert integration tests to unit (#5891) [MAINTENANCE] Update distinct metric dependencies and implementations (#5811) [MAINTENANCE] Add slow pytest marker to config and sort them alphabetically. (#5892) [MAINTENANCE] Adding serialization tests for Spark (#5897) [MAINTENANCE] Improve existing expectation suite unit tests (phase 1) (#5898) [MAINTENANCE] SqlAlchemyExecutionEngine case for SQL Alchemy Select and TextualSelect due to SADeprecationWarning (#5902)  0.15.20  [FEATURE] query.pair_column Metric (#5743) [FEATURE] Enhance execution time measurement utility, and save DomainBuilder execution time per Rule of Rule-Based Profiler (#5796) [FEATURE] Support single-batch mode in MetricMultiBatchParameterBuilder (#5808) [FEATURE] Inline ExpectationSuite Rendering (#5726) [FEATURE] Better error for missing expectation (#5750) (thanks @tylertrussell) [FEATURE] DataAssistants Example Notebook - Pandas (#5820) [BUGFIX] Ensure name not persisted (#5813) [DOCS] Change the selectable to a list (#5780) (thanks @itaise) [DOCS] Fix how to create custom table expectation (#5807) (thanks @itaise) [DOCS] DOC-363 how to configure a pandas datasource (#5779) [MAINTENANCE] Remove xfail markers on cloud tests (#5793) [MAINTENANCE] build-gallery enhancements (#5616) [MAINTENANCE] Refactor save_profiler to remove explicit name and ge_cloud_id args (#5792) [MAINTENANCE] Add v2_api flag for v2_api specific tests (#5803) [MAINTENANCE] Clean up ge_cloud_id reference from DataContext ExpectationSuite CRUD (#5791) [MAINTENANCE] Refactor convert_dictionary_to_parameter_node (#5805) [MAINTENANCE] Remove ge_cloud_id from DataContext.add_profiler() signature (#5804) [MAINTENANCE] Remove \"copy.deepcopy()\" calls from ValidationGraph (#5809) [MAINTENANCE] Add vectorized is_between for common numpy dtypes (#5711) [MAINTENANCE] Make partitioning directives of PartitionParameterBuilder configurable (#5810) [MAINTENANCE] Write E2E Cloud test for RuleBasedProfiler creation and retrieval (#5815) [MAINTENANCE] Change recursion to iteration for function in parameter_container.py (#5817) [MAINTENANCE] add pytest-mock & pytest-icdiff plugins (#5819) [MAINTENANCE] Surface cloud errors (#5797) [MAINTENANCE] Clean up build_parameter_container_for_variables (#5823) [MAINTENANCE] Bugfix/snowflake temp table schema name (#5814) [MAINTENANCE] Update list_ methods on DataContext to emit names along with object ids (#5826) [MAINTENANCE] xfail Cloud E2E tests due to schema issue with DataContextVariables (#5828) [MAINTENANCE] Clean up xfails in preparation for 0.15.20 release (#5835) [MAINTENANCE] Add back xfails for E2E Cloud tests that fail on env var retrieval in Docker (#5837)  0.15.19  [FEATURE] DataAssistantResult plot multiple metrics per expectation (#5556) [FEATURE] Enable passing \"exact_estimation\" boolean at DataAssistant.run() level (default value is True) (#5744) [FEATURE] Example notebook for Onboarding DataAssistant - postgres (#5776) [BUGFIX] dir update for data_assistant_result (#5751) [BUGFIX] Fix docs_integration pipeline (#5734) [BUGFIX] Patch flaky E2E Cloud test with randomized suite names (#5752) [BUGFIX] Fix RegexPatternStringParameterBuilder to use legal character repetition.  Remove median, mean, and standard deviation features from OnboardingDataAssistant \"datetime_columns_rule\" definition. (#5757) [BUGFIX] Move SuiteValidationResult.meta validation id propogation before ValidationOperator._run_action (#5760) [BUGFIX] Update \"column.partition\" Metric to handle DateTime Arithmetic Properly (#5764) [BUGFIX] JSON-serialize RowCondition and enable IDDict to support comparison operations (#5765) [BUGFIX] Insure all estimators properly handle datetime-float conversion (#5774) [BUGFIX] Return appropriate subquery type to Query Metrics for SA version (#5783) [DOCS] added guide how to use gx with emr serverless (#5623) (thanks @bvolodarskiy) [DOCS] DOC-362: how to choose between working with a single or multiple batches of data (#5745) [MAINTENANCE] Temporarily xfail E2E Cloud tests due to Azure env var issues (#5787) [MAINTENANCE] Add ids to DataConnectorConfig (#5740) [MAINTENANCE] Rename GX Cloud \"contract\" resource to \"checkpoint\" (#5748) [MAINTENANCE] Rename GX Cloud \"suite_validation_result\" resource to \"validation_result\" (#5749) [MAINTENANCE] Store Refactor - cloud store return types & http-errors (#5730) [MAINTENANCE] profile_numeric_columns_diff_expectation (#5741) (thanks @stevensecreti) [MAINTENANCE] Clean up type hints around class constructors (#5738) [MAINTENANCE] invoke docker (#5703) [MAINTENANCE] Add plist to build docker test image daily. (#5754) [MAINTENANCE] opt-out type-checking  (#5713) [MAINTENANCE] Enable Algolia UI (#5753) [MAINTENANCE] Linting & initial typing for data context (#5756) [MAINTENANCE] Update oneshot estimator to quantiles estimator (#5737) [MAINTENANCE] Update Auto-Initializing Expectations to use exact estimator by default (#5759) [MAINTENANCE] Send a Gx-Version header set to version in requests to cloud (#5758) (thanks @wookasz) [MAINTENANCE]  invoke docker --detach and more typing (#5770) [MAINTENANCE] In ParameterBuilder implementations, enhance handling of numpy.ndarray metric values, whose elements are or can be converted into datetime.datetime type. (#5771) [MAINTENANCE] Config/Schema round_tripping (#5697) [MAINTENANCE] Add experimental label to MetricStore Doc (#5782) [MAINTENANCE] Remove GeCloudIdentifier creation in Checkpoint.run() (#5784)  0.15.18  [FEATURE] Example notebooks for multi-batch Spark (#5683) [FEATURE] Introduce top-level default_validation_id in CheckpointConfig (#5693) [FEATURE] Pass down validation ids to ExpectationSuiteValidationResult.meta within Checkpoint.run() (#5725) [FEATURE] Refactor data assistant runner to compute formal parameters for data assistant run method signatures (#5727) [BUGFIX] Restored sqlite database for tests (#5742) [BUGFIX] Fixing a typo in variable name for default profiler for auto-initializing expectation \"expect_column_mean_to_be_between\" (#5687) [BUGFIX] Remove resource_type from call to StoreBackend.build_key (#5690) [BUGFIX] Update how_to_use_great_expectations_in_aws_glue.md (#5685) (thanks @bvolodarskiy) [BUGFIX] Updated how_to_use_great_expectations_in_aws_glue.md again (#5696) (thanks @bvolodarskiy) [BUGFIX] Update how_to_use_great_expectations_in_aws_glue.md (#5722) (thanks @bvolodarskiy) [BUGFIX] Update aws_glue_deployment_patterns.py (#5721) (thanks @bvolodarskiy) [DOCS] added guide how to use great expectations with aws glue (#5536) (thanks @bvolodarskiy) [DOCS] Document the ZenML integration for Great Expectations (#5672) (thanks @stefannica) [DOCS] Converts broken ZenML md refs to Technical Tags (#5714) [DOCS] How to create a Custom Query Expectation (#5460) [MAINTENANCE] Pin makefun package to version range for support assurance (#5746) [MAINTENANCE] s3 link for logo (#5731) [MAINTENANCE] Assign resource_type in InlineStoreBackend constructor (#5671) [MAINTENANCE] Add mysql client to Dockerfile.tests (#5681) [MAINTENANCE] RuleBasedProfiler corner case configuration changes (#5631) [MAINTENANCE] Update teams.yml (#5684) [MAINTENANCE] Utilize e2e mark on E2E Cloud tests (#5691) [MAINTENANCE] pyproject.tooml build-system typo (#5692) [MAINTENANCE] expand flake8 coverage (#5676) [MAINTENANCE] Ensure Cloud E2E tests are isolated to gx-cloud-e2e stage of CI (#5695) [MAINTENANCE] Add usage stats and initial database docker tests to CI (#5682) [MAINTENANCE] Add e2e mark to pyproject.toml (#5699) [MAINTENANCE] Update docker readme to mount your repo over the builtin one. (#5701) [MAINTENANCE] Combine packages rule_based_profiler and rule_based_profiler.types (#5680) [MAINTENANCE] ExpectColumnValuesToBeInSetSparkOptimized (#5702) [MAINTENANCE] expect_column_pair_values_to_have_difference_of_custom_perc\u2026 (#5661) (thanks @exteli) [MAINTENANCE] Remove non-docker version of CI tests that are now running in docker. (#5700) [MAINTENANCE] Add back integration mark to tests in test_datasource_crud.py (#5708) [MAINTENANCE] DEVREL-2289/Stale/Triage (#5694) [MAINTENANCE] revert expansive flake8 pre-commit checking - flake8 5.0.4 (#5706) [MAINTENANCE] Bugfix for cloud-db-integration-pipeline (#5704) [MAINTENANCE] Remove pytest-azurepipelines (#5716) [MAINTENANCE] Remove deprecation warning from DataConnector-level batch_identifiers for RuntimeDataConnector (#5717) [MAINTENANCE] Refactor AbstractConfig to make name and id_ consistent attrs (#5698) [MAINTENANCE] Move CLI tests to docker (#5719) [MAINTENANCE] Leverage DataContextVariables in DataContext hierarchy to automatically determine how to persist changes (#5715) [MAINTENANCE] Refactor InMemoryStoreBackend out of store_backend.py (#5679) [MAINTENANCE] Move compatibility matrix tests to docker (#5728) [MAINTENANCE] Adds additional file extensions for Parquet assets (#5729) [MAINTENANCE] MultiBatch SqlExample notebook Update.  (#5718) [MAINTENANCE] Introduce NumericRangeEstimator class hierarchy and encapsulate existing estimator implementations (#5735)  0.15.17  [FEATURE] Improve estimation histogram computation in NumericMetricRangeMultiBatchParameterBuilder to include both counts and bin edges (#5628) [FEATURE] Enable retrieve by name for datasource with cloud store backend (#5640) [FEATURE] Update DataContext.add_checkpoint() to ensure validations within CheckpointConfig contain ids (#5638) [FEATURE] Add expect_column_values_to_be_valid_crc32 (#5580) (thanks @sp1thas) [FEATURE] Enable showing expectation suite by domain and by expectation_type -- from DataAssistantResult (#5673) [BUGFIX] Patch flaky E2E GX Cloud tests (#5629) [BUGFIX] Pass --cloud flag to dgtest-cloud-overrides section of Azure YAML (#5632) [BUGFIX] Remove datasource from config on delete (#5636) [BUGFIX] Patch issue with usage stats sync not respecting usage stats opt-out (#5644) [BUGFIX] SlackRenderer / EmailRenderer links to deprecated doc (#5648) [BUGFIX] Fix table.head metric issue when using BQ without temp tables (#5630) [BUGFIX] Quick bugfix on all profile numeric column diff bounds expectations (#5651) (thanks @stevensecreti) [BUGFIX] Patch bug with id vs id_ in Cloud integration tests (#5677) [DOCS] Fix a typo in batch_request_parameters variable (#5612) (thanks @StasDeep) [MAINTENANCE] CloudDataContext add_datasource test (#5626) [MAINTENANCE] Update stale.yml (#5602) [MAINTENANCE] Add id to CheckpointValidationConfig (#5603) [MAINTENANCE] Better error message for RuntimeDataConnector for BatchIdentifiers (#5635) [MAINTENANCE] type-checking round 2 (#5576) [MAINTENANCE] minor cleanup of old comments (#5641) [MAINTENANCE] add --clear-cache flag for invoke type-check (#5639) [MAINTENANCE] Install dgtest test runner utilizing Git URL in CI (#5645) [MAINTENANCE] Make comparisons of aggregate values date aware (#5642) (thanks @jcampbell) [MAINTENANCE] Add E2E Cloud test for DataContext.add_checkpoint() (#5653) [MAINTENANCE] Use docker to run tests in the Azure CI pipeline. (#5646) [MAINTENANCE] add new invoke tasks to tasks.py and create new file usage_stats_utils.py (#5593) [MAINTENANCE] Don't include 'test-pipeline' in extras_require dict (#5659) [MAINTENANCE] move tool config to pyproject.toml (#5649) [MAINTENANCE] Refactor docker test CI steps into jobs. (#5665) [MAINTENANCE] Only run Cloud E2E tests in primary pipeline (#5670) [MAINTENANCE] Improve DateTime Conversion Candling in Comparison Metrics & Expectations and Provide a Clean Object Model for Metrics Computation Bundling (#5656) [MAINTENANCE] Ensure that id_ fields in Marshmallow schema serialize as id (#5660) [MAINTENANCE] data_context initial type checking (#5662)  0.15.16  [FEATURE] Multi-Batch Example Notebook - SqlDataConnector examples (#5575) [FEATURE] Implement \"is_close()\" for making equality comparisons \"reasonably close\" for each ExecutionEngine subclass (#5597) [FEATURE] expect_profile_numeric_columns_percent_diff_(inclusive bounds) (#5586) (thanks @stevensecreti) [FEATURE] DataConnector Query enabled for SimpleSqlDatasource (#5610) [FEATURE] Implement the exact metric range estimate for NumericMetricRangeMultiBatchParameterBuilder (#5620) [FEATURE] Ensure that id propogates from RuleBasedProfilerConfig to RuleBasedProfiler (#5617) [BUGFIX] Pass cloud base url to datasource store (#5595) [BUGFIX] Temporarily disable Trino 0.315.0 from requirements (#5606) [BUGFIX] Update _create_trino_engine to check for schema before creating it (#5607) [BUGFIX] Support ExpectationSuite CRUD at BaseDataContext level (#5604) [BUGFIX] Update test due to change in postgres stdev calculation method (#5624) [BUGFIX] Patch issue with get_validator on Cloud-backed DataContext (#5619) [MAINTENANCE] Add name and id to DatasourceConfig (#5560) [MAINTENANCE] Clear datasources in test_data_context_datasources to improve test performance and narrow test scope (#5588) [MAINTENANCE] Fix tests that rely on guessing pytest generated random file paths. (#5589) [MAINTENANCE] Do not set google cloud credentials for lifetime of pytest process. (#5592) [MAINTENANCE] Misc updates to Datasource CRUD on DataContext to ensure consistent behavior (#5584) [MAINTENANCE] Add id to RuleBasedProfiler config (#5590) [MAINTENANCE] refactor to enable customization of quantile bias correction threshold for bootstrap estimation method (#5587) [MAINTENANCE] Ensure that resource_type used in GeCloudStoreBackend is converted to GeCloudRESTResource enum as needed (#5601) [MAINTENANCE] Create datasource with id (#5591) [MAINTENANCE] Enable Azure blob storage integration tests (#5594) [MAINTENANCE] Increase expectation kwarg line stroke width (#5608) [MAINTENANCE] Added Algolia Scripts (#5544) (thanks @devanshdixit) [MAINTENANCE] Handle numpy deprecation warnings (#5615) [MAINTENANCE] remove approximate comparisons -- they will be replaced by estimator alternatives (#5618) [MAINTENANCE] Making the dependency on dev-lite clearer (#5514) [MAINTENANCE] Fix tests in tests/integration/profiling/rule_based_profiler/ and tests/render/renderer/ (#5611) [MAINTENANCE] DataContext in cloud mode test add_datasource (#5625)  0.15.15  [FEATURE] Integrate DataContextVariables with DataContext (#5466) [FEATURE] Add mostly to MulticolumnMapExpectation (#5481) [FEATURE] [MAINTENANCE] Revamped expect_profile_numeric_columns_diff_between_exclusive_threshold_range (#5493) (thanks @stevensecreti) [FEATURE] [CONTRIB] expect_profile_numeric_columns_diff_(less/greater)_than_or_equal_to_threshold (#5522) (thanks @stevensecreti) [FEATURE] Provide methods for returning ExpectationConfiguration list grouped by expectation_type and by domain_type (#5532) [FEATURE] add support for Azure authentication methods (#5229) (thanks @sdebruyn) [FEATURE] Show grouped sorted expectations by Domain and by expectation_type (#5539) [FEATURE] Categorical Rule in VolumeDataAssistant Should Use Same Cardinality As Categorical Rule in OnboardingDataAssistant (#5551) [BUGFIX] Handle \"division by zero\" in \"ColumnPartition\" metric when all column values are NULL (#5507) [BUGFIX] Use string dialect name if not found in enum (#5546) [BUGFIX] Add try/except around DataContext._save_project_config to mitigate issues with permissions (#5550) [BUGFIX] Explicitly pass in mostly as 1 if not set in configuration. (#5548) [BUGFIX] Increase precision for categorical rule for fractional comparisons (#5552) [DOCS] DOC-340 partition local installation guide (#5425) [DOCS] Add DataHub Ingestion docs  (#5330) (thanks @maggiehays) [DOCS] toc update for DataHub integration doc (#5518) [DOCS] Updating discourse to GitHub Discussions in Docs (#4953) [MAINTENANCE] Clean up payload for /data-context-variables endpoint to adhere to desired chema (#5509) [MAINTENANCE] DataContext Refactor: DataAssistants (#5472) [MAINTENANCE] Ensure that validation operators are omitted from Cloud variables payload (#5510) [MAINTENANCE] Add end-to-end tests for multicolumn map expectations (#5517) [MAINTENANCE] Ensure that *_store_name attrs are omitted from Cloud variables payload (#5519) [MAINTENANCE] Refactor key arg out of Store.serialize/deserialize (#5511) [MAINTENANCE] Fix links to documentation (#5177) (thanks @andyjessen) [MAINTENANCE] Readme Update (#4952) [MAINTENANCE] E2E test for FileDataContextVariables (#5516) [MAINTENANCE] Cleanup/refactor prerequisite for group/filter/sort Expectations by domain (#5523) [MAINTENANCE] Refactor GeCloudStoreBackend to use PUT and DELETE HTTP verbs instead of PATCH (#5527) [MAINTENANCE] /profiler Cloud endpoint support (#5499) [MAINTENANCE] Add type hints to Store (#5529) [MAINTENANCE] Move MetricDomainTypes to core (it is used more widely now than previously). (#5530) [MAINTENANCE] Remove dependency pins on pyarrow and snowflake-connector-python (#5533) [MAINTENANCE] use invoke for common contrib/dev tasks (#5506) [MAINTENANCE] Add snowflake-connector-python dependency lower bound. (#5538) [MAINTENANCE] enforce pre-commit in ci (#5526) [MAINTENANCE] Providing more robust error handling for determining domain_type of an ExpectationConfiguration object (#5542) [MAINTENANCE] Remove extra indentation from store backend test (#5545) [MAINTENANCE] Plot-level dropdown for DataAssistantResult display charts (#5528) [MAINTENANCE] Make DataAssistantResult.batch_id_to_batch_identifier_display_name_map private (in order to optimize auto-complete for ease of use) (#5549) [MAINTENANCE] Initial Dockerfile for running tests and associated README. (#5541) [MAINTENANCE] Other dialect test (#5547)  0.15.14  [FEATURE] QueryExpectations (#5223) [FEATURE] Control volume of metadata output when running DataAssistant classes. (#5483) [BUGFIX] Snowflake Docs Integration Test Fix (#5463) [BUGFIX] DataProfiler Linting Fix (#5468) [BUGFIX] Update renderer snapshots with None values removed (#5474) [BUGFIX] Rendering Test failures (#5475) [BUGFIX] Update dependency-graph pipeline YAML to ensure --spark gets passed to dgtest (#5477) [BUGFIX] Make sure the profileReport obj does not have defaultdicts (breaks gallery JSON) (#5491) [BUGFIX] Use Pandas.isnull() instead of NumPy.isnan() to check for empty values in TableExpectation._validate_metric_value_between(), due to wider types applicability. (#5502) [BUGFIX] Spark Schema has unexpected field for spark.sql.warehouse.dir (#5490) [BUGFIX] Conditionally pop values from Spark config in tests (#5508) [DOCS] DOC-349 re-write and partition interactive mode expectations guide (#5448) [DOCS] DOC-344 partition data docs on s3 guide (#5437) [DOCS] DOC-342 partition how to configure a validation result store in amazon s3 guide (#5428) [DOCS] link fix in onboarding data assistant guide (#5469) [DOCS] Integrate great-expectation with ydata-synthetic (#4568) (thanks @arunnthevapalan) [DOCS] Add 'test' extra to setup.py with docs (#5415) [DOCS] DOC-343 partition how to configure expectation store for aws s3 guide (#5429) [DOCS] DOC-357 partition the how to create a new checkpoint guide (#5458) [DOCS] Remove outdated release process docs. (#5484) [MAINTENANCE] Update teams.yml (#5457) [MAINTENANCE] Clean up GitHub Actions (#5461) [MAINTENANCE] Adds documentation and examples changes for snowflake connection string (#5447) [MAINTENANCE] DOC-345 partition the connect to s3 cloud storage with Pandas guide (#5439) [MAINTENANCE] Add unit and integration tests for Splitting on Mod Integer  (#5452) [MAINTENANCE] Remove InlineRenderer invocation feature flag from ExpectationValidationResult (#5441) [MAINTENANCE] DataContext Refactor. Migration of datasource and store (#5404) [MAINTENANCE] Add unit and integration tests for Splitting on Multi-Column Values (#5464) [MAINTENANCE] Refactor DataContextVariables to leverage @property and @setter (#5446) [MAINTENANCE] expect_profile_numeric_columns_diff_between_threshold_range (#5467) (thanks @stevensecreti) [MAINTENANCE] Make DataAssistantResult fixtures module scoped (#5465) [MAINTENANCE] Remove keyword arguments within table row count expectations (#4874) (thanks @andyjessen) [MAINTENANCE] Add unit tests for Splitting on Converted DateTime (#5470) [MAINTENANCE] Rearrange integration tests to insure categorization into proper deployment-style based lists (#5471) [MAINTENANCE] Provide better error messaging if batch_request is not supplied to DataAssistant.run() (#5473) [MAINTENANCE] Adds run time envvar for Snowflake Partner ID (#5485) [MAINTENANCE] fixed algolia search page (#5099) [MAINTENANCE] Remove pyspark<3.0.0 constraint for python 3.7 (#5496) [MAINTENANCE] Ensure that parter-integration pipeline only runs on cronjob (#5500) [MAINTENANCE] Adding fixtures Query Expectations tests  (#5486) [MAINTENANCE] Misc updates to GeCloudStoreBackend to better integrate with GE Cloud (#5497) [MAINTENANCE] Update automated release schedule (#5488) [MAINTENANCE] Update core-team in teams.yml (#5489) [MAINTENANCE] Update how_to_create_a_new_expectation_suite_using_rule_based_profile\u2026 (#5495) [MAINTENANCE] Remove pypandoc pin in constraints-dev.txt. (#5501) [MAINTENANCE] Ensure that add_datasource method on AbstractDataContext does not persist by default (#5482)  0.15.13  [FEATURE] Add atomic rendered_content to ExpectationValidationResult and ExpectationConfiguration (#5369) [FEATURE] Add DataContext.update_datasource CRUD method (#5417) [FEATURE] Refactor Splitter Testing Modules so as to Make them More General and Add Unit and Integration Tests for \"split_on_whole_table\" and \"split_on_column_value\" on SQLite and All Supported Major SQL Backends (#5430) [FEATURE] Support underscore in the condition_value of a row_condition (#5393) (thanks @sp1thas) [DOCS] DOC-322 update terminology to v3 (#5326) [MAINTENANCE] Change property name of TaxiSplittingTestCase to make it more general (#5419) [MAINTENANCE] Ensure that BaseDataContext does not persist Datasource changes by default (#5423) [MAINTENANCE] Migration of project_config_with_variables_substituted to AbstractDataContext (#5385) [MAINTENANCE] Improve type hinting in GeCloudStoreBackend (#5427) [MAINTENANCE] Test serialization of text, table, and bulleted list rendered_content in ExpectationValidationResult (#5438) [MAINTENANCE] Refactor datasource_name out of DataContext.update_datasource (#5440) [MAINTENANCE] Add checkpoint name to validation results (#5442) [MAINTENANCE] Remove checkpoint from top level of schema since it is captured in meta (#5445) [MAINTENANCE] Add unit and integration tests for Splitting on Divided Integer (#5449) [MAINTENANCE] Update cli with new default simple checkpoint name (#5450)  0.15.12  [FEATURE] Add Rule Statistics to DataAssistantResult for display in Jupyter notebook (#5368) [FEATURE] Include detailed Rule Execution statistics in jupyter notebook \"repr\" style output (#5375) [FEATURE] Support datetime/date-part splitters on Amazon Redshift (#5408) [DOCS] Capital One DataProfiler Expectations README Update (#5365) (thanks @stevensecreti) [DOCS] Add Trino guide (#5287) [DOCS] DOC-339 remove redundant how-to guide (#5396) [DOCS] Capital One Data Profiler README update (#5387) (thanks @taylorfturner) [DOCS] Add sqlalchemy-redshfit to dependencies in redshift doc (#5386) [MAINTENANCE] Reduce output amount in Jupyter notebooks when displaying DataAssistantResult (#5362) [MAINTENANCE] Update linter thresholds (#5367) [MAINTENANCE] Move _apply_global_config_overrides() to AbstractDataContext (#5285) [MAINTENANCE] WIP: [MAINTENANCE] stalebot configuration (#5301) [MAINTENANCE] expect_column_values_to_be_equal_to_or_greater_than_profile_min (#5372) (thanks @stevensecreti) [MAINTENANCE] expect_column_values_to_be_equal_to_or_less_than_profile_max (#5380) (thanks @stevensecreti) [MAINTENANCE] Replace string formatting with f-string (#5225) (thanks @andyjessen) [MAINTENANCE] Fix links in docs (#5340) (thanks @andyjessen) [MAINTENANCE] Caching of config_variables in DataContext (#5376) [MAINTENANCE] StaleBot Half DryRun (#5390) [MAINTENANCE] StaleBot DryRun 2  (#5391) [MAINTENANCE] file extentions applied to rel links (#5399) [MAINTENANCE] Allow installing jinja2 version 3.1.0 and higher (#5382) [MAINTENANCE] expect_column_values_confidence_for_data_label_to_be_less_than_or_equal_to_threshold (#5392) (thanks @stevensecreti) [MAINTENANCE] Add warnings to internal linters if actual error count does not match threshold (#5401) [MAINTENANCE] Ensure that changes made to env vars / config vars are recognized within subsequent calls of the same process (#5410) [MAINTENANCE] Stack RuleBasedProfiler progress bars for better user experience (#5400) [MAINTENANCE] Keep all Pandas Splitter Tests in a Dedicated Module (#5411) [MAINTENANCE] Refactor DataContextVariables to only persist state to Store using explicit save command (#5366) [MAINTENANCE] Refactor to put tests for splitting and sampling into modules for respective ExecutionEngine implementation (#5412)  0.15.11  [FEATURE] Enable NumericMetricRangeMultiBatchParameterBuilder to use evaluation dependencies (#5323) [FEATURE] Improve Trino Support (#5261) (thanks @aezomz) [FEATURE] added support to Aws Athena quantiles (#5114) (thanks @kuhnen) [FEATURE] Implement the \"column.standard_deviation\" metric for sqlite database (#5338) [FEATURE] Update add_datasource to leverage the DatasourceStore (#5334) [FEATURE] Provide ability for DataAssistant to return its effective underlying BaseRuleBasedProfiler configuration (#5359) [BUGFIX] Fix Netlify build issue that was being caused by entry in changelog (#5322) [BUGFIX] Numpy dtype.float64 formatted floating point numbers must be converted to Python float for use in SQLAlchemy Boolean clauses (#5336) [BUGFIX] Fix for failing Expectation test in cloud_db_integration pipeline (#5321) [DOCS] revert getting started tutorial to RBP process (#5307) [DOCS] mark onboarding assistant guide as experimental and update cli command (#5308) [DOCS] Fix line numbers in getting started guide (#5324) [DOCS] DOC-337 automate updates to the version information displayed in the getting started tutorial. (#5348) [MAINTENANCE] Fix link in suite profile renderer (#5242) (thanks @andyjessen) [MAINTENANCE] Refactor of _apply_global_config_overrides() method to return config (#5286) [MAINTENANCE] Remove \"json_serialize\" directive from ParameterBuilder computations (#5320) [MAINTENANCE] Misc cleanup post 0.15.10 release (#5325) [MAINTENANCE] Standardize instantiation of NumericMetricRangeMultibatchParameterBuilder throughout the codebase. (#5327) [MAINTENANCE] Reuse MetricMultiBatchParameterBuilder computation results as evaluation dependencies for performance enhancement (#5329) [MAINTENANCE] clean up type declarations (#5331) [MAINTENANCE] Maintenance/great 761/great 1010/great 1011/alexsherstinsky/rule based profiler/data assistant/include only essential public methods in data assistant dispatcher class 2022 06 21 177 (#5351) [MAINTENANCE] Update release schedule JSON (#5349) [MAINTENANCE] Include only essential public methods in DataAssistantResult class (and its descendants) (#5360)  0.15.10  [FEATURE] DataContextVariables CRUD for stores (#5268) [FEATURE] DataContextVariables CRUD for data_docs_sites (#5269) [FEATURE] DataContextVariables CRUD for anonymous_usage_statistics (#5271) [FEATURE] DataContextVariables CRUD for notebooks  (#5272) [FEATURE] DataContextVariables CRUD for concurrency (#5273) [FEATURE] DataContextVariables CRUD for progress_bars (#5274) [FEATURE] Integrate DatasourceStore with DataContext (#5292) [FEATURE] Support both UserConfigurableProfiler and OnboardingDataAssistant in \"CLI SUITE NEW --PROFILE name\" command (#5306) [BUGFIX] Fix ColumnPartition metric handling of the number of bins (must always be integer). (#5282) [BUGFIX] Add new high precision rule for mean and stdev in OnboardingDataAssistant (#5276) [BUGFIX] Warning in Getting Started Guide notebook. (#5297) [DOCS] how to create an expectation suite with the onboarding assistant (#5266) [DOCS] update getting started tutorial for onboarding assistant (#5294) [DOCS] getting started tutorial doc standards updates (#5295) [DOCS] Update standard arguments doc for Expectations to not reference datasets. (#5052) [MAINTENANCE] Add check to check_type_hint_coverage script to ensure proper mypy installation (#5291) [MAINTENANCE] DataAssistantResult cleanup and extensibility enhancements (#5259) [MAINTENANCE] Handle compare Expectation in presence of high precision floating point numbers and NaN values (#5298) [MAINTENANCE] Suppress persisting of temporary ExpectationSuite configurations in Rule-Based Profiler computations (#5305) [MAINTENANCE] Adds column values github user validation (#5302) [MAINTENANCE] Adds column values IATA code validation (#5303) [MAINTENANCE] Adds column values ARN validation (#5304) [MAINTENANCE] Fixing a typo in a comment (in several files) (#5310) [MAINTENANCE] Adds column scientific notation string validation (#5309) [MAINTENANCE] lint fixes (#5312) [MAINTENANCE] Adds column value JSON validation (#5313) [MAINTENANCE] Expect column values to be valid scientific notation (#5311)  0.15.9  [FEATURE] Add new expectation: expect column values to match powers of a base g\u2026 (#5219) (thanks @rifatKomodoDragon) [FEATURE] Replace UserConfigurableProfiler with OnboardingDataAssistant in \"CLI suite new --profile\" Jupyter Notebooks (#5236) [FEATURE] DatasourceStore (#5206) [FEATURE] add new expectation on validating hexadecimals (#5188) (thanks @andrewsx) [FEATURE] Usage Statistics Events for Profiler and DataAssistant \"get_expectation_suite()\" methods. (#5251) [FEATURE] InlineStoreBackend (#5216) [FEATURE] The \"column.histogram\" metric must support integer values of the \"bins\" parameter for all execution engine options. (#5258) [FEATURE] Initial implementation of DataContextVariables accessors (#5238) [FEATURE] OnboardingDataAssistant plots for expect_table_columns_to_match_set (#5208) [FEATURE] DataContextVariables CRUD for config_variables_file_path (#5262) [FEATURE] DataContextVariables CRUD for plugins_directory (#5263) [FEATURE] DataContextVariables CRUD for store name accessors (#5264) [BUGFIX] Hive temporary tables creation fix (#4956) (thanks @jaume-ferrarons) [BUGFIX] Provide error handling when metric fails for all Batch data samples (#5256) [BUGFIX] Patch automated release test date comparisons (#5278) [DOCS] How to compare two tables with the UserConfigurableProfiler (#5050) [DOCS] How to create a Custom Column Pair Map Expectation w/ supporting template & example (#4926) [DOCS] Auto API documentation script (#4964) [DOCS] Update formatting of links to public methods in class docs generated by auto API script (#5247) [DOCS] In the reference section of the ToC remove duplicates and update category pages  (#5248) [DOCS] Update DataContext docstring (#5250) [MAINTENANCE] Add CodeSee architecture diagram workflow to repository (#5235) (thanks @codesee-maps[bot]) [MAINTENANCE] Fix links to API docs (#5246) (thanks @andyjessen) [MAINTENANCE] Unpin cryptography upper bound (#5249) [MAINTENANCE] Don't use jupyter-client 7.3.2 (#5252) [MAINTENANCE] Re-introduce jupyter-client 7.3.2 (#5253) [MAINTENANCE] Add cloud mark to pytest.ini (#5254) [MAINTENANCE] add partner integration framework (#5132) [MAINTENANCE] DataContextVariableKey for use in Stores (#5255) [MAINTENANCE] Clarification of events in test with multiple checkpoint validations (#5257) [MAINTENANCE] Misc updates to improve security and automation of the weekly release process (#5244) [MAINTENANCE] show more test output and minor fixes (#5239) [MAINTENANCE] Add proper unit tests for Column Histogram metric and use Column Value Partitioner in OnboardingDataAssistant (#5267) [MAINTENANCE] Updates contributor docs to reflect updated linting guidance (#4909) [MAINTENANCE] Remove condition from autoupdate GitHub action (#5270) [MAINTENANCE] Improve code readability in the processing section of \"MapMetricColumnDomainBuilder\". (#5279)  0.15.8  [FEATURE] OnboardingDataAssistant plots for expect_table_row_count_to_be_between non-sequential batches (#5212) [FEATURE] Limit sampling for spark and pandas (#5201) [FEATURE] Groundwork for DataContext Refactor (#5203) [FEATURE] Implement ability to change rule variable values through DataAssistant run() method arguments at runtime (#5218) [FEATURE] Plot numeric column domains in OnboardingDataAssistant (#5189) [BUGFIX] Repair \"CLI Suite --Profile\" Operation (#5230) [DOCS] Remove leading underscore from sampling docs (#5214) [MAINTENANCE] suppressing type hints in ill-defined situations (#5213) [MAINTENANCE] Change CategoricalColumnDomainBuilder property name from \"limit_mode\" to \"cardinality_limit_mode\". (#5215) [MAINTENANCE] Update Note in BigQuery Docs (#5197) [MAINTENANCE] Sampling cleanup refactor (use BatchSpec in sampling methods) (#5217) [MAINTENANCE] Globally increase Azure timeouts to 120 mins (#5222) [MAINTENANCE] Comment out kl_divergence for build_gallery (#5196) [MAINTENANCE] Fix docstring on expectation (#5204) (thanks @andyjessen) [MAINTENANCE] Improve NaN handling in numeric ParameterBuilder implementations (#5226) [MAINTENANCE] Update type hint and docstring linter thresholds (#5228)  0.15.7  [FEATURE] Add Rule for TEXT semantic domains within the Onboarding Assistant (#5144) [FEATURE] Helper method to determine whether Expectation is self-initializing  (#5159) [FEATURE] OnboardingDataAssistantResult plotting feature parity with VolumeDataAssistantResult (#5145) [FEATURE] Example Notebook for self-initializing Expectations (#5169) [FEATURE] DataAssistant: Enable passing directives to run() method using runtime_environment argument (#5187) [FEATURE] Adding DataAssistantResult.get_expectation_suite(expectation_suite_name) method (#5191) [FEATURE] Cronjob to automatically create release PR (#5181) [BUGFIX] Insure TABLE Domain Metrics Do Not Get Column Key From Column Type Rule Domain Builder (#5166) [BUGFIX] Update name for stdev expectation in OnboardingDataAssistant backend (#5193) [BUGFIX] OnboardingDataAssistant and Underlying Metrics: Add Defensive Programming Into Metric Implementations So As To Avoid Warnings About Incompatible Data (#5195) [BUGFIX] Insure that Histogram Metric in Pandas operates on numerical columns that do not have NULL values (#5199) [BUGFIX] RuleBasedProfiler: Ensure that run() method runtime environment directives are handled correctly when existing setting is None (by default) (#5202) [BUGFIX] In aggregate metrics, Spark Implementation already gets Column type as argument -- no need for F.col() as the operand is not a string. (#5207) [DOCS] Update ToC with category links (#5155) [DOCS] update on availability and parameters of conditional expectations (#5150) [MAINTENANCE] Helper method for RBP Notebook tests that does clean-up (#5171) [MAINTENANCE] Increase timeout for longer stages in Azure pipelines (#5175) [MAINTENANCE] Rule-Based Profiler -- In ParameterBuilder insure that metrics are validated for conversion to numpy array (to avoid deprecation warnings) (#5173) [MAINTENANCE] Increase timeout in packaging & installation pipeline (#5178) [MAINTENANCE] OnboardingDataAssistant handle multiple expectations per domain (#5170) [MAINTENANCE] Update timeout in pipelines to fit Azure syntax (#5180) [MAINTENANCE] Error message when Validator is instantiated with Incorrect BatchRequest (#5172) [MAINTENANCE] Don't include infinity in rendered string for diagnostics (#5190) [MAINTENANCE] Mark Great Expectations Cloud tests and add stage to CI/CD (#5186) [MAINTENANCE] Trigger expectation gallery build with scheduled CI/CD runs (#5192) [MAINTENANCE] expectation_gallery Azure pipeline (#5194) [MAINTENANCE] General cleanup/refactor of DataAssistantResult (#5198)  0.15.6  [FEATURE] NumericMetricRangeMultiBatchParameterBuilder kernel density estimation (#5084) [FEATURE] Splitters and limit sample work on AWS Athena (#5024) [FEATURE] ColumnValuesLengthMin and ColumnValuesLengthMax metrics (#5107) [FEATURE] Use batch_identifiers in plot tooltips (#5091) [FEATURE] Updated DataAssistantResult plotting API (#5117) [FEATURE] Onboarding DataAssistant: Numeric Rules and Relevant Metrics (#5120) [FEATURE] DateTime Rule for OnboardingDataAssistant (#5121) [FEATURE] Categorical Rule is added to OnboardingDataAssistant (#5134) [FEATURE] OnboardingDataAssistant: Introduce MeanTableColumnsSetMatchMultiBatchParameterBuilder (to enable expect_table_columns_to_match_set) (#5135) [FEATURE] Giving the \"expect_table_columns_to_match_set\" Expectation Self-Initializing Capabilities. (#5136) [FEATURE] For OnboardingDataAssistant: Implement a TABLE Domain level rule to output \"expect_table_columns_to_match_set\" (#5137) [FEATURE] Enable self-initializing ExpectColumnValueLengthsToBeBetween (#4985) [FEATURE] DataAssistant plotting for non-sequential batches (#5126) [BUGFIX] Insure that Batch IDs are accessible in the order in which they were loaded in Validator (#5112) [BUGFIX] Update DataAssistant notebook for new plotting API (#5118) [BUGFIX] For DataAssistants, added try-except for Notebook tests (#5124) [BUGFIX] CategoricalColumnDomainBuilder needs to accept limit_mode with dictionary type (#5127) [BUGFIX] Use external_sqldialect mark to skip during lightweight runs (#5139) [BUGFIX] Use RANDOM_STATE in fixture to make tests deterministic (#5142) [BUGFIX] Read deployment_version instead of using versioneer in deprecation tests (#5147) [MAINTENANCE] DataAssistant: Refactoring Access to common ParameterBuilder instances (#5108) [MAINTENANCE] Refactor ofMetricTypes and AttributedResolvedMetrics (#5100) [MAINTENANCE] Remove references to show_cta_footer except in schemas.py (#5111) [MAINTENANCE] Adding unit tests for sqlalchemy limit sampler part 1 (#5109) [MAINTENANCE] Don't re-raise connection errors in CI (#5115) [MAINTENANCE] Sqlite specific tests for splitting and sampling (#5119) [MAINTENANCE] Add Trino dialect in SqlAlchemyDataset (#5085) (thanks @ms32035) [MAINTENANCE] Move upper bound on sqlalchemy to <2.0.0. (#5140) [MAINTENANCE] Update primary pipeline to cut releases with tags (#5128) [MAINTENANCE] Improve handling of \"expect_column_unique_values_count_to_be_between\" in VolumeDataAssistant (#5146) [MAINTENANCE] Simplify DataAssistant Operation to not Depend on Self-Initializing Expectations (#5148) [MAINTENANCE] Improvements to Trino support (#5152) [MAINTENANCE] Update how_to_configure_a_new_checkpoint_using_test_yaml_config.md (#5157) [MAINTENANCE] Speed up the site builder (#5125) (thanks @tanelk) [MAINTENANCE] remove account id deprecation notice (#5158)  0.15.5  [FEATURE] Add subset operation to Domain class (#5049) [FEATURE] In DataAssistant: Use Domain instead of domain_type as key for Metrics Parameter Builders (#5057) [FEATURE] Self-initializing ExpectColumnStddevToBeBetween (#5065) [FEATURE] Enum used by DateSplitter able to be represented as YAML (#5073) [FEATURE] Implementation of auto-complete for DataAssistant class names in Jupyter notebooks (#5077) [FEATURE] Provide display (\"friendly\") names for batch identifiers (#5086) [FEATURE] Onboarding DataAssistant -- Initial Rule Implementations (Data Aspects) (#5101) [FEATURE] OnboardingDataAssistant: Implement Nullity/Non-nullity Rules and Associated Metrics (#5104) [BUGFIX] self_check() now also checks for aws_config_file (#5040) [BUGFIX] multi_batch_rule_based_profiler test up to date with RBP changes (#5066) [BUGFIX] Splitting Support at Asset level (#5026) [BUGFIX] Make self-initialization in expect_column_values_to_be_between truly multi batch (#5068) [BUGFIX] databricks engine create temporary view (#4994) (thanks @gvillafanetapia) [BUGFIX] Patch broken Expectation gallery script (#5090) [BUGFIX] Sampling support at asset level (#5092) [DOCS] Update process and configurations in OpenLineage Action guide. (#5039) [DOCS] Update process and config examples in Opsgenie guide (#5037) [DOCS] Correct name of openlineage-integration-common package (#5041) (thanks @mobuchowski) [DOCS] Remove reference to validation operator process from how to trigger slack notifications guide (#5034) [DOCS] Update process and configuration examples in email Action guide. (#5036) [DOCS] Update Docusaurus version (#5063) [MAINTENANCE] Saved output of usage stats schema script in repo (#5053) [MAINTENANCE] Apply Altair custom themes to return objects (#5044) [MAINTENANCE] Introducing RuleBasedProfilerResult -- neither expectation suite name nor expectation suite must be passed to RuleBasedProfiler.run() (#5061) [MAINTENANCE] Refactor DataAssistant plotting to leverage utility dataclasses (#5022) [MAINTENANCE] Check that a passed string is parseable as an integer (mssql limit param) (#5071) [MAINTENANCE] Clean up mssql limit sampling code path and comments (#5074) [MAINTENANCE] Make saving bootstraps histogram for NumericMetricRangeMultiBatchParameterBuilder  optional (absent by default) (#5075) [MAINTENANCE] Make self-initializing expectations return estimated kwargs with auto-generation timestamp and Great Expectation version (#5076) [MAINTENANCE] Adding a unit test for batch_id mapping to batch display names (#5087) [MAINTENANCE] pypandoc version constraint added (< 1.8) (#5093) [MAINTENANCE] Utilize Rule objects in Profiler construction in DataAssistant (#5089) [MAINTENANCE] Turn off metric calculation progress bars in RuleBasedProfiler and DataAssistant workflows (#5080) [MAINTENANCE] A small refactor of ParamerBuilder management used in DataAssistant classes (#5102) [MAINTENANCE] Convenience method refactor for Onboarding DataAssistant (#5103)  0.15.4  [FEATURE] Enable self-initializing ExpectColumnMeanToBeBetween (#4986) [FEATURE] Enable self-initializing ExpectColumnMedianToBeBetween (#4987) [FEATURE] Enable self-initializing ExpectColumnSumToBeBetween (#4988) [FEATURE] New MetricSingleBatchParameterBuilder for specifically single-Batch Rule-Based Profiler scenarios (#5003) [FEATURE] Enable Pandas DataFrame and Series as MetricValues Output of Metric ParameterBuilder Classes (#5008) [FEATURE] Notebook for VolumeDataAssistant Example (#5010) [FEATURE] Histogram/Partition Single-Batch ParameterBuilder (#5011) [FEATURE] Update DataAssistantResult.plot() return value to emit PlotResult wrapper dataclass (#4962) [FEATURE] Limit samplers work with supported sqlalchemy backends (#5014) [FEATURE] trino support (#5021) [BUGFIX] RBP Profiling Dataset ProgressBar Fix (#4999) [BUGFIX] Fix DataAssistantResult serialization issue (#5020) [DOCS] Update slack notification guide to not use validation operators. (#4978) [MAINTENANCE] Update autoupdate GitHub action (#5001) [MAINTENANCE] Move DataAssistant registry capabilities into DataAssistantRegistry to enable user aliasing (#4991) [MAINTENANCE] Fix continuous partition example (#4939) (thanks @andyjessen) [MAINTENANCE] Preliminary refactors for data samplers. (#4996) [MAINTENANCE] Clean up unused imports and enforce through flake8 in CI/CD (#5005) [MAINTENANCE] ParameterBuilder tests should maximally utilize polymorphism (#5007) [MAINTENANCE] Clean up type hints in CLI (#5006) [MAINTENANCE] Making ParameterBuilder metric computations robust to failures through logging and exception handling (#5009) [MAINTENANCE] Condense column-level vconcat plots into one interactive plot (#5002) [MAINTENANCE] Update version of black in pre-commit config (#5019) [MAINTENANCE] Improve tooltips and formatting for distinct column values chart in VolumeDataAssistantResult (#5017) [MAINTENANCE] Enhance configuring serialization for DotDict type classes (#5023) [MAINTENANCE] Pyarrow upper bound (#5028)  0.15.3  [FEATURE] Enable self-initializing capabilities for ExpectColumnProportionOfUniqueValuesToBeBetween (#4929) [FEATURE] Enable support for plotting both Table and Column charts in VolumeDataAssistant (#4930) [FEATURE] BigQuery Temp Table Support (#4925) [FEATURE] Registry for DataAssistant classes with ability to execute from DataContext by registered name (#4966) [FEATURE] Enable self-intializing capabilities for ExpectColumnValuesToMatchRegex/ExpectColumnValuesToNotMatchRegex (#4958) [FEATURE] Provide \"estimation histogram\" ParameterBuilder output details . (#4975) [FEATURE] Enable self-initializing ExpectColumnValuesToMatchStrftimeFormat (#4977) [BUGFIX] check contrib requirements (#4922) [BUGFIX] Use monkeypatch to set a consistent bootstrap seed in tests (#4960) [BUGFIX] Make all Builder Configuration classes of Rule-Based Profiler Configuration Serializable (#4972) [BUGFIX] extras_require (#4968) [BUGFIX] Fix broken packaging test and update dgtest-overrides (#4976) [MAINTENANCE] Add timeout to great_expectations pipeline stages to prevent false positive build failures (#4957) [MAINTENANCE] Defining Common Test Fixtures for DataAssistant Testing (#4959) [MAINTENANCE] Temporarily pin cryptography package (#4963) [MAINTENANCE] Type annotate relevant functions with -> None (per PEP 484) (#4969) [MAINTENANCE] Handle edge cases where false_positive_rate is not in range [0, 1] or very close to bounds (#4946) [MAINTENANCE] fix a typo  (#4974)  0.15.2  [FEATURE] Split data assets using sql datetime columns (#4871) [FEATURE] Plot metrics with DataAssistantResult.plot() (#4873) [FEATURE] RuleBasedProfiler/DataAssistant/MetricMultiBatchParameterBuilder: Enable Returning Metric Computation Results with batch_id Attribution (#4862) [FEATURE] Enable variables to be specified at both Profiler and its constituent individual Rule levels (#4912) [FEATURE] Enable self-initializing ExpectColumnUniqueValueCountToBeBetween (#4902) [FEATURE] Improve diagnostic testing process (#4816) [FEATURE] Add Azure CI/CD action to aid with style guide enforcement (type hints) (#4878) [FEATURE] Add Azure CI/CD action to aid with style guide enforcement (docstrings) (#4617) [FEATURE] Use formal interfaces to clean up DataAssistant and DataAssistantResult modules/classes (#4901) [BUGFIX] fix validation issue for column domain type and implement expect_column_unique_value_count_to_be_between for VolumeDataAssistant (#4914) [BUGFIX] Fix issue with not using the generated table name on read (#4905) [BUGFIX] Add deprecation comment to RuntimeDataConnector [BUGFIX] Ensure proper class_name within all RuleBasedProfilerConfig instantiations [BUGFIX] fix rounding directive handling (#4887) [BUGFIX] great_expectations import fails when SQL Alchemy is not installed (#4880) [MAINTENANCE] Altair types cleanup (#4916) [MAINTENANCE] test: update test time (#4911) [MAINTENANCE] Add module docstring and simplify access to DatePart (#4910) [MAINTENANCE] Chip away at type hint violations around data context (#4897) [MAINTENANCE] Improve error message outputted to user in DocstringChecker action (#4895) [MAINTENANCE] Re-enable bigquery tests (#4903) [MAINTENANCE] Unit tests for sqlalchemy splitter methods, docs and other improvements (#4900) [MAINTENANCE] Move plot logic from DataAssistant into DataAssistantResult (#4896) [MAINTENANCE] Add condition to primary pipeline to ensure import_ge stage doesn't cause misleading Slack notifications (#4898) [MAINTENANCE] Refactor RuleBasedProfilerConfig (#4882) [MAINTENANCE] Refactor DataAssistant Access to Parameter Computation Results and Plotting Utilities (#4893) [MAINTENANCE] Update dgtest-overrides list to include all test files not captured by primary strategy (#4891) [MAINTENANCE] Add dgtest-overrides section to dependency_graph Azure pipeline [MAINTENANCE] Data Source and DataContext-level tests for RuntimeDataConnector changes (#4866) [MAINTENANCE] Temporarily disable bigquery tests. (#4888) [MAINTENANCE] Import GE after running ge init in packaging CI pipeline (#4885) [MAINTENANCE] Add CI stage importing GE with only required dependencies installed (#4884) [MAINTENANCE] DataAssistantResult.plot() conditional formatting and tooltips (#4881) [MAINTENANCE] split data context files (#4879) [MAINTENANCE] Add Tanner to CODEOWNERS for schemas.py (#4875) [MAINTENANCE]  Use defined constants for ParameterNode accessor keys (#4872)  0.15.1  [FEATURE] Additional Rule-Based Profiler Parameter/Variable Access Methods (#4814) [FEATURE] DataAssistant and VolumeDataAssistant classes (initial implementation -- to be enhanced as part of subsequent work) (#4844) [FEATURE] Add Support for Returning Parameters and Metrics as DataAssistantResult class (#4848) [FEATURE] DataAssistantResult Includes Underlying Profiler Execution Time (#4854) [FEATURE] Add batch_id for every resolved metric_value to ParameterBuilder.get_metrics() result object (#4860) [FEATURE] RuntimeDataConnector able to specify Assets (#4861) [BUGFIX] Linting error from hackathon automerge (#4829) [BUGFIX] Cleanup contrib (#4838) [BUGFIX] Add notebook to GE_REQUIRED_DEPENDENCIES (#4842) [BUGFIX] ParameterContainer return value formatting bug fix (#4840) [BUGFIX] Ensure that Parameter Validation/Configuration Dependency Configurations are included in Serialization (#4843) [BUGFIX] Correctly handle SQLA unexpected count metric for empty tables (#4618) (thanks @douglascook) [BUGFIX] Temporarily adjust Deprecation Warning Count (#4869) [DOCS] How to validate data with an in memory checkpoint (#4820) [DOCS] Update all tutorial redirect fix (#4841) [DOCS] redirect/remove dead links in docs (#4846) [MAINTENANCE] Refactor Rule-Based Profiler instantiation in Validator to make it available as a public method (#4823) [MAINTENANCE] String Type is not needed as Return Type from DomainBuilder.domain_type() (#4827) [MAINTENANCE] Fix Typo in Checkpoint Readme (#4835) (thanks @andyjessen) [MAINTENANCE] Modify conditional expectations readme (#4616) (thanks @andyjessen) [MAINTENANCE] Fix links within datasource new notebook (#4833) (thanks @andyjessen) [MAINTENANCE] Adds missing dependency, which is breaking CLI workflows (#4839) [MAINTENANCE] Update testing and documentation for oneshot estimation method (#4852) [MAINTENANCE] Refactor Datasource tests that work with RuntimeDataConnector by backend.  (#4853) [MAINTENANCE] Update DataAssistant interfaces (#4857) [MAINTENANCE] Improve types returned by DataAssistant interface methods (#4859) [MAINTENANCE] Refactor DataContext tests that work with RuntimeDataConnector by backend (#4858) [HACKATHON] Hackathon PRs in this release  0.15.0  [BREAKING] EOL Python 3.6 (#4567) [FEATURE] Implement Multi-Column Domain Builder for Rule-Based Profiler (#4604) [FEATURE] Update RBP notebook to include example for Multi-Column Domain Builder (#4606) [FEATURE] Rule-Based Profiler: ColumnPairDomainBuilder (#4608) [FEATURE] More package contrib info (#4693) [FEATURE] Introducing RuleState class and RuleOutput class for Rule-Based Profiler in support of richer use cases (such as DataAssistant). (#4704) [FEATURE] Add support for returning fully-qualified parameters names/values from RuleOutput object (#4773) [BUGFIX] Pass random seed to bootstrap estimator (#4605) [BUGFIX] Adjust output of regex ParameterBuilder to match Expectation (#4594) [BUGFIX] Rule-Based Profiler: Only primitive type based BatchRequest is allowed for Builder classes (#4614) [BUGFIX] Fix DataContext templates test (#4678) [BUGFIX] update module_name in NoteBookConfigSchema from v2 path to v3 (#4589) (thanks @Josephmaclean) [BUGFIX] request S3 bucket location only when necessary (#4526) (thanks @error418) [DOCS] Update ignored_columns snippet in \"Getting Started\" (#4609) [DOCS] Fixes import statement.  (#4694) [DOCS] Update tutorial_review.md typo with intended word. (#4611) (thanks @cjbramble) [DOCS] Correct typo in url in docstring for set_based_column_map_expectation_template.py (example script) (#4817) [MAINTENANCE] Add retries to requests in usage stats integration tests (#4600) [MAINTENANCE] Miscellaneous test cleanup (#4602) [MAINTENANCE] Simplify ParameterBuilder.build_parameter() interface (#4622) [MAINTENANCE] War on Warnings - DataContext (#4572) [MAINTENANCE] Update links within great_expectations.yml (#4549) (thanks @andyjessen) [MAINTENANCE] Provide cardinality limit modes from CategoricalColumnDomainBuilder (#4662) [MAINTENANCE] Rule-Based Profiler: Rename Rule.generate() to Rule.run() (#4670) [MAINTENANCE] Refactor ValidationParameter computation (to be more elegant/compact) and fix a type hint in SimpleDateFormatStringParameterBuilder (#4687) [MAINTENANCE] Remove pybigquery check that is no longer needed (#4681) [MAINTENANCE] Rule-Based Profiler: Allow ExpectationConfigurationBuilder to be Optional (#4698) [MAINTENANCE] Slightly Clean Up NumericMetricRangeMultiBatchParameterBuilder (#4699) [MAINTENANCE] ParameterBuilder must not recompute its value, if it already exists in RuleState (ParameterContainer for its Domain). (#4701) [MAINTENANCE] Improve get validator functionality (#4661) [MAINTENANCE] Add checks for mostly=1.0 for all renderers (#4736) [MAINTENANCE] revert to not raising datasource errors on data context init (#4732) [MAINTENANCE] Remove unused bootstrap methods that were migrated to ML Flow (#4742) [MAINTENANCE] Update README.md (#4595) (thanks @andyjessen) [MAINTENANCE] Check for mostly equals 1 in renderers (#4815) [MAINTENANCE] Remove bootstrap tests that are no longer needed (#4818) [HACKATHON] ExpectColumnValuesToBeIsoLanguages (#4627) (thanks @szecsip) [HACKATHON] ExpectColumnAverageLatLonPairwiseDistanceToBeLessThan (#4559) (thanks @mmi333) [HACKATHON] ExpectColumnValuesToBeValidIPv6 (#4561) (thanks @voidforall) [HACKATHON] ExpectColumnValuesToBeValidMac (#4562) (thanks @voidforall) [HACKATHON] ExpectColumnValuesToBeValidMIME (#4563) (thanks @voidforall) [HACKATHON] ExpectColumnValuesToBeValidHexColor (#4564) (thanks @voidforall) [HACKATHON] ExpectColumnValuesToBeValidIban (#4565) (thanks @voidforall) [HACKATHON] ExpectColumnValuesToBeValidIsoCountry (#4566) (thanks @voidforall) [HACKATHON] add expect_column_values_to_be_private_ipv4_class (#4656) (thanks @szecsip) [HACKATHON] Feature/expect column values url hostname match with cert (#4649) (thanks @szecsip) [HACKATHON] add expect_column_values_url_has_got_valid_cert (#4648) (thanks @szecsip) [HACKATHON] add expect_column_values_to_be_valid_us_state_or_territory (#4655) (thanks @Derekma73) [HACKATHON] ExpectColumnValuesToBeValidSsn (#4646) (thanks @voidforall) [HACKATHON] ExpectColumnValuesToBeValidHttpStatusName (#4645) (thanks @voidforall) [HACKATHON] ExpectColumnValuesToBeValidHttpStatusCode (#4644) (thanks @voidforall) [HACKATHON] Feature/expect column values to be daytime (#4643) (thanks @szecsip) [HACKATHON] add expect_column_values_ip_address_in_network (#4640) (thanks @szecsip) [HACKATHON] add expect_column_values_ip_asn_country_code_in_set (#4638) (thanks @szecsip) [HACKATHON] add expect_column_values_to_be_valid_us_state (#4654) (thanks @Derekma73) [HACKATHON] add expect_column_values_to_be_valid_us_state_or_territory_abbreviation (#4653) (thanks @Derekma73) [HACKATHON] add expect_column_values_to_be_weekday (#4636) (thanks @szecsip) [HACKATHON] add expect_column_values_to_be_valid_us_state_abbrevation (#4650) (thanks @Derekma73) [HACKATHON] ExpectColumnValuesGeometryDistanceToAddressToBeBetween (#4652) (thanks @pjdobson) [HACKATHON] ExpectColumnValuesToBeValidUdpPort (#4635) (thanks @voidforall) [HACKATHON] add expect_column_values_to_be_fibonacci_number (#4629) (thanks @szecsip) [HACKATHON] add expect_column_values_to_be_slug (#4628) (thanks @szecsip) [HACKATHON] ExpectColumnValuesGeometryToBeWithinPlace (#4626) (thanks @pjdobson) [HACKATHON] add expect_column_values_to_be_private_ipv6 (#4624) (thanks @szecsip) [HACKATHON] add expect_column_values_to_be_private_ip_v4  (#4623) (thanks @szecsip) [HACKATHON] ExpectColumnValuesToBeValidPrice (#4593) (thanks @voidforall) [HACKATHON] ExpectColumnValuesToBeValidPhonenumber (#4592) (thanks @voidforall) [HACKATHON] ExpectColumnValuesToBePolygonAreaBetween (#4591) (thanks @mmi333) [HACKATHON] ExpectColumnValuesToBeValidTcpPort (#4634) (thanks @voidforall)  0.14.13  [FEATURE] Convert Existing Self-Initializing Expectations to Make ExpectationConfigurationBuilder Self-Contained with its own validation_parameter_builder settings (#4547) [FEATURE] Improve diagnostic checklist details (#4548) [BUGFIX] Moves testing dependencies out of core reqs (#4522) [BUGFIX] Adjust output of datetime ParameterBuilder to match Expectation (#4590) [DOCS] Technical term tags for Adding features to Expectations section of the ToC (#4462) [DOCS] Contributing integrations ToC update. (#4551) [DOCS] Update intro page overview image (#4540) [DOCS] clarifications on execution engines and scalability (#4539) [DOCS] technical terms for validate data advanced (#4535) [DOCS] technical terms for validate data actions docs (#4518) [DOCS] correct code reference line numbers and snippet tags for how to create a batch of data from an in memory data frame (#4573) [DOCS] Update links in page; fix markdown link in html block (#4585) [MAINTENANCE] Don't return from validate configuration methods (#4545) [MAINTENANCE] Rule-Based Profiler: Refactor utilities into appropriate modules/classes for better separation of concerns (#4553) [MAINTENANCE] Refactor global conftest (#4534) [MAINTENANCE] clean up docstrings (#4554) [MAINTENANCE] Small formatting rearrangement for RegexPatternStringParameterBuilder (#4558) [MAINTENANCE] Refactor Anonymizer utilizing the Strategy design pattern (#4485) [MAINTENANCE] Remove duplicate mistune dependency (#4569) [MAINTENANCE] Run PEP273 checks on a schedule or release cut (#4570) [MAINTENANCE] Package dependencies usage stats instrumentation - part 1 (#4546) [MAINTENANCE] Add DevRel team to GitHub auto-label action (#4575) [MAINTENANCE] Add GitHub action to conditionally auto-update PR's  (#4574) [MAINTENANCE] Bump version of black in response to hotfix for Click v8.1.0 (#4577) [MAINTENANCE] Update overview.md (#4556) [MAINTENANCE] Minor clean-up (#4571) [MAINTENANCE] Instrument package dependencies (#4583) [MAINTENANCE] Standardize DomainBuilder Constructor Arguments Ordering (#4599)  0.14.12  [FEATURE] Enables Regex-Based Column Map Expectations (#4315) [FEATURE] Update diagnostic checklist to do linting checks (#4491) [FEATURE] format docstrings as markdown for gallery (#4502) [FEATURE] Introduces SetBasedColumnMapExpectation w/ supporting templates & doc (#4497) [FEATURE] YAMLHandler Class (#4510) [FEATURE] Remove conflict between filter directives and row_conditions (#4488) [FEATURE] Add SNS as a Validation Action (#4519) (thanks @michael-j-thomas) [BUGFIX] Fixes ExpectColumnValuesToBeInSet to enable behavior indicated in Parameterized Expectations Doc (#4455) [BUGFIX] Fixes minor typo in custom expectation docs, adds missing link (#4507) [BUGFIX] Removes validate_config from RegexBasedColumnMap templates & doc (#4506) [BUGFIX] Update ExpectColumnValuesToMatchRegex to support parameterized expectations (#4504) [BUGFIX] Add back nbconvert to dev dependencies (#4515) [BUGFIX] Account for case where SQLAlchemy dialect is not downloaded when masking a given URL (#4516) [BUGFIX] Fix failing test for How to Configure Credentials (#4525) [BUGFIX] Remove Temp Dir (#4528) [BUGFIX] Add pin to Jinja 2 due to API changes in v3.1.0 release (#4537) [BUGFIX] Fixes broken links in How To Write A How-To Guide (#4536) [BUGFIX] Removes cryptography upper bound for general reqs (#4487) [BUGFIX] Don't assume boto3 is installed (#4542) [DOCS] Update tutorial_review.md (#3981) [DOCS] Update AUTHORING_INTRO.md (#4470) (thanks @andyjessen) [DOCS] Add clarification (#4477) (thanks @strickvl) [DOCS] Add missing word and fix wrong dataset reference (#4478) (thanks @strickvl) [DOCS] Adds documentation on how to use Great Expectations with Prefect (#4433) (thanks @desertaxle) [DOCS] technical terms validate data checkpoints (#4486) [DOCS] How to use a Custom Expectation (#4467) [DOCS] Technical Terms for Validate Data: Overview and Core Skills docs (#4465) [DOCS] technical terms create expectations advanced skills (#4441) [DOCS] Integration documentation (#4483) [DOCS] Adding Meltano implementation pattern to docs (#4509) (thanks @pnadolny13) [DOCS] Update tutorial_create_expectations.md (#4512) (thanks @andyjessen) [DOCS] Fix relative links on github (#4479) (thanks @andyjessen) [DOCS] Update README.md (#4533) (thanks @andyjessen) [HACKATHON] ExpectColumnValuesToBeValidIPv4 (#4457) (thanks @voidforall) [HACKATHON] ExpectColumnValuesToBeValidIanaTimezone (#4532) (thanks @lucasasmith) [MAINTENANCE] Clean up Checkpoints documentation and add snippet  (#4474) [MAINTENANCE] Finalize Great Expectations contrib JSON structure (#4482) [MAINTENANCE] Update expectation filenames to match snake_case of their defined Expectations (#4484) [MAINTENANCE] Clean Up Types and Rely on \"to_json_dict()\" where appropriate (#4489) [MAINTENANCE] type hints for Batch Request to be string (which leverages parameter/variable resolution) (#4494) [MAINTENANCE] Insure consistent ordering of arguments to ParameterBuilder instantiations (#4496) [MAINTENANCE] Refactor build_gallery.py script (#4493) [MAINTENANCE] Feature/cloud 385/mask cloud creds (#4444) [MAINTENANCE] Enforce consistent JSON schema through usage stats (#4499) [MAINTENANCE] Applies camel_to_snake util to RegexBasedColumnMapExpectation (#4511) [MAINTENANCE] Removes unused dependencies (#4508) [MAINTENANCE] Revert changes made to dependencies in #4508 (#4520) [MAINTENANCE] Add compatability stage to dependency_graph pipeline (#4514) [MAINTENANCE] Add prod metadata and remove package attribute from library_metadata (#4517) [MAINTENANCE] Move builder instantiation methods to utility module for broader usage among sub-components within Rule-Based Profiler (#4524) [MAINTENANCE] Update package info for Capital One DataProfiler (#4523) [MAINTENANCE] Remove tag 'needs migration to modular expectations api' for some Expectations (#4521) [MAINTENANCE] Add type hints and PyCharm macros in a test module for DefaultExpectationConfigurationBuilder (#4529) [MAINTENANCE] Continue War on Warnings (#4500)  0.14.11  [FEATURE] Script to validate docs snippets line number refs (#4377) [FEATURE] GitHub action to auto label core-team (#4382) [FEATURE] add_rule() method for RuleBasedProfilers and tests (#4358) [FEATURE] Enable the passing of an existing suite to RuleBasedProfiler.run() (#4386) [FEATURE] Impose Ordering on Marshmallow Schema validated Rule-Based Profiler Configuration fields (#4388) [FEATURE] Use more granular requirements-dev-xxx.txt files (#4327) [FEATURE] Rule-Based Profiler: Implement Utilities for getting all available parameter node names and objects resident in memory (#4442) [BUGFIX] Minor Serialization Correction for MeanUnexpectedMapMetricMultiBatchParameterBuilder (#4385) [BUGFIX] Fix CategoricalColumnDomainBuilder to be compliant with serialization / instantiation interfaces (#4395) [BUGFIX] Fix bug around get_parent usage stats utility in test_yaml_config (#4410) [BUGFIX] Adding --spark flag back to azure-pipelines.yml compatibility_matrix stage.  (#4418) [BUGFIX] Remove remaining usage of --no-spark and --no-postgresql flags for pytest (#4425) [BUGFIX] Insure Proper Indexing of Metric Computation Results in ParameterBuilder (#4426) [BUGFIX] Include requirements-dev-contrib.txt in dev-install-matrix.yml for lightweight (#4430) [BUGFIX] Remove pytest-azurepiplines usage from test_cli stages in Azure pipelines (#4432) [BUGFIX] Updates or deletes broken and deprecated example notebooks (#4404) [BUGFIX] Add any dependencies we import directly, but don't have as explicit requirements (#4447) [BUGFIX] Removes potentially sensitive webhook URLs from logging (#4440) [BUGFIX] Fix packaging test (#4452) [DOCS] Fix typo in how_to_create_custom_metrics (#4379) [DOCS] Add snippet tag to gcs data docs (#4383) [DOCS] adjust lines for py reference (#4390) [DOCS] technical tags for connecting to data: core skills docs (#4403) [DOCS] technical term tags for connect to data database documents (#4413) [DOCS] Technical term tags for documentation under Connect to data: Filesystem (#4411) [DOCS] Technical term tags for setup pages (#4392) [DOCS] Technical term tags for Connect to Data: Advanced docs. (#4406) [DOCS] Technical tags: Connect to data:In memory docs (#4405) [DOCS] Add misc snippet tags to existing documentation (#4397) [DOCS] technical terms create expectations: core skills (#4435) [DOCS] Creates Custom Table Expectation How-To (#4399) [HACKATHON] ExpectTableLinearFeatureImportancesToBe (#4400) [MAINTENANCE] Group MAP_SERIES and MAP_CONDITION_SERIES with VALUE-type metrics (#3286) [MAINTENANCE] minor imports cleanup (#4381) [MAINTENANCE] Change schedule for packaging_and_installation pipeline to run at off-hours (#4384) [MAINTENANCE] Implicitly anonymize object based on module (#4387) [MAINTENANCE] Preparatory cleanup refactoring of get_compute_domain (#4371) [MAINTENANCE] RBP -- make parameter builder configurations for self initializing expectations consistent with ParameterBuilder class interfaces (#4398) [MAINTENANCE] Refactor ge_class attr out of Anonymizer and related child classes (#4393) [MAINTENANCE] Removing Custom Expectation Renderer docs from sidebar (#4401) [MAINTENANCE] Enable \"rule_based_profiler.run()\" Method to Accept Batch Data Arguments Directly (#4409) [MAINTENANCE] Refactor out unnecessary Anonymizer child classes (#4408) [MAINTENANCE] Replace \"sampling_method\" with \"estimator\" in Rule-Based Profiler code (#4420) [MAINTENANCE] Add docstrings and type hints to Anonymizer (#4419) [MAINTENANCE] Continue chipping away at warnings (#4422) [MAINTENANCE] Rule-Based Profiler: Standardize on Include/Exclude Column Names List (#4424) [MAINTENANCE] Set upper bound on number of allowed warnings in snippet validation script (#4434) [MAINTENANCE] Clean up of RegexPatternStringParameterBuilder tests to use unittests (#4436)  0.14.10  [FEATURE] ParameterBuilder for Computing Average Unexpected Values Fractions for any Map Metric (#4340) [FEATURE] Improve bootstrap quantile method accuracy (#4270) [FEATURE] Decorate RuleBasedProfiler.run() with usage statistics (#4321) [FEATURE] MapMetricColumnDomainBuilder for Rule-Based Profiler (#4353) [FEATURE] Enable expect_column_min/_max_to_be_between expectations to be self-initializing (#4363) [FEATURE] Azure pipeline to perform nightly CI/CD runs around packaging/installation (#4274) [BUGFIX] Fix IndexError around data asset pagination from CLI (#4346) [BUGFIX] Upper bound pyathena to <2.5.0 (#4350) [BUGFIX] Fixes PyAthena type checking for core expectations & tests (#4359) [BUGFIX] BatchRequest serialization (CLOUD-743) (#4352) [BUGFIX] Update the favicon on docs site (#4376) [BUGFIX] Fix issue with datetime objects in expecatation args (#2652) (thanks @jstammers) [DOCS] Universal map TOC update (#4292) [DOCS] add Config section (#4355) [DOCS] Deployment Patterns to Reference Architectures (#4344) [DOCS] Fixes tutorial link in reference architecture prereqs component (#4360) [DOCS] Tag technical terms in getting started tutorial (#4354) [DOCS] Update overview pages to link to updated tutorial pages. (#4378) [HACKATHON] ExpectColumnValuesToBeValidUUID (#4322) [HACKATHON] add expectation core (#4357) [HACKATHON] ExpectColumnAverageToBeWithinRangeOfGivenPoint (#4356) [MAINTENANCE] rule based profiler minor clean up of ValueSetParameterBuilder (#4332) [MAINTENANCE] Adding tests that exercise single and multi-batch BatchRequests (#4330) [MAINTENANCE] Formalize ParameterBuilder contract API usage in ValueSetParameterBuilder (#4333) [MAINTENANCE] Rule-Based Profiler: Create helpers directory; use column domain generation convenience method (#4335) [MAINTENANCE] Deduplicate table domain kwargs splitting (#4338) [MAINTENANCE] Update Azure CI/CD cron schedule to run more frequently (#4345) [MAINTENANCE] Optimize CategoricalColumnDomainBuilder to compute metrics in a single method call (#4348) [MAINTENANCE] Reduce tries to 2 for probabilistic tests (#4351) [MAINTENANCE] Refactor Checkpoint toolkit (#4342) [MAINTENANCE] Refactor all uses of format in favor of f-strings (#4347) [MAINTENANCE] Update great_expectations_contrib CLI tool to use existing diagnostic classes (#4316) [MAINTENANCE] Setting stage for removal of --no-postgresql and --no-spark flags from pytest. Enable --postgresql and --spark (#4309) [MAINTENANCE] convert unexpected_list contents to hashable type (#4336) [MAINTENANCE] add operator and func handling to stores urns (#4334) [MAINTENANCE]  Refactor ParameterBuilder classes to extend parent class where possible; also, minor cleanup (#4375)  0.14.9  [FEATURE] Enable Simultaneous Execution of all Metric Computations for ParameterBuilder implementations in Rule-Based Profiler (#4282) [FEATURE] Update print_diagnostic_checklist with an option to show any failed tests (#4288) [FEATURE] Self-Initializing Expectations (implemented for three example expectations). (#4258) [FEATURE] ValueSetMultiBatchParameterBuilder and CategoricalColumnDomainBuilder (#4269) [FEATURE] Remove changelog-bot GitHub Action (#4297) [FEATURE] Add requirements-dev-lite.txt and update tests/docs (#4273) [FEATURE] Enable All ParameterBuilder and DomainBuilder classes to accept batch_list generically (#4302) [FEATURE] Enable Probabilistic Tests To Retry upon Assertion Failure (#4308) [FEATURE] Update usage stats schema to account for RBP's run() payload (#4266) [FEATURE] ProfilerRunAnonymizer (#4264) [FEATURE] Enable Expectation \"expect_column_values_to_be_in_set\" to be Self-Initializing (#4318) [BUGFIX] Add redirect for removed Spark EMR page (#4280) [BUGFIX] ConfiguredAssetSqlDataConnector now correctly handles schema and prefix/suffix (#4268) [BUGFIX] Fixes Expectation Diagnostics failing on multi-line docstrings with leading linebreaks (#4286) [BUGFIX] Respect test backends (#4287) [BUGFIX] Skip test__generate_expectations_tests__xxx tests when sqlalchemy isn't there (#4300) [BUGFIX] test_backends integration test fix and supporting docs code ref fixes (#4306) [BUGFIX] Update deep_filter_properties_iterable to ensure that empty values are cleaned (#4298) [BUGFIX] Fixes validate_configuration checking in diagnostics (#4307) [BUGFIX] Update test output that should be returned from generate_diagnostic_checklist (#4317) [BUGFIX] Standardizes imports in expectation templates and examples (#4320) [BUGFIX] Only validate row_condition if not None (#4329) [BUGFIX] Fix PEP273 Windows issue (#4328) [DOCS] Fixes misc. verbiage & typos in new Custom Expectation docs (#4283) [DOCS] fix formatting in configuration details block of Getting Started (#4289) (thanks @afeld) [DOCS] Fixes imports and code refs to expectation templates (#4314) [DOCS] Update creating_custom_expectations/overview.md (#4278) (thanks @binarytom) [CONTRIB] CapitalOne Dataprofiler expectations (#4174) (thanks @taylorfturner) [HACKATHON] ExpectColumnValuesToBeLatLonCoordinatesInRangeOfGivenPoint (#4284) [HACKATHON] ExpectColumnValuesToBeValidDegreeDecimalCoordinates (#4319) [MAINTENANCE] Refactor parameter setting for simpler ParameterBuilder interface (#4299) [MAINTENANCE] SimpleDateTimeFormatStringParameterBuilder and general RBP example config updates (#4304) [MAINTENANCE] Make adherence to Marshmallow Schema more robust (#4325) [MAINTENANCE] Refactor rule based profiler to keep objects/utilities within intended scope (#4331) [MAINTENANCE] Dependabot version upgrades (#4253, #4231, #4058, #4041, #3916, #3886, #3583, #2856, #3370, #3216, #2935, #2855, #3302, #4008, #4252)  0.14.8  [FEATURE] Add run_profiler_on_data method to DataContext (#4190) [FEATURE] RegexPatternStringParameterBuilder for RuleBasedProfiler (#4167) [FEATURE] experimental column map expectation checking for vectors (#3102) (thanks @manyshapes) [FEATURE] Pre-requisites in Rule-Based Profiler for Self-Estimating Expectations (#4242) [FEATURE] Add optional parameter condition to DefaultExpectationConfigurationBuilder (#4246) [BUGFIX] Ensure that test result for RegexPatternStringParameterBuilder is deterministic (#4240) [BUGFIX] Remove duplicate RegexPatternStringParameterBuilder test (#4241) [BUGFIX] Improve pandas version checking in test_expectations[_cfe].py files (#4248) [BUGFIX] Ensure test_script_runner.py actually raises AssertionErrors correctly (#4239) [BUGFIX] Check for pandas>=024 not pandas>=24 (#4263) [BUGFIX] Add support for SqlAlchemyQueryStore connection_string credentials (#4224) (thanks @davidvanrooij) [BUGFIX] Remove assertion (#4271) [DOCS] Hackathon Contribution Docs (#3897) [MAINTENANCE] Rule-Based Profiler: Fix Circular Imports; Configuration Schema Fixes; Enhanced Unit Tests; Pre-Requisites/Refactoring for Self-Estimating Expectations (#4234) [MAINTENANCE] Reformat contrib expectation with black (#4244) [MAINTENANCE] Resolve cyclic import issue with usage stats (#4251) [MAINTENANCE] Additional refactor to clean up cyclic imports in usage stats (#4256) [MAINTENANCE] Rule-Based Profiler prerequisite: fix quantiles profiler configuration and add comments (#4255) [MAINTENANCE] Introspect Batch Request Dictionary for its kind and instantiate accordingly (#4259) [MAINTENANCE] Minor clean up in style of an RBP test fixture; making variables access more robust (#4261) [MAINTENANCE] define empty sqla_bigquery object (#4249)  0.14.7  [FEATURE] Support Multi-Dimensional Metric Computations Generically for Multi-Batch Parameter Builders (#4206) [FEATURE] Add support for sqlalchemy-bigquery while falling back on pybigquery (#4182) [BUGFIX] Update validate_configuration for core Expectations that don't return True (#4216) [DOCS] Fixes two references to the Getting Started tutorial (#4189) [DOCS] Deepnote Deployment Pattern Guide (#4169) [DOCS] Allow Data Docs to be rendered in night mode (#4130) [DOCS] Fix datepicker filter on data docs (#4217) [DOCS] Deepnote Deployment Pattern Image Fixes (#4229) [MAINTENANCE] Refactor RuleBasedProfiler toolkit pattern (#4191) [MAINTENANCE] Revert dependency_graph pipeline changes to ensure usage_stats runs in parallel (#4198) [MAINTENANCE] Refactor relative imports (#4195) [MAINTENANCE] Remove temp file that was accidently committed (#4201) [MAINTENANCE] Update default candidate strings SimpleDateFormatString parameter builder (#4193) [MAINTENANCE] minor type hints clean up (#4214) [MAINTENANCE] RBP testing framework changes (#4184) [MAINTENANCE] add conditional check for 'expect_column_values_to_be_in_type_list' (#4200) [MAINTENANCE] Allow users to pass in any set of polygon points in expectation for point to be within region (#2520) (thanks @ryanlindeborg) [MAINTENANCE] Better support Hive, better support BigQuery. (#2624) (thanks @jacobpgallagher) [MAINTENANCE] move process_evaluation_parameters into conditional (#4109) [MAINTENANCE] Type hint usage stats (#4226)  0.14.6  [FEATURE] Create profiler from DataContext (#4070) [FEATURE] Add read_sas function (#3972) (thanks @andyjessen) [FEATURE] Run profiler from DataContext (#4141) [FEATURE] Instantiate Rule-Based Profiler Using Typed Configuration Object (#4150) [FEATURE] Provide ability to instantiate Checkpoint using CheckpointConfig typed object (#4166) [FEATURE] Misc cleanup around CLI suite command and related utilities (#4158) [FEATURE] Add scheduled runs for primary Azure pipeline (#4117) [FEATURE] Promote dependency graph test strategy to production (#4124) [BUGFIX] minor updates to test definition json files (#4123) [BUGFIX] Fix typo for metric name in expect_column_values_to_be_edtf_parseable (#4140) [BUGFIX] Ensure that CheckpointResult object can be pickled (#4157) [BUGFIX] Custom notebook templates (#2619) (thanks @luke321321) [BUGFIX] Include public fields in property_names (#4159) [DOCS] Reenable docs-under-test for RuleBasedProfiler (#4149) [DOCS] Provided details for using GE_HOME in commandline. (#4164) [MAINTENANCE] Return Rule-Based Profiler base.py to its dedicated config subdirectory (#4125) [MAINTENANCE] enable filter properties dict to handle both inclusion and exclusion lists  (#4127) [MAINTENANCE] Remove unused Great Expectations imports (#4135) [MAINTENANCE] Update trigger for scheduled Azure runs (#4134) [MAINTENANCE] Maintenance/upgrade black (#4136) [MAINTENANCE] Alter great_expectations pipeline trigger to be more consistent (#4138) [MAINTENANCE] Remove remaining unused imports (#4137) [MAINTENANCE] Remove class_name as mandatory field from RuleBasedProfiler (#4139) [MAINTENANCE] Ensure AWSAthena does not create temporary table as part of processing Batch by default, which is currently not supported (#4103) [MAINTENANCE] Remove unused Exception as e instances (#4143) [MAINTENANCE] Standardize DictDot Method Behaviors Formally for Consistent Usage Patterns in Subclasses (#4131) [MAINTENANCE] Remove unused f-strings (#4142) [MAINTENANCE] Minor Validator code clean up -- for better code clarity (#4147) [MAINTENANCE] Refactoring of test_script_runner.py. Integration and Docs tests (#4145) [MAINTENANCE] Remove compatability stage from dependency-graph pipeline (#4161) [MAINTENANCE] CLOUD-618: GE Cloud \"account\" to \"organization\" rename (#4146)  0.14.5  [FEATURE] Delete profilers from DataContext (#4067) [FEATURE] [BUGFIX] Support nullable int column types (#4044) (thanks @scnerd) [FEATURE] Rule-Based Profiler Configuration and Runtime Arguments Reconciliation Logic (#4111) [BUGFIX] Add default BIGQUERY_TYPES (#4096) [BUGFIX] Pin pip --upgrade to a specific version for CI/CD pipeline (#4100) [BUGFIX] Use pip==20.2.4 for usage statistics stage of CI/CD (#4102) [BUGFIX] Fix shared state issue in renderer test (#4000) [BUGFIX] Missing docstrings on validator expect_ methods (#4062) (#4081) [BUGFIX] Fix s3 path suffix bug on windows (#4042) (thanks @scnerd) [MAINTENANCE] fix typos in changelogs (#4093) [MAINTENANCE] Migration of GCP tests to new project (#4072) [MAINTENANCE] Refactor Validator methods (#4095) [MAINTENANCE] Fix Configuration Schema and Refactor Rule-Based Profiler; Initial Implementation of Reconciliation Logic Between Configuration and Runtime Arguments (#4088) [MAINTENANCE] Minor Cleanup -- remove unnecessary default arguments from dictionary cleaner (#4110)  0.14.4  [BUGFIX] Fix typing_extensions requirement to allow for proper build (#4083) (thanks @vojtakopal and @Godoy) [DOCS] data docs action rewrite (#4087) [DOCS] metric store how to rewrite (#4086) [MAINTENANCE] Change logger.warn to logger.warning to remove deprecation warnings (#4085)  0.14.3  [FEATURE] Profiler Store (#3990) [FEATURE] List profilers from DataContext (#4023) [FEATURE] add bigquery json credentials kwargs for sqlalchemy connect (#4039) [FEATURE] Get profilers from DataContext (#4033) [FEATURE] Add RuleBasedProfiler to test_yaml_config utility (#4038) [BUGFIX] Checkpoint Configurator fix to allow notebook logging suppression (#4057) [DOCS] Created a page containing our glossary of terms and definitions. (#4056) [DOCS] swap of old uri for new in data docs generated (#4013) [MAINTENANCE] Refactor test_yaml_config (#4029) [MAINTENANCE] Additional distinction made between V2 and V3 upgrade script (#4046) [MAINTENANCE] Correcting Checkpoint Configuration and Execution Implementation (#4015) [MAINTENANCE] Update minimum version for SQL Alchemy (#4055) [MAINTENANCE] Refactor RBP constructor to work with **kwargs instantiation pattern through config objects (#4043) [MAINTENANCE] Remove unnecessary metric dependency evaluations and add common table column types metric. (#4063) [MAINTENANCE] Clean up new RBP types, method signatures, and method names for the long term. (#4064) [MAINTENANCE] fixed broken function call in CLI (#4068)  0.14.8  [FEATURE] Add run_profiler_on_data method to DataContext (#4190) [FEATURE] RegexPatternStringParameterBuilder for RuleBasedProfiler (#4167) [FEATURE] experimental column map expectation checking for vectors (#3102) (thanks @manyshapes) [FEATURE] Pre-requisites in Rule-Based Profiler for Self-Estimating Expectations (#4242) [FEATURE] Add optional parameter condition to DefaultExpectationConfigurationBuilder (#4246) [BUGFIX] Ensure that test result for RegexPatternStringParameterBuilder is deterministic (#4240) [BUGFIX] Remove duplicate RegexPatternStringParameterBuilder test (#4241) [BUGFIX] Improve pandas version checking in test_expectations[_cfe].py files (#4248) [BUGFIX] Ensure test_script_runner.py actually raises AssertionErrors correctly (#4239) [BUGFIX] Check for pandas>=024 not pandas>=24 (#4263) [BUGFIX] Add support for SqlAlchemyQueryStore connection_string credentials (#4224) (thanks @davidvanrooij) [BUGFIX] Remove assertion (#4271) [DOCS] Hackathon Contribution Docs (#3897) [MAINTENANCE] Rule-Based Profiler: Fix Circular Imports; Configuration Schema Fixes; Enhanced Unit Tests; Pre-Requisites/Refactoring for Self-Estimating Expectations (#4234) [MAINTENANCE] Reformat contrib expectation with black (#4244) [MAINTENANCE] Resolve cyclic import issue with usage stats (#4251) [MAINTENANCE] Additional refactor to clean up cyclic imports in usage stats (#4256) [MAINTENANCE] Rule-Based Profiler prerequisite: fix quantiles profiler configuration and add comments (#4255) [MAINTENANCE] Introspect Batch Request Dictionary for its kind and instantiate accordingly (#4259) [MAINTENANCE] Minor clean up in style of an RBP test fixture; making variables access more robust (#4261) [MAINTENANCE] define empty sqla_bigquery object (#4249)  0.14.7  [FEATURE] Support Multi-Dimensional Metric Computations Generically for Multi-Batch Parameter Builders (#4206) [FEATURE] Add support for sqlalchemy-bigquery while falling back on pybigquery (#4182) [BUGFIX] Update validate_configuration for core Expectations that don't return True (#4216) [DOCS] Fixes two references to the Getting Started tutorial (#4189) [DOCS] Deepnote Deployment Pattern Guide (#4169) [DOCS] Allow Data Docs to be rendered in night mode (#4130) [DOCS] Fix datepicker filter on data docs (#4217) [DOCS] Deepnote Deployment Pattern Image Fixes (#4229) [MAINTENANCE] Refactor RuleBasedProfiler toolkit pattern (#4191) [MAINTENANCE] Revert dependency_graph pipeline changes to ensure usage_stats runs in parallel (#4198) [MAINTENANCE] Refactor relative imports (#4195) [MAINTENANCE] Remove temp file that was accidently committed (#4201) [MAINTENANCE] Update default candidate strings SimpleDateFormatString parameter builder (#4193) [MAINTENANCE] minor type hints clean up (#4214) [MAINTENANCE] RBP testing framework changes (#4184) [MAINTENANCE] add conditional check for 'expect_column_values_to_be_in_type_list' (#4200) [MAINTENANCE] Allow users to pass in any set of polygon points in expectation for point to be within region (#2520) (thanks @ryanlindeborg) [MAINTENANCE] Better support Hive, better support BigQuery. (#2624) (thanks @jacobpgallagher) [MAINTENANCE] move process_evaluation_parameters into conditional (#4109) [MAINTENANCE] Type hint usage stats (#4226)  0.14.6  [FEATURE] Create profiler from DataContext (#4070) [FEATURE] Add read_sas function (#3972) (thanks @andyjessen) [FEATURE] Run profiler from DataContext (#4141) [FEATURE] Instantiate Rule-Based Profiler Using Typed Configuration Object (#4150) [FEATURE] Provide ability to instantiate Checkpoint using CheckpointConfig typed object (#4166) [FEATURE] Misc cleanup around CLI suite command and related utilities (#4158) [FEATURE] Add scheduled runs for primary Azure pipeline (#4117) [FEATURE] Promote dependency graph test strategy to production (#4124) [BUGFIX] minor updates to test definition json files (#4123) [BUGFIX] Fix typo for metric name in expect_column_values_to_be_edtf_parseable (#4140) [BUGFIX] Ensure that CheckpointResult object can be pickled (#4157) [BUGFIX] Custom notebook templates (#2619) (thanks @luke321321) [BUGFIX] Include public fields in property_names (#4159) [DOCS] Reenable docs-under-test for RuleBasedProfiler (#4149) [DOCS] Provided details for using GE_HOME in commandline. (#4164) [MAINTENANCE] Return Rule-Based Profiler base.py to its dedicated config subdirectory (#4125) [MAINTENANCE] enable filter properties dict to handle both inclusion and exclusion lists  (#4127) [MAINTENANCE] Remove unused Great Expectations imports (#4135) [MAINTENANCE] Update trigger for scheduled Azure runs (#4134) [MAINTENANCE] Maintenance/upgrade black (#4136) [MAINTENANCE] Alter great_expectations pipeline trigger to be more consistent (#4138) [MAINTENANCE] Remove remaining unused imports (#4137) [MAINTENANCE] Remove class_name as mandatory field from RuleBasedProfiler (#4139) [MAINTENANCE] Ensure AWSAthena does not create temporary table as part of processing Batch by default, which is currently not supported (#4103) [MAINTENANCE] Remove unused Exception as e instances (#4143) [MAINTENANCE] Standardize DictDot Method Behaviors Formally for Consistent Usage Patterns in Subclasses (#4131) [MAINTENANCE] Remove unused f-strings (#4142) [MAINTENANCE] Minor Validator code clean up -- for better code clarity (#4147) [MAINTENANCE] Refactoring of test_script_runner.py. Integration and Docs tests (#4145) [MAINTENANCE] Remove compatability stage from dependency-graph pipeline (#4161) [MAINTENANCE] CLOUD-618: GE Cloud \"account\" to \"organization\" rename (#4146)  0.14.5  [FEATURE] Delete profilers from DataContext (#4067) [FEATURE] [BUGFIX] Support nullable int column types (#4044) (thanks @scnerd) [FEATURE] Rule-Based Profiler Configuration and Runtime Arguments Reconciliation Logic (#4111) [BUGFIX] Add default BIGQUERY_TYPES (#4096) [BUGFIX] Pin pip --upgrade to a specific version for CI/CD pipeline (#4100) [BUGFIX] Use pip==20.2.4 for usage statistics stage of CI/CD (#4102) [BUGFIX] Fix shared state issue in renderer test (#4000) [BUGFIX] Missing docstrings on validator expect_ methods (#4062) (#4081) [BUGFIX] Fix s3 path suffix bug on windows (#4042) (thanks @scnerd) [MAINTENANCE] fix typos in changelogs (#4093) [MAINTENANCE] Migration of GCP tests to new project (#4072) [MAINTENANCE] Refactor Validator methods (#4095) [MAINTENANCE] Fix Configuration Schema and Refactor Rule-Based Profiler; Initial Implementation of Reconciliation Logic Between Configuration and Runtime Arguments (#4088) [MAINTENANCE] Minor Cleanup -- remove unnecessary default arguments from dictionary cleaner (#4110)  0.14.4  [BUGFIX] Fix typing_extensions requirement to allow for proper build (#4083) (thanks @vojtakopal and @Godoy) [DOCS] data docs action rewrite (#4087) [DOCS] metric store how to rewrite (#4086) [MAINTENANCE] Change logger.warn to logger.warning to remove deprecation warnings (#4085)  0.14.3  [FEATURE] Profiler Store (#3990) [FEATURE] List profilers from DataContext (#4023) [FEATURE] add bigquery json credentials kwargs for sqlalchemy connect (#4039) [FEATURE] Get profilers from DataContext (#4033) [FEATURE] Add RuleBasedProfiler to test_yaml_config utility (#4038) [BUGFIX] Checkpoint Configurator fix to allow notebook logging suppression (#4057) [DOCS] Created a page containing our glossary of terms and definitions. (#4056) [DOCS] swap of old uri for new in data docs generated (#4013) [MAINTENANCE] Refactor test_yaml_config (#4029) [MAINTENANCE] Additional distinction made between V2 and V3 upgrade script (#4046) [MAINTENANCE] Correcting Checkpoint Configuration and Execution Implementation (#4015) [MAINTENANCE] Update minimum version for SQL Alchemy (#4055) [MAINTENANCE] Refactor RBP constructor to work with **kwargs instantiation pattern through config objects (#4043) [MAINTENANCE] Remove unnecessary metric dependency evaluations and add common table column types metric. (#4063) [MAINTENANCE] Clean up new RBP types, method signatures, and method names for the long term. (#4064) [MAINTENANCE] fixed broken function call in CLI (#4068)  0.14.2  [FEATURE] Marshmallow schema for Rule Based Profiler (#3982) [FEATURE] Enable Rule-Based Profile Parameter Access To Collection Typed Values (#3998) [BUGFIX] Docs integration pipeline bugfix  (#3997) [BUGFIX] Enables spark-native null filtering (#4004) [DOCS] Gtm/cta in docs (#3993) [DOCS] Fix incorrect variable name in how_to_configure_an_expectation_store_in_amazon_s3.md (#3971) (thanks @moritzkoerber) [DOCS] update custom docs css to add a subtle border around tabbed content (#4001) [DOCS] Migration Guide now includes example for Spark data (#3996) [DOCS] Revamp Airflow Deployment Pattern (#3963) (thanks @denimalpaca) [DOCS] updating redirects to reflect a moved file (#4007) [DOCS] typo in gcp + bigquery tutorial (#4018) [DOCS] Additional description of Kubernetes Operators in GCP Deployment Guide (#4019) [DOCS] Migration Guide now includes example for Databases (#4005) [DOCS] Update how to instantiate without a yml file (#3995) [MAINTENANCE] Refactor of test_script_runner.py to break-up test list (#3987) [MAINTENANCE] Small refactor for tests that allows DB setup to be done from all tests (#4012)  0.14.1  [FEATURE] Add pagination/search to CLI batch request listing (#3854) [BUGFIX] Safeguard against using V2 API with V3 Configuration (#3954) [BUGFIX] Bugfix and refactor for cloud-db-integration pipeline (#3977) [BUGFIX] Fixes breaking typo in expect_column_values_to_be_json_parseable (#3983) [BUGFIX] Fixes issue where nested columns could not be addressed properly in spark (#3986) [DOCS] How to connect to your data in mssql (#3950) [DOCS] MigrationGuide - Adding note on Migrating Expectation Suites (#3959) [DOCS] Incremental Update: The Universal Map's Getting Started Tutorial (#3881) [DOCS] Note about creating backup of Checkpoints (#3968) [DOCS] Connecting to BigQuery Doc line references fix (#3974) [DOCS] Remove RTD snippet about comments/suggestions from Docusaurus docs (#3980) [DOCS] Add howto for the OpenLineage validation operator (#3688) (thanks @rossturk) [DOCS] Updates to README.md (#3964) [DOCS] Update migration guide (#3967) [MAINTENANCE] Refactor docs dependency script (#3952) [MAINTENANCE] Use Effective SQLAlchemy for Reflection Fallback Logic and SQL Metrics (#3958) [MAINTENANCE] Remove outdated scripts (#3953) [MAINTENANCE] Add pytest opt to improve collection time (#3976) [MAINTENANCE] Refactor render method in PageRenderer (#3962) [MAINTENANCE] Standardize rule based profiler testing directories organization (#3984) [MAINTENANCE] Metrics Cleanup (#3989) [MAINTENANCE] Refactor render method of Content Block Renderer (#3960)  0.14.0  [BREAKING] Change Default CLI Flag To V3 (#3943) [FEATURE] Cloud-399/Cloud-519: Add Cloud Notification Action (#3891) [FEATURE] great_expectations_contrib CLI tool (#3909) [FEATURE] Update dependency_graph pipeline to use dgtest CLI (#3912) [FEATURE] Incorporate updated dgtest CLI tool in experimental pipeline (#3927) [FEATURE] Add YAML config option to disable progress bars (#3794) [BUGFIX] Fix internal links to docs that may be rendered incorrectly (#3915) [BUGFIX] Update SlackNotificationAction to send slack_token and slack_channel to send_slack_notification function (#3873) (thanks @Calvo94) [BUGFIX] CheckDocsDependenciesChanges to only handle .py files (#3936) [BUGFIX] Provide ability to capture schema_name for SQL-based datasources; fix method usage bugs. (#3938) [BUGFIX] Ensure that Jupyter Notebook cells convert JSON strings to Python-compliant syntax (#3939) [BUGFIX] Cloud-519/cloud notification action return type (#3942) [BUGFIX] Fix issue with regex groups in check_docs_deps (#3949) [DOCS] Created link checker, fixed broken links (#3930) [DOCS] adding the link checker to the build (#3933) [DOCS] Add name to link checker in build (#3935) [DOCS] GCP Deployment Pattern (#3926) [DOCS] remove v3api flag in documentation (#3944) [DOCS] Make corrections in HOWTO Guides for Getting Data from SQL Sources (#3945) [DOCS] Tiny doc fix (#3948) [MAINTENANCE] Fix breaking change caused by the new version of ruamel.yaml (#3908) [MAINTENANCE] Drop extraneous print statement in self_check/util.py. (#3905) [MAINTENANCE] Raise exceptions on init in cloud mode (#3913) [MAINTENANCE] removing commented requirement (#3920) [MAINTENANCE] Patch for atomic renderer snapshot tests (#3918) [MAINTENANCE] Remove types/expectations.py (#3928) [MAINTENANCE] Tests/test data class serializable dot dict (#3924) [MAINTENANCE] Ensure that concurrency is backwards compatible (#3872) [MAINTENANCE] Fix issue where meta was not recognized as a kwarg (#3852)  0.13.49  [FEATURE] PandasExecutionEngine is able to instantiate Google Storage client in Google Cloud Composer (#3896) [BUGFIX] Revert change to ExpectationSuite constructor (#3902) [MAINTENANCE] SQL statements that are of TextClause type expressed as subqueries (#3899)  0.13.48  [DOCS] Updates to configuring credentials (#3856) [DOCS] Add docs on creating suites with the UserConfigurableProfiler (#3877) [DOCS] Update how to configure an expectation store in GCS (#3874) [DOCS] Update how to configure a validation result store in GCS (#3887) [DOCS] Update how to host and share data docs on GCS (#3889) [DOCS] Organize metadata store sidebar category by type of store (#3890) [MAINTENANCE] add_expectation() in ExpectationSuite supports usage statistics for GE.  (#3824) [MAINTENANCE] Clean up Metrics type usage, SQLAlchemyExecutionEngine and SQLAlchemyBatchData implementation, and SQLAlchemy API usage (#3884)  0.13.47  [FEATURE] Add support for named groups in data asset regex (#3855) [BUGFIX] Fix issue where dependency graph tester picks up non *.py files and add test file (#3830) [BUGFIX] Ensure proper exit code for dependency graph script (#3839) [BUGFIX] Allows GE to work when installed in a zip file (PEP 273). Fixes issue #3772 (#3798) (thanks @joseignaciorc) [BUGFIX] Update conditional for TextClause isinstance check in SQLAlchemyExecutionEngine (#3844) [BUGFIX] Fix usage stats events (#3857) [BUGFIX] Make ExpectationContext optional and remove when null to ensure backwards compatability (#3859) [BUGFIX] Fix sqlalchemy expect_compound_columns_to_be_unique (#3827) (thanks @harperweaver-dox) [BUGFIX] Ensure proper serialization of SQLAlchemy Legacy Row (#3865) [DOCS] Update migration_guide.md (#3832) [MAINTENANCE] Remove the need for DataContext registry in the instrumentation of the Legacy Profiler profiling method. (#3836) [MAINTENANCE] Remove DataContext registry (#3838) [MAINTENANCE] Refactor cli suite conditionals (#3841) [MAINTENANCE] adding hints to stores in data context (#3849) [MAINTENANCE] Improve usage stats testing (#3858, #3861) [MAINTENANCE] Make checkpoint methods in DataContext pass-through (#3860) [MAINTENANCE] Data Source and ExecutionEngine Anonymizers handle missing module_name (#3867) [MAINTENANCE] Add logging around DatasourceInitializationError in DataContext (#3846) [MAINTENANCE] Use f-string to prevent string concat issue in Evaluation Parameters (#3864) [MAINTENANCE] Test for errors / invalid messages in logs & fix various existing issues (#3875)  0.13.46  [FEATURE] Instrument Runtime DataConnector for Usage Statistics: Add \"checkpoint.run\" Event Schema (#3797) [FEATURE] Add suite creation type field to CLI SUITE \"new\" and \"edit\" Usage Statistics events (#3810) [FEATURE] [EXPERIMENTAL] Dependency graph based testing strategy and related pipeline (#3738, #3815, #3818) [FEATURE] BaseDataContext registry (#3812, #3819) [FEATURE] Add usage statistics instrumentation to Legacy UserConfigurableProfiler execution (#3828) [BUGFIX] CheckpointConfig.deepcopy() must copy all fields, including the null-valued fields (#3793) [BUGFIX] Fix issue where configuration store didn't allow nesting (#3811) [BUGFIX] Fix Minor Bugs in and Clean Up UserConfigurableProfiler (#3822) [BUGFIX] Ensure proper replacement of nulls in Jupyter Notebooks (#3782) [BUGFIX] Fix issue where configuration store didn't allow nesting (#3811) [DOCS] Clean up TOC (#3783) [DOCS] Update Checkpoint and Actions Reference with testing (#3787) [DOCS] Update How to install Great Expectations locally (#3805) [DOCS] How to install Great Expectations in a hosted environment (#3808) [MAINTENANCE] Make BatchData Serialization More Robust (#3791) [MAINTENANCE] Refactor SiteIndexBuilder.build() (#3789) [MAINTENANCE] Update ref to ge-cla-bot in PR template (#3799) [MAINTENANCE] Anonymizer clean up and refactor (#3801) [MAINTENANCE] Certify the expectation \"expect_table_row_count_to_equal_other_table\" for V3 API (#3803) [MAINTENANCE] Refactor to enable broader use of event emitting method for usage statistics (#3825) [MAINTENANCE] Clean up temp file after CI/CD run (#3823) [MAINTENANCE] Raising exceptions for misconfigured datasources in cloud mode (#3866)  0.13.45  [FEATURE] Feature/render validation metadata (#3397) (thanks @vshind1) [FEATURE] Added expectation expect_column_values_to_not_contain_special_characters() (#2849, #3771) (thanks @jaibirsingh) [FEATURE] Like and regex-based expectations in Athena dialect (#3762) (thanks @josges) [FEATURE] Rename deep_filter_properties_dict() to deep_filter_properties_iterable() [FEATURE] Extract validation result failures (#3552) (thanks @BenGale93) [BUGFIX] Allow now() eval parameter to be used by itself (#3719) [BUGFIX] Fixing broken logo for legacy RTD docs (#3769) [BUGFIX] Adds version-handling to sqlalchemy make_url imports (#3768) [BUGFIX] Integration test to avoid regression of simple PandasExecutionEngine workflow (#3770) [BUGFIX] Fix copying of CheckpointConfig for substitution and printing purposes (#3759) [BUGFIX] Fix evaluation parameter usage with Query Store (#3763) [BUGFIX] Feature/fix row condition quotes (#3676) (thanks @benoitLebreton-perso) [BUGFIX] Fix incorrect filling out of anonymized event payload (#3780) [BUGFIX] Don't reset_index for conditional expectations (#3667) (thanks @abekfenn) [DOCS] Update expectations gallery link in V3 notebook documentation (#3747) [DOCS] Correct V3 documentation link in V2 notebooks to point to V2 documentation (#3750) [DOCS] How to pass an in-memory DataFrame to a Checkpoint (#3756) [MAINTENANCE] Fix typo in Getting Started Guide (#3749) [MAINTENANCE] Add proper docstring and type hints to Validator (#3767) [MAINTENANCE] Clean up duplicate logging statements about optional black dep (#3778)  0.13.44  [FEATURE] Add new result_format to include unexpected_row_list (#3346) [FEATURE] Implement \"deep_filter_properties_dict()\" method (#3703) [FEATURE] Create Constants for GETTING_STARTED Entities (e.g., datasource_name, expectation_suite_name, etc.) (#3712) [FEATURE] Add usage statistics event for DataContext.get_batch_list() method (#3708) [FEATURE] Add data_context.run_checkpoint event to usage statistics (#3721) [FEATURE] Add event_duration to usage statistics events (#3729) [FEATURE] InferredAssetSqlDataConnector's introspection can list external tables in Redshift Spectrum (#3646) [BUGFIX] Using a RuntimeBatchRequest in a Checkpoint with a top-level batch_request instead of validations (#3680) [BUGFIX] Using a RuntimeBatchRequest in a Checkpoint at runtime with Checkpoint.run() (#3713) [BUGFIX] Using a RuntimeBatchRequest in a Checkpoint at runtime with context.run_checkpoint() (#3718) [BUGFIX] Use SQLAlchemy make_url helper where applicable when parsing URLs (#3722) [BUGFIX] Adds check for quantile_ranges to be ordered or unbounded pairs (#3724) [BUGFIX] Updates MST renderer to return JSON-parseable boolean (#3728) [BUGFIX] Removes sqlite suppression for expect_column_quantile_values_to_be_between test definitions (#3735) [BUGFIX] Handle contradictory configurations in checkpoint.yml, checkpoint.run(), and context.run_checkpoint() (#3723) [BUGFIX] fixed a bug where expectation metadata doesn't appear in edit template for table-level expectations (#3129) (thanks @olechiw) [BUGFIX] Added temp_table creation for Teradata in SqlAlchemyBatchData (#3731) (thanks @imamolp) [DOCS] Add Databricks video walkthrough link (#3702, #3704) [DOCS] Update the link to configure a MetricStore (#3711, #3714) (thanks @txblackbird) [DOCS] Updated code example to remove deprecated \"File\" function (#3632) (thanks @daccorti) [DOCS] Delete how_to_add_a_validation_operator.md as OBE. (#3734) [DOCS] Update broken link in FOOTER.md to point to V3 documentation (#3745) [MAINTENANCE] Improve type hinting (using Optional type) (#3709) [MAINTENANCE] Standardize names for assets that are used in Getting Started Guide (#3706) [MAINTENANCE] Clean up remaining improper usage of Optional type annotation (#3710) [MAINTENANCE] Refinement of Getting Started Guide script (#3715) [MAINTENANCE] cloud-410 - Support for Column Descriptions (#3707) [MAINTENANCE] Types Clean Up in Checkpoint, Batch, and DataContext Classes (#3737) [MAINTENANCE] Remove DeprecationWarning for validator.remove_expectation (#3744)  0.13.43  [FEATURE] Enable support for Teradata SQLAlchemy dialect (#3496) (thanks @imamolp) [FEATURE] Dremio connector added (SQLalchemy) (#3624) (thanks @chufe-dremio) [FEATURE] Adds expect_column_values_to_be_string_integers_increasing (#3642) [FEATURE] Enable \"column.quantile_values\" and \"expect_column_quantile_values_to_be_between\" for SQLite; add/enable new tests (#3695) [BUGFIX] Allow glob_directive for DBFS Data Connectors (#3673) [BUGFIX] Update black version in pre-commit config (#3674) [BUGFIX] Make sure to add \"mostly_pct\" value if \"mostly\" kwarg present (#3661) [BUGFIX] Fix BatchRequest.to_json_dict() to not overwrite original fields; also type usage cleanup in CLI tests (#3683) [BUGFIX] Fix pyfakefs boto / GCS incompatibility (#3694) [BUGFIX] Update prefix attr assignment in cloud-based DataConnector constructors (#3668) [BUGFIX] Update 'list_keys' signature for all cloud-based tuple store child classes (#3669) [BUGFIX] evaluation parameters from different expectation suites dependencies (#3684) (thanks @OmriBromberg) [DOCS] Databricks deployment pattern documentation (#3682) [DOCS] Remove how_to_instantiate_a_data_context_on_databricks_spark_cluster (#3687) [DOCS] Updates to Databricks doc based on friction logging (#3696) [MAINTENANCE] Fix checkpoint anonymization and make BatchRequest.to_json_dict() more robust (#3675) [MAINTENANCE] Update kl_divergence domain_type (#3681) [MAINTENANCE] update filter_properties_dict to use set for inclusions and exclusions (instead of list) (#3698) [MAINTENANCE] Adds CITATION.cff (#3697)  0.13.42  [FEATURE] DBFS Data connectors (#3659) [BUGFIX] Fix \"null\" appearing in notebooks due to incorrect ExpectationConfigurationSchema serialization (#3638) [BUGFIX] Ensure that result_format from saved expectation suite json file takes effect (#3634) [BUGFIX] Allowing user specified run_id to appear in WarningAndFailureExpectationSuitesValidationOperator validation result (#3386) (thanks @wniroshan) [BUGFIX] Update black dependency to ensure passing Azure builds on Python 3.9 (#3664) [BUGFIX] fix Issue #3405 - gcs client init in pandas engine (#3408) (thanks @dz-1) [BUGFIX] Recursion error when passing RuntimeBatchRequest with query into Checkpoint using validations (#3654) [MAINTENANCE] Cloud 388/supported expectations query (#3635) [MAINTENANCE] Proper separation of concerns between specific File Path Data Connectors and corresponding ExecutionEngine objects (#3643) [MAINTENANCE] Enable Docusaurus tests for S3 (#3645) [MAINTENANCE] Formalize Exception Handling Between DataConnector and ExecutionEngine Implementations, and Update DataConnector Configuration Usage in Tests (#3644) [MAINTENANCE] Adds util for handling SADeprecation warning (#3651)  0.13.41  [FEATURE] Support median calculation in AWS Athena (#3596) (thanks @persiyanov) [BUGFIX] Be able to use spark execution engine with spark reuse flag (#3541) (thanks @fep2) [DOCS] punctuation how_to_contribute_a_new_expectation_to_great_expectations.md (#3484) (thanks @plain-jane-gray) [DOCS] Update next_steps.md (#3483) (thanks @plain-jane-gray) [DOCS] Update how_to_configure_a_validation_result_store_in_gcs.md (#3482) (thanks @plain-jane-gray) [DOCS] Choosing and configuring DataConnectors (#3533) [DOCS] Remove --no-spark flag from docs tests (#3625) [DOCS] DevRel - docs fixes (#3498) [DOCS] Adding a period (#3627) (thanks @plain-jane-gray) [DOCS] Remove comments that describe Snowflake parameters as optional (#3639) [MAINTENANCE] Update CODEOWNERS (#3604) [MAINTENANCE] Fix logo (#3598) [MAINTENANCE] Add Expectations to docs navbar (#3597) [MAINTENANCE] Remove unused fixtures (#3218) [MAINTENANCE] Remove unnecessary comment (#3608) [MAINTENANCE] Superconductive Warnings hackathon (#3612) [MAINTENANCE] Bring Core Skills Doc for Creating Batch Under Test (#3629) [MAINTENANCE] Refactor and Clean Up Expectations and Metrics Parts of the Codebase (better encapsulation, improved type hints) (#3633)  0.13.40  [FEATURE] Retrieve data context config through Cloud API endpoint #3586 [FEATURE] Update Batch IDs to match name change in paths included in batch_request #3587 [FEATURE] V2-to-V3 Upgrade/Migration #3592 [FEATURE] table and graph atomic renderers #3595 [FEATURE] V2-to-V3 Upgrade/Migration (Sidebar.js update) #3603 [DOCS] Fixing broken links and linking to Expectation Gallery #3591 [MAINTENANCE] Get TZLocal back to its original version control. #3585 [MAINTENANCE] Add tests for datetime evaluation parameters #3601 [MAINTENANCE] Removed warning for pandas option display.max_colwidth #3606  0.13.39  [FEATURE] Migration of Expectations to Atomic Prescriptive Renderers (#3530, #3537) [FEATURE] Cloud: Editing Expectation Suites programmatically (#3564) [BUGFIX] Fix deprecation warning for importing from collections (#3546) (thanks @shpolina) [BUGFIX] SQLAlchemy version 1.3.24 compatibility in map metric provider (#3507) (thanks @shpolina) [DOCS] Clarify how to configure optional Snowflake parameters in CLI datasource new notebook (#3543) [DOCS] Added breaks to code snippets, reordered guidance (#3514) [DOCS] typo in documentation (#3542) (thanks @DanielEdu) [DOCS] Update how_to_configure_a_new_data_context_with_the_cli.md (#3556) (thanks @plain-jane-gray) [DOCS] Improved installation instructions, included in-line installation instructions to getting started (#3509) [DOCS] Update contributing_style.md (#3521) (thanks @plain-jane-gray) [DOCS] Update contributing_test.md (#3519) (thanks @plain-jane-gray) [DOCS] Revamp style guides (#3554) [DOCS] Update contributing.md (#3523, #3524) (thanks @plain-jane-gray) [DOCS] Simplify getting started (#3555) [DOCS] How to introspect and partition an SQL database (#3465) [DOCS] Update contributing_checklist.md (#3518) (thanks @plain-jane-gray) [DOCS] Removed duplicate prereq, how_to_instantiate_a_data_context_without_a_yml_file.md (#3481) (thanks @plain-jane-gray) [DOCS] fix link to expectation glossary (#3558) (thanks @sephiartlist) [DOCS] Minor Friction (#3574) [MAINTENANCE] Make CLI Check-Config and CLI More Robust (#3562) [MAINTENANCE] tzlocal version fix (#3565)  0.13.38  [FEATURE] Atomic Renderer: Initial framework and Prescriptive renderers (#3529) [FEATURE] Atomic Renderer: Diagnostic renderers (#3534) [BUGFIX] runtime_parameters: {batch_data: Spark DF} serialization (#3502) [BUGFIX] Custom query in RuntimeBatchRequest for expectations using table.row_count metric (#3508) [BUGFIX] Transpose \\n and , in notebook (#3463) (thanks @mccalluc) [BUGFIX] Fix contributor link (#3462) (thanks @mccalluc) [DOCS] How to introspect and partition a files based data store (#3464) [DOCS] fixed duplication of text in code example (#3503) [DOCS] Make content better reflect the document organization. (#3510) [DOCS] Correcting typos and improving the language. (#3513) [DOCS] Better Sections Numbering in Documentation (#3515) [DOCS] Improved wording (#3516) [DOCS] Improved title wording for section heading (#3517) [DOCS] Improve Readability of Documentation Content (#3536) [MAINTENANCE] Content and test script update (#3532) [MAINTENANCE] Provide Deprecation Notice for the \"parse_strings_as_datetimes\" Expectation Parameter in V3 (#3539)  0.13.37  [FEATURE] Implement CompoundColumnsUnique metric for SqlAlchemyExecutionEngine (#3477) [FEATURE] add get_available_data_asset_names_and_types (#3476) [FEATURE] add s3_put_options to TupleS3StoreBackend (#3470) (Thanks @kj-9) [BUGFIX] Fix TupleS3StoreBackend remove_key bug (#3489) [DOCS] Adding Flyte Deployment pattern to docs (#3383) [DOCS] g_e docs branding updates (#3471) [MAINTENANCE] Add type-hints; add utility method for creating temporary DB tables; clean up imports; improve code readability; and add a directory to pre-commit (#3475) [MAINTENANCE] Clean up for a better code readability. (#3493) [MAINTENANCE] Enable SQL for the \"expect_compound_columns_to_be_unique\" expectation. (#3488) [MAINTENANCE] Fix some typos (#3474) (Thanks @mohamadmansourX) [MAINTENANCE] Support SQLAlchemy version 1.3.24 for compatibility with Airflow (Airflow does not currently support later versions of SQLAlchemy). (#3499) [MAINTENANCE] Update contributing_checklist.md (#3478) (Thanks @plain-jane-gray) [MAINTENANCE] Update how_to_configure_a_validation_result_store_in_gcs.md (#3480) (Thanks @plain-jane-gray) [MAINTENANCE] update implemented_expectations (#3492)  0.13.36  [FEATURE] GREAT-3439 extended SlackNotificationsAction for slack app tokens (#3440) (Thanks @psheets) [FEATURE] Implement Integration Test for \"Simple SQL Data Source\" with Partitioning, Splitting, and Sampling (#3454) [FEATURE] Implement Integration Test for File Path Data Connectors with Partitioning, Splitting, and Sampling (#3452) [BUGFIX] Fix Incorrect Implementation of the \"_sample_using_random\" Sampling Method in SQLAlchemyExecutionEngine (#3449) [BUGFIX] Handle RuntimeBatchRequest passed to Checkpoint programatically (without yml) (#3448) [DOCS] Fix typo in command to create new checkpoint (#3434) (Thanks @joeltone) [DOCS] How to validate data by running a Checkpoint (#3436) [ENHANCEMENT] cloud-199 - Update Expectation and ExpectationSuite classes for GE Cloud (#3453) [MAINTENANCE] Does not test numpy.float128 when it doesn't exist (#3460) [MAINTENANCE] Remove Unnecessary SQL OR Condition (#3469) [MAINTENANCE] Remove validation playground notebooks (#3467) [MAINTENANCE] clean up type hints, API usage, imports, and coding style (#3444) [MAINTENANCE] comments (#3457)  0.13.35  [FEATURE] Create ExpectationValidationGraph class to Maintain Relationship Between Expectation and Metrics and Use it to Associate Exceptions to Expectations (#3433) [BUGFIX] Addresses issue #2993 (#3054) by using configuration when it is available instead of discovering keys (listing keys) in existing sources. (#3377) [BUGFIX] Fix Data asset name rendering (#3431) (Thanks @shpolina) [DOCS] minor fix to syntax highlighting in how_to_contribute_a_new_expectation\u2026 (#3413) (Thanks @edjoesu) [DOCS] Fix broken links in how_to_create_a_new_expectation_suite_using_rule_based_profile\u2026 (#3410) (Thanks @edjoesu) [ENHANCEMENT] update list_expectation_suite_names and ExpectationSuiteValidationResult payload (#3419) [MAINTENANCE] Clean up Type Hints, JSON-Serialization, ID Generation and Logging in Objects in batch.py Module and its Usage (#3422) [MAINTENANCE] Fix Granularity of Exception Handling in ExecutionEngine.resolve_metrics() and Clean Up Type Hints (#3423) [MAINTENANCE] Fix broken links in how_to_create_a_new_expectation_suite_using_rule_based_profiler (#3441) [MAINTENANCE] Fix issue where BatchRequest object in configuration could cause Checkpoint to fail (#3438) [MAINTENANCE] Insure consistency between implementation of overriding Python hash() and internal ID property value (#3432) [MAINTENANCE] Performance improvement refactor for Spark unexpected values (#3368) [MAINTENANCE] Refactor MetricConfiguration out of validation_graph.py to Avoid Future Circular Dependencies in Python (#3425) [MAINTENANCE] Use ExceptionInfo to encapsulate common expectation validation result error information. (#3427)  0.13.34  [FEATURE] Configurable multi-threaded checkpoint speedup (#3362) (Thanks @jdimatteo) [BUGFIX] Insure that the \"result_format\" Expectation Argument is Processed Properly (#3364) [BUGFIX] fix error getting validation result from DataContext (#3359) (Thanks @zachzIAM) [BUGFIX] fixed typo and added CLA links (#3347) [DOCS] Azure Data Connector Documentation for Pandas and Spark. (#3378) [DOCS] Connecting to GCS using Spark (#3375) [DOCS] Docusaurus - Deploying Great Expectations in a hosted environment without file system or CLI (#3361) [DOCS] How to get a batch from configured datasource (#3382) [MAINTENANCE] Add Flyte to README (#3387) (Thanks @samhita-alla) [MAINTENANCE] Adds expect_table_columns_to_match_set (#3329) (Thanks @viniciusdsmello) [MAINTENANCE] Bugfix/skip substitute config variables in ge cloud mode (#3393) [MAINTENANCE] Clean Up ValidationGraph API Usage, Improve Exception Handling for Metrics, Clean Up Type Hints (#3399) [MAINTENANCE] Clean up ValidationGraph API and add Type Hints (#3392) [MAINTENANCE] Enhancement/update _set methods with kwargs (#3391) (Thanks @roblim) [MAINTENANCE] Fix incorrect ToC section name (#3395) [MAINTENANCE] Insure Correct Processing of the catch_exception Flag in Metrics Resolution (#3360) [MAINTENANCE] exempt batch_data from a deep_copy operation on RuntimeBatchRequest (#3388) [MAINTENANCE] [WIP] Enhancement/cloud 169/update checkpoint.run for ge cloud (#3381)  0.13.33  [FEATURE] Add optional ge_cloud_mode flag to DataContext to enable use with Great Expectations Cloud. [FEATURE] Rendered Data Doc JSONs can be uploaded and retrieved from GE Cloud [FEATURE] Implement InferredAssetAzureDataConnector with Support for Pandas and Spark Execution Engines (#3372) [FEATURE] Spark connecting to Google Cloud Storage (#3365) [FEATURE] SparkDFExecutionEngine can load data accessed by ConfiguredAssetAzureDataConnector (integration tests are included). (#3345) [FEATURE] [MER-293] GE Cloud Mode for DataContext (#3262) (Thanks @roblim) [BUGFIX] Allow for RuntimeDataConnector to accept custom query while suppressing temp table creation (#3335) (Thanks @NathanFarmer) [BUGFIX] Fix issue where multiple validators reused the same execution engine, causing a conflict in active batch (GE-3168) (#3222) (Thanks @jcampbell) [BUGFIX] Run batch_request dictionary through util function convert_to_json_serializable (#3349) (Thanks @NathanFarmer) [BUGFIX] added casting of numeric value to fix redshift issue #3293 (#3338) (Thanks @sariabod) [DOCS] Docusaurus - How to connect to an MSSQL database (#3353) (Thanks @NathanFarmer) [DOCS] GREAT-195 Docs remove all stubs and links to them (#3363) [MAINTENANCE] Update azure-pipelines-docs-integration.yml for Azure Pipelines [MAINTENANCE] Update implemented_expectations.md (#3351) (Thanks @spencerhardwick) [MAINTENANCE] Updating to reflect current Expectation dev state (#3348) (Thanks @spencerhardwick) [MAINTENANCE] docs: Clean up Docusaurus refs (#3371)  0.13.32  [FEATURE] Add Performance Benchmarks Using BigQuery. (Thanks @jdimatteo) [WIP] [FEATURE] add backend args to run_diagnostics (#3257) (Thanks @edjoesu) [BUGFIX] Addresses Issue 2937. (#3236) (Thanks @BenGale93) [BUGFIX] SQL dialect doesn't register for BigQuery for V2 (#3324) [DOCS] \"How to connect to data on GCS using Pandas\" (#3311) [MAINTENANCE] Add CODEOWNERS with a single check for sidebars.js (#3332) [MAINTENANCE] Fix incorrect DataConnector usage of _get_full_file_path() API method. (#3336) [MAINTENANCE] Make Pandas against S3 and GCS integration tests more robust by asserting on number of batches returned and row counts (#3341) [MAINTENANCE] Make integration tests of Pandas against Azure more robust. (#3339) [MAINTENANCE] Prepare AzureUrl to handle WASBS format (for Spark) (#3340) [MAINTENANCE] Renaming default_batch_identifier in examples #3334 [MAINTENANCE] Tests for RuntimeDataConnector at DataContext-level (#3304) [MAINTENANCE] Tests for RuntimeDataConnector at DataContext-level (Spark and Pandas) (#3325) [MAINTENANCE] Tests for RuntimeDataConnector at Data Source-level (Spark and Pandas) (#3318) [MAINTENANCE] Various doc patches (#3326) [MAINTENANCE] clean up imports and method signatures (#3337)  0.13.31  [FEATURE] Enable GCS DataConnector integration with PandasExecutionEngine (#3264) [FEATURE] Enable column_pair expectations and tests for Spark (#3294) [FEATURE] Implement InferredAssetGCSDataConnector (#3284) [FEATURE]/CHANGE run time format (#3272) (Thanks @serialbandicoot) [DOCS] Fix misc errors in \"How to create renderers for Custom Expectations\" (#3315) [DOCS] GDOC-217 remove stub links (#3314) [DOCS] Remove misc TODOs to tidy up docs (#3313) [DOCS] Standardize capitalization of various technologies in docs (#3312) [DOCS] Fix broken link to Contributor docs (#3295) (Thanks @discdiver) [MAINTENANCE] Additional tests for RuntimeDataConnector at Data Source-level (query) (#3288) [MAINTENANCE] Update GCSStoreBackend + tests (#2630) (Thanks @hmandsager) [MAINTENANCE] Write integration/E2E tests for ConfiguredAssetAzureDataConnector (#3204) [MAINTENANCE] Write integration/E2E tests for both GCSDataConnectors (#3301)  0.13.30  [FEATURE] Implement Spark Decorators and Helpers; Demonstrate on MulticolumnSumEqual Metric (#3289) [FEATURE] V3 implement expect_column_pair_values_to_be_in_set for SQL Alchemy execution engine (#3281) [FEATURE] Implement ConfiguredAssetGCSDataConnector (#3247) [BUGFIX] Fix import issues around cloud providers (GCS/Azure/S3) (#3292) [MAINTENANCE] Add force_reuse_spark_context to DatasourceConfigSchema (#3126) (thanks @gipaetusb and @mbakunze)  0.13.29  [FEATURE] Implementation of the Metric \"select_column_values.unique.within_record\" for SQLAlchemyExecutionEngine (#3279) [FEATURE] V3 implement ColumnPairValuesInSet for SQL Alchemy execution engine (#3278) [FEATURE] Edtf with support levels (#2594) (thanks @mielvds) [FEATURE] V3 implement expect_column_pair_values_to_be_equal for SqlAlchemyExecutionEngine (#3267) [FEATURE] add expectation for discrete column entropy  (#3049) (thanks @edjoesu) [FEATURE] Add SQLAlchemy Provider for the column_pair_values.a_greater_than_b Metric (#3268) [FEATURE] Expectations tests for BigQuery backend (#3219) (Thanks @jdimatteo) [FEATURE] Add schema validation for different GCS auth methods (#3258) [FEATURE] V3 - Implement column_pair helpers/providers for SqlAlchemyExecutionEngine (#3256) [FEATURE] V3 implement expect_column_pair_values_to_be_equal expectation for PandasExecutionEngine (#3252) [FEATURE] GCS DataConnector schema validation (#3253) [FEATURE] Implementation of the \"expect_select_column_values_to_be_unique_within_record\" Expectation (#3251) [FEATURE] Implement the SelectColumnValuesUniqueWithinRecord metric (for PandasExecutionEngine) (#3250) [FEATURE] V3 - Implement ColumnPairValuesEqual for PandasExecutionEngine (#3243) [FEATURE] Set foundation for GCS DataConnectors (#3220) [FEATURE] Implement \"expect_column_pair_values_to_be_in_set\" expectation (support for PandasExecutionEngine) (#3242) [BUGFIX] Fix deprecation warning for importing from collections (#3228) (thanks @ismaildawoodjee) [DOCS] Document BigQuery test dataset configuration (#3273) (Thanks @jdimatteo) [DOCS] Syntax and Link (#3266) [DOCS] API Links and Supporting Docs (#3265) [DOCS] redir and search (#3249) [MAINTENANCE] Update azure-pipelines-docs-integration.yml to include env vars for Azure docs integration tests [MAINTENANCE] Allow Wrong ignore_row_if Directive from V2 with Deprecation Warning (#3274) [MAINTENANCE] Refactor test structure for \"Connecting to your data\" cloud provider integration tests (#3277) [MAINTENANCE] Make test method names consistent for Metrics tests (#3254) [MAINTENANCE] Allow PandasExecutionEngine to accept Azure DataConnectors (#3214) [MAINTENANCE] Standardize Arguments to MetricConfiguration Constructor; Use {} instead of dict(). (#3246)  0.13.28  [FEATURE] Implement ColumnPairValuesInSet metric for PandasExecutionEngine [BUGFIX] Wrap optional azure imports in data_connector setup  0.13.27  [FEATURE] Accept row_condition (with condition_parser) and ignore_row_if parameters for expect_multicolumn_sum_to_equal (#3193) [FEATURE] ConfiguredAssetDataConnector for Azure Blob Storage (#3141) [FEATURE] Replace MetricFunctionTypes.IDENTITY domain type with convenience method get_domain_records() for SparkDFExecutionEngine (#3226) [FEATURE] Replace MetricFunctionTypes.IDENTITY domain type with convenience method get_domain_records() for SqlAlchemyExecutionEngine (#3215) [FEATURE] Replace MetricFunctionTypes.IDENTITY domain type with convenience method get_full_access_compute_domain() for PandasExecutionEngine (#3210) [FEATURE] Set foundation for Azure-related DataConnectors (#3188) [FEATURE] Update ExpectCompoundColumnsToBeUnique for V3 API (#3161) [BUGFIX] Fix incorrect schema validation for Azure data connectors (#3200) [BUGFIX] Fix incorrect usage of \"all()\" in the comparison of validation results when executing an Expectation (#3178) [BUGFIX] Fixes an error with expect_column_values_to_be_dateutil_parseable (#3190) [BUGFIX] Improve parsing of .ge_store_backend_id (#2952) [BUGFIX] Remove fixture parameterization for Cloud DBs (Snowflake and BigQuery) (#3182) [BUGFIX] Restore support for V2 API style custom expectation rendering (#3179) (Thanks @jdimatteo) [DOCS] Add conda as installation option in README (#3196) (Thanks @rpanai) [DOCS] Standardize capitalization of \"Python\" in \"Connecting to your data\" section of new docs (#3209) [DOCS] Standardize capitalization of Spark in docs (#3198) [DOCS] Update BigQuery docs to clarify the use of temp tables (#3184) [DOCS] Create _redirects (#3192) [ENHANCEMENT] RuntimeDataConnector messaging is made more clear for test_yaml_config() (#3206) [MAINTENANCE] Add credentials YAML key support for DataConnectors (#3173) [MAINTENANCE] Fix minor typo in S3 DataConnectors (#3194) [MAINTENANCE] Fix typos in argument names and types (#3207) [MAINTENANCE] Update changelog. (#3189) [MAINTENANCE] Update documentation. (#3203) [MAINTENANCE] Update validate_your_data.md (#3185) [MAINTENANCE] update tests across execution engines and clean up coding patterns (#3223)  0.13.26  [FEATURE] Enable BigQuery tests for Azure CI/CD (#3155) [FEATURE] Implement MulticolumnMapExpectation class (#3134) [FEATURE] Implement the MulticolumnSumEqual Metric for PandasExecutionEngine (#3130) [FEATURE] Support row_condition and ignore_row_if Directives Combined for PandasExecutionEngine (#3150) [FEATURE] Update ExpectMulticolumnSumToEqual for V3 API (#3136) [FEATURE] add python3.9 to python versions (#3143) (Thanks @dswalter) [FEATURE]/MER-16/MER-75/ADD_ROUTE_FOR_VALIDATION_RESULT (#3090) (Thanks @rreinoldsc) [BUGFIX] Enable --v3-api suite edit to proceed without selecting DataConnectors (#3165) [BUGFIX] Fix error when RuntimeBatchRequest is passed to SimpleCheckpoint with RuntimeDataConnector (#3152) [BUGFIX] allow reader_options in the CLI so can read .csv.gz files (#2695) (Thanks @luke321321) [DOCS] Apply Docusaurus tabs to relevant pages in new docs [DOCS] Capitalize python to Python in docs (#3176) [DOCS] Improve Core Concepts - Expectation Concepts (#2831) [MAINTENANCE] Error messages must be friendly. (#3171) [MAINTENANCE] Implement the \"compound_columns_unique\" metric for PandasExecutionEngine (with a unit test). (#3159) [MAINTENANCE] Improve Coding Practices in \"great_expectations/expectations/expectation.py\" (#3151) [MAINTENANCE] Update test_script_runner.py (#3177)  0.13.25  [FEATURE] Pass on meta-data from expectation json to validation result json (#2881) (Thanks @sushrut9898) [FEATURE] Add sqlalchemy engine support for column.most_common_value metric (#3020) (Thanks @shpolina) [BUGFIX] Added newline to CLI message for consistent formatting (#3127) (Thanks @ismaildawoodjee) [BUGFIX] fix pip install snowflake build error with Python 3.9 (#3119) (Thanks @jdimatteo) [BUGFIX] Populate (data) asset name in data docs for RuntimeDataConnector (#3105) (Thanks @ceshine) [DOCS] Correct path to docs_rtd/changelog.rst (#3120) (Thanks @jdimatteo) [DOCS] Fix broken links in \"How to write a 'How to Guide'\" (#3112) [DOCS] Port over \"How to add comments to Expectations and display them in DataDocs\" from RTD to Docusaurus (#3078) [DOCS] Port over \"How to create a Batch of data from an in memory Spark or Pandas DF\" from RTD to Docusaurus (#3099) [DOCS] Update CLI codeblocks in create_your_first_expectations.md (#3106) (Thanks @ories) [MAINTENANCE] correct typo in docstring (#3117) [MAINTENANCE] DOCS/GDOC-130/Add Changelog (#3121) [MAINTENANCE] fix docstring for expectation \"expect_multicolumn_sum_to_equal\" (previous version was not precise) (#3110) [MAINTENANCE] Fix typos in docstrings in map_metric_provider partials (#3111) [MAINTENANCE] Make sure that all imports use column_aggregate_metric_provider (not column_aggregate_metric). (#3128) [MAINTENANCE] Rename column_aggregate_metric.py into column_aggregate_metric_provider.py for better code readability. (#3123) [MAINTENANCE] rename ColumnMetricProvider to ColumnAggregateMetricProvider (with DeprecationWarning) (#3100) [MAINTENANCE] rename map_metric.py to map_metric_provider.py (with DeprecationWarning) for a better code readability/interpretability (#3103) [MAINTENANCE] rename table_metric.py to table_metric_provider.py with a deprecation notice (#3118) [MAINTENANCE] Update CODE_OF_CONDUCT.md (#3066) [MAINTENANCE] Upgrade to modern Python syntax (#3068) (Thanks @cclauss)  0.13.24  [FEATURE] Script to automate proper triggering of Docs Azure pipeline (#3003) [BUGFIX] Fix an undefined name that could lead to a NameError (#3063) (Thanks @cclauss) [BUGFIX] fix incorrect pandas top rows usage (#3091) [BUGFIX] Fix parens in Expectation metric validation method that always returned True assertation (#3086) (Thanks @morland96) [BUGFIX] Fix run_diagnostics for contrib expectations (#3096) [BUGFIX] Fix typos discovered by codespell (#3064) (Thanks cclauss) [BUGFIX] Wrap get_view_names in try clause for passing the NotImplemented error (#2976) (Thanks @kj-9) [DOCS] Ensuring consistent style of directories, files, and related references in docs (#3053) [DOCS] Fix broken link to example DAG (#3061) (Thanks fritz-astronomer) [DOCS] GDOC-198 cleanup TOC (#3088) [DOCS] Migrating pages under guides/miscellaneous (#3094) (Thanks @spbail) [DOCS] Port over \u201cHow to configure a new Checkpoint using test_yaml_config\u201d from RTD to Docusaurus [DOCS] Port over \u201cHow to configure an Expectation store in GCS\u201d from RTD to Docusaurus (#3071) [DOCS] Port over \u201cHow to create renderers for custom Expectations\u201d from RTD to Docusaurus [DOCS] Port over \u201cHow to run a Checkpoint in Airflow\u201d from RTD to Docusaurus (#3074) [DOCS] Update how-to-create-and-edit-expectations-in-bulk.md (#3073) [MAINTENANCE] Adding a comment explaining the IDENTITY metric domain type. (#3057) [MAINTENANCE] Change domain key value from \u201ccolumn\u201d to \u201ccolumn_list\u201d in ExecutionEngine implementations (#3059) [MAINTENANCE] clean up metric errors (#3085) [MAINTENANCE] Correct the typo in the naming of the IDENTIFICATION semantic domain type name. (#3058) [MAINTENANCE] disable snowflake tests temporarily (#3093) [MAINTENANCE] [DOCS] Port over \u201cHow to host and share Data Docs on GCS\u201d from RTD to Docusaurus (#3070) [MAINTENANCE] Enable repr for MetricConfiguration to assist with troubleshooting. (#3075) [MAINTENANCE] Expand test of a column map metric to underscore functionality. (#3072) [MAINTENANCE] Expectation anonymizer supports v3 expectation registry (#3092) [MAINTENANCE] Fix -- check for column key existence in accessor_domain_kwargsn for condition map partials. (#3082) [MAINTENANCE] Missing import of SparkDFExecutionEngine was added. (#3062)  Older Changelist Older changelist can be found at https://github.com/great-expectations/great_expectations/blob/develop/docs_rtd/changelog.rst", "\"Glossary\"": " id: glossary title: \"Glossary\"  Action: A Python class with a run method that takes a Validation Result and does something with it Batch: A selection of records from a Data Asset. Batch Request: Provided to a Data Source in order to create a Batch. CLI: Command Line Interface Checkpoint: The primary means for validating data in a production deployment of Great Expectations. Checkpoint Store: A connector to store and retrieve information about means for validating data in a production deployment of Great Expectations. Custom Expectation: An extension of the Expectation class, developed outside of the Great Expectations library. Data Asset: A collection of records within a Data Source which is usually named based on the underlying data system and sliced to correspond to a desired specification. Data Assistant: A utility that asks questions about your data, gathering information to describe what is observed, and then presents Metrics and proposes Expectations based on the answers. Data Context: The primary entry point for a Great Expectations deployment, with configurations and methods for all supporting components. Data Docs: Human readable documentation generated from Great Expectations metadata detailing Expectations, Validation Results, etc. Data Docs Store: A connector to store and retrieve information pertaining to Human readable documentation generated from Great Expectations metadata detailing Expectations, Validation Results, etc. Data Source: Provides a standard API for accessing and interacting with data from a wide variety of source systems. Evaluation Parameter: A dynamic value used during Validation of an Expectation which is populated by evaluating simple expressions or by referencing previously generated metrics. Evaluation Parameter Store: A connector to store and retrieve information about parameters used during Validation of an Expectation which reference simple expressions or previously generated metrics. Execution Engine: A system capable of processing data to compute Metrics. Expectation: A verifiable assertion about data. Expectation Store: A connector to store and retrieve information about collections of verifiable assertions about data. Expectation Suite: A collection of verifiable assertions about data. Metric: A computed attribute of data such as the mean of a column. MetricProviders: Generate and register Metrics to support Expectations, and they are an important part of the Expectation software development kit (SDK). Metric Store: A connector to store and retrieve information about computed attributes of data, such as the mean of a column. Plugin: Extends Great Expectations' components and/or functionality. Profiler: Generates Metrics and candidate Expectations from data. Profiling: The act of generating Metrics and candidate Expectations from data. Renderer: A method for converting Expectations, Validation Results, etc. into Data Docs or other output such as email notifications or slack messages. Store: A connector to store and retrieve information about metadata in Great Expectations. Supporting Resource: A resource external to the Great Expectations code base which Great Expectations utilizes. Validation: The act of applying an Expectation Suite to a Batch. Validation Result: Generated when data is Validated against an Expectation or Expectation Suite. Validation Result Store: A connector to store and retrieve information about objects generated when data is Validated against an Expectation Suite. Validator: Used to run an Expectation Suite against data.", "Welcome": " title: Welcome slug: /  Great Expectations is the leading tool for validating, documenting, and profiling your data to maintain quality and improve communication between teams. If you're ready to get started, see the Quickstart. Software developers have long known that automated testing is essential for managing complex codebases. Great Expectations brings the same discipline, confidence, and acceleration to data science and data engineering teams.  Why use Great Expectations? With Great Expectations, you can assert what you expect from the data you load and transform, and catch data issues quickly \u2013 Expectations are basically unit tests for your data. Not only that, but Great Expectations also creates data documentation and data quality reports from those Expectations. Data science and data engineering teams use Great Expectations to:  Test data they ingest from other teams or vendors and ensure its validity. Validate data they transform as a step in their data pipeline in order to ensure the correctness of transformations. Prevent data quality issues from slipping into data products. Streamline knowledge capture from subject-matter experts and make implicit knowledge explicit. Develop rich, shared documentation of their data.  To learn more about how data teams are using Great Expectations, see Case studies from Great Expectations. Key Features Expectations Expectations are assertions about your data. In Great Expectations, those assertions are expressed in a declarative language in the form of simple, human-readable Python methods. For example, in order to assert that you want the column \u201cpassenger_count\u201d to be between 1 and 6, you can say: python expect_column_values_to_be_between(     column=\"passenger_count\",     min_value=1,     max_value=6 ) Great Expectations then uses this statement to validate whether the column passenger_count in a given table is indeed between 1 and 6, and returns a success or failure result. The library currently provides several dozen highly expressive built-in Expectations, and allows you to write custom Expectations. Automated data profiling Writing pipeline tests from scratch can be tedious and overwhelming. Great Expectations jump starts the process by providing automated data profiling. The library profiles your data to get basic statistics, and automatically generates a suite of Expectations based on what is observed in the data. For example, using the profiler on a column passenger_count that only contains integer values between 1 and 6, Great Expectations automatically generates this Expectation we\u2019ve already seen: python expect_column_values_to_be_between(     column=\"passenger_count\",     min_value=1,     max_value=6 ) This allows you to quickly create tests for your data, without having to write them from scratch. Data validation Once you\u2019ve created your Expectations, Great Expectations can load any batch or several batches of data to validate with your suite of Expectations. Great Expectations tells you whether each Expectation in an Expectation Suite passes or fails, and returns any unexpected values that failed a test, which can significantly speed up debugging data issues! Data Docs Great Expectations renders Expectations in a clean, human-readable format called Data Docs. These HTML docs contain both your Expectation Suites and your data Validation Results each time validation is run \u2013 think of it as a continuously updated data quality report. The following image shows a sample Data Doc:  Support for various Datasources and Store backends Great Expectations currently supports native execution of Expectations against various Datasources, such as Pandas dataframes, Spark dataframes, and SQL databases via SQLAlchemy. This means you\u2019re not tied to having your data in a database in order to validate it: You can also run Great Expectations against CSV files or any piece of data you can load into a dataframe. Great Expectations is highly configurable. It allows you to store all relevant metadata, such as the Expectations and Validation Results in file systems, database backends, as well as cloud storage such as S3 and Google Cloud Storage, by configuring metadata Stores. What does Great Expectations NOT do? Great Expectations is NOT a pipeline execution framework. Great Expectations integrates seamlessly with DAG execution tools such as Airflow, dbt, Prefect, Dagster, and Kedro. Great Expectations does not execute your pipelines for you, but instead, validation can simply be run as a step in your pipeline. Great Expectations is NOT a data versioning tool. Great Expectations does not store data itself. Instead, it deals in metadata about data: Expectations, Validation Results, etc. If you want to bring your data itself under version control, check out tools like: DVC, Quilt, and lakeFS. Great Expectations currently works best in a Python environment. Great Expectations is Python-based. You can invoke it from the command line without using a Python programming environment, but if you\u2019re working in another ecosystem, other tools might be a better choice. If you\u2019re running in a pure R environment, you might consider assertR  as an alternative. Within the TensorFlow ecosystem, TFDV fulfills a similar function as Great Expectations. Community Resources Great Expectations is committed to supporting and the growing Great Expectations community. It\u2019s not enough to build a great tool. Great Expectations wants to build a great community as well. Open source doesn\u2019t always have the best reputation for being friendly and welcoming, and that makes us sad. Everyone belongs in open source, and Great Expectations is dedicated to making you feel welcome. Contact Great Expectations Join the Great Expectations public Slack channel. Before you post for the first time, review the Slack Guidelines. Ask a question Slack is good for that, too: join Slack and read How to write a good question in Slack. You can also use GitHub Discussions. File a bug report or feature request If you have bugfix or feature request, see upvote an existing issue or open a new issue. Contribute code or documentation To make a contribution to Great Expectations, see Contribute. Not interested in managing your own configuration or infrastructure? Learn more about Great Expectations Cloud, a fully managed SaaS offering. Sign up for the weekly cloud workshop! You\u2019ll get to preview the latest features and participate in the private Alpha program!", "How to set up GX to work with Athena": " title: How to set up GX to work with Athena tag: [how-to, setup] keywords: [Great Expectations, SQL, Athena]  How to set up Great Expectations to work with Athena import TechnicalTag from '/docs/term_tags/_tag.mdx'; import Prerequisites from '/docs/components/_prerequisites.jsx'  import IntroInstallPythonGxAndDependencies from '/docs/components/setup/installation/_intro_python_environment_with_dependencies.mdx'   import PythonCheckVersion from '/docs/components/setup/python_environment/_python_check_version.mdx'  import PythonCreateVenv from '/docs/components/setup/python_environment/_python_create_venv.md' import TipPythonOrPython3Executable from '/docs/components/setup/python_environment/_tip_python_or_python3_executable.md'  import InstallDependencies from '/docs/components/setup/dependencies/_sql_install_dependencies.mdx'  import GxVerifyInstallation from '/docs/components/setup/_gx_verify_installation.md'  import InitializeDataContextFromCli from '/docs/components/setup/data_context/_filesystem_data_context_initialize_with_cli.md' import VerifyDataContextInitializedFromCli from '/docs/components/setup/data_context/_filesystem_data_context_verify_initialization_from_cli.md'    Prerequisites   The ability to install Python modules with pip   Steps 1. Check your Python version   2. Create a Python virtual environment  3. Install GX with optional dependencies for Athena  4. Verify that GX has been installed correctly   :::info Verifying the Data Context initialized successfully  :::  Next steps ", "'Tutorial, Step 3: Create Expectations'": " title: 'Tutorial, Step 3: Create Expectations' import UniversalMap from '/docs/images/universal_map/_universal_map.mdx'; import TechnicalTag from '@site/docs/term_tags/_tag.mdx';  Prerequisites  Completion of Step 2: Connect to Data  Expectations are the workhorse abstraction in Great Expectations. Each Expectation is a declarative, machine-verifiable assertion about the expected format, content, or behavior of your data. Great Expectations comes with dozens of built-in Expectations, and it\u2019s possible to develop your own custom Expectations, too. The CLI will help you create your first Expectation Suite. Suites are simply collections of Expectations. In order to create a new suite, we will use the built-in Profiler to automatically create an Expectation Suite called getting_started_expectation_suite_taxi.demo. Create an Expectation Suite using the CLI Since we are using the CLI, you will want to return to your console and the ge_tutorials folder.  Remember: This is where we initialized our Data Context, and the Data Context is our access point to everything else in Great Expectations! From the ge_tutorials folder, type the following into your terminal: console great_expectations suite new This will bring up the following prompt: console How would you like to create your Expectation Suite?     1. Manually, without interacting with a sample Batch of data (default)     2. Interactively, with a sample Batch of data     3. Automatically, using a Data Assistant : 3 In this tutorial we will be using a Profiler to populate the Expectation Suite, so go ahead and enter 3 and hit enter to continue to the next prompt. ```console A batch of data is required to edit the suite - let's help you to specify it. Which data asset (accessible by data connector \"default_inferred_data_connector_name\") would you like to use?     1. yellow_tripdata_sample_2019-01.csv     2. yellow_tripdata_sample_2019-02.csv : 1 `` The Profiler will require a <TechnicalTag relative=\"../../\" tag=\"batch\" text=\"Batch\" /> to analyze.  This prompt is asking us which data to use for that.  As you can see, the prompt it is giving corresponds to the .CSV files in ourdata` folder.  These are the very same ones we configured our Data Source to connect to back in Step 2: Connect to Data. We're going to choose the first file.  If you're wondering why, here's an explanation: Recall that our data directory contains two CSV files: yellow_tripdata_sample_2019-01 and yellow_tripdata_sample_2019-02.  yellow_tripdata_sample_2019-01 contains the January 2019 taxi data. Since we want to build an Expectation Suite based on what we know about our taxi data from the January 2019 data set, we want to use it for profiling. yellow_tripdata_sample_2019-02 contains the February 2019 data, which we consider the \u201cnew\u201d data set that we want to validate before using in production. We\u2019ll use it later when showing you how to validate data.  Makes sense, right? Go ahead and answer 1 and hit enter now.  That will bring up the next prompt. console Name the new Expectation Suite [yellow_tripdata_sample_2019-01.csv.warning]: getting_started_expectation_suite_taxi.demo This prompt is asking for a name for our new Expectation Suite.  You can name it whatever you would like, but since this is the Getting Started Tutorial, we're demonstrating how to create an expectation suite, and we're using NYC taxi data we've used getting_started_expectation_suite_taxi.demo as the provided name. Once you've provided a name for your Expectation Suite and hit enter, you will receive one more prompt.   This one will ask if you want to proceed with creating the Expectation Suite as you've specified so far: ```console Great Expectations will create a notebook, containing code cells that select from available columns in your dataset and generate expectations about them to demonstrate some examples of assertions you can make about your data. When you run this notebook, Great Expectations will store these expectations in a new Expectation Suite \"Name the new Expectation Suite [yellow_tripdata_sample_2019-01.csv.warning]: getting_started_expectation_suite_taxi.demo\" here: /ge_tutorials/great_expectations/expectations/Name the new Expectation Suite [yellow_tripdata_sample_2019-01/csv/warning]: getting_started_expectation_suite_taxi/demo.json Would you like to proceed? [Y/n]: Y ``` When you answer with Y (or just press enter) Great Expectations will open a Jupyter Notebook that helps you populate the new suite. Creating Expectations in Jupyter Notebooks Notebooks are a simple way of interacting with the Great Expectations Python API. You could also just write all this in plain Python code, but for convenience, Great Expectations provides you some boilerplate code in notebooks. Since notebooks are often less permanent, creating Expectations in a notebook also helps reinforce that the source of truth about Expectations is the Expectation Suite, not the code that generates the Expectations. Let\u2019s take a look through the notebook and see what\u2019s happening in each cell Cell 1   The first cell does several things: It imports all the relevant libraries, loads a Data Context, and creates a Validator, which combines a Batch Request to define your batch of data, and an Expectation Suite.  Cell 2   The second cell allows you to specify which columns you want to ignore when creating Expectations. For our tutorial, we're going to ensure that the number of passengers recorded in our data is reasonable.  To do this, we'll want our Profiler to examine the passenger_count column and determine just what a reasonable range is based on our January data. Let\u2019s comment just this one line to include it:  python name=\"tests/integration/docusaurus/tutorials/getting-started/getting_started.py exclude_column_names no comment\" Cell 3   Cell 3 is where you run a Data Assistant.  In this case, the assistant being used is the Onboarding Assistant, which will Profile the data provided by your Batch Request and create the relevant Expectations to add to your taxi.demo suite.  Cell 4   The last cell does several things again: It saves the Expectation Suite to disk, runs the Validation against the loaded data batch, and then builds and opens Data Docs, so you can look at the Validation Results. We will explain the validation step later in the next step, Step 4: Validate Data.  For purposes of this tutorial, the default values in all of these cells (except for the second one, which we changed to include the passenger_count field) provide the configurations and execute the steps that we need them to.  So as long as you've made that one change, you're ready to continue. Let\u2019s execute all the cells and wait for Great Expectations to open a browser window with Data Docs, which will then let us see and edit the Expectations that were composed for us by the Profiler. Viewing your Expectations in Data Docs Once the Profiler is done executing it will open up Data Docs in your browser automatically. Data Docs translate Expectations, Validation Results, and other metadata into clean, human-readable documentation. Automatically compiling your data documentation from your data tests in the form of Data Docs guarantees that your documentation will never go stale.  Feel free to take a moment to browse through the Expectations that the Profiler put together from the data that we provided it. In particular, take a look at the Expectations that were created for the passenger_count field.  These are the rules that we will be comparing the February data against when we validate it in step four of this tutorial. How did we get those Expectations? You can create and edit Expectations using several different workflows. Using an automated Profiler as we just did is one of the quickest options to get started with an Expectation Suite. This Profiler connected to your data (using the Data Source you configured in the previous step), took a quick look at the contents of the data, and produced an initial set of Expectations. The Profiler considers the following properties, amongst others:   the data type of the column   simple statistics like column min, max, mean   the number of times values occur   the number of NULL values   These Expectations are not intended to be very smart. Instead, the goal is to quickly provide some good examples, so that you\u2019re not starting from a blank slate.  Creating Custom Expectations         Later, you should also take a look at other workflows for creating Custom Expectations. Creating Custom Expectations is an active area of work in the Great Expectations community. Stay tuned for improvements over time.        For the purposes of this tutorial, the Expectations created by the Profiler are all we need.  On to Step 4: Validate your data!", "Expectation classes": " title: Expectation classes In our daily lives and our data, we expect different things from different types of objects. After all, it would be both alarming and disturbing if a chair suddenly behaved like a cat! Similarly, it is entirely reasonable to expect different results when we evaluate the contents of an entire table and when we evaluate the contents of a single column. Expectation classes have been created to help make sure Custom Expectations return the expected results. This document provides an overview of the available Expectation classes, why they are helpful, and when they should be used. Class hierarchy The following is the Expectation class hierarchy: text Expectation     BatchExpectation         ColumnAggregateExpectation         ColumnMapExpectation             RegexBasedColumnMapExpectation             SetBasedColumnMapExpectation         ColumnPairMapExpectation         MulticolumnMapExpectation         QueryExpectation Most Expectations are a combination of a Domain (Batch, Column, ColumnPair, Multicolumns) and an approach (Map or Aggregate). In some cases, the Expectation classes include a prefix such as RegexBasedColumnMapExpectation. There are also two classes that don\u2019t follow the standard naming convention; BatchExpectations and QueryExpectations. Expectation Domain types Domains provide a way to address a specific set of data, such as a Batch within a table, or a column within a Batch. Domains do this by describing the data locale. The data locale is the conceptual equivalent of \u201cdata that arrived last Tuesday in the UserEvents table in the Redshift database,\u201d or \u201cthe timestamp column in the User's table in the Redshift database\u201d. The following are the four Expectation Domains:  Batch Column ColumnPair Multicolumn  ColumnPair A ColumnPair is the special case of a MultiColumn where the number of columns equals two. A Column Expectation is the special case where the number equals one. From a software engineering perspective, there are meaningful differences between Expectations with different domains. Specifically, all Column Expectations accept a column argument, all ColumnPair Expectations accept a pair of arguments, usually named column_A and column_B, and all MultiColumn Expectations accept a column_list argument. As the arguments for each Expectation are different, they are implemented as different classes. However, this can affect the logic for query optimization. For this reason, GX recommends using the smallest applicable domain when you\u2019re implementing a custom Expectation. For example, don't subclass a MultiColumn Expectation when a ColumnPair Expectation will do. Column Expectations operate on individual columns. ColumnPair and Multicolumn Expectations operate on column pairs, in the same Batch, but not necessarily adjacent to each other.  GX doesn\u2019t have a TableExpectation type because you can get the same functionality from a BatchExpectation. If you want to run Expectations on an entire table, you configure a DataAsset to use an entire table as its domain.  Aggregate Expectations Aggregate Expectations are based on a single Metric for the whole Batch. This Metric is called the observed_value for the Expectation.  A common pattern is to calculate a numeric Metric, and then verify that it falls between a min_value and max_value, as in expect_column_mean_to_be_between. However, some Expectations only have a max or a min, such as expect_column_kl_divergence_to_be_less_than.  Some Expectations don\u2019t use a numeric Metric for the observed_value. For example, expect_column_distinct_values_to_equal_set creates a set of distinct column values, that is then compared against a specified set and expect_column_to_have_no_days_missing looks for continuity within the column\u2019s values.  Aggregate Expectations calculate summary statistics across Batches of data. As a result, they can be a computationally efficient way to gain insight into the overall behavior of a dataset and can provide a useful foundation for identifying trends, patterns, and outliers. However, because Aggregate Expectations do not verify individual rows of data, they can't identify specific data issues. Map Expectations Map Expectations are evaluated on a row-by-row basis and each row is checked independently. For example, expect_column_values_to_not_be_null, expect_column_values_to_be_in_set, expect_column_pair_values_to_be_equal.  Map Expectations are useful when you want to be certain that the content of a given dataset is correct. If you\u2019re validating data within a pipeline, Map Expectations can help you identify invalid rows, remove invalid rows from the dataset, and process the remaining data. Unfortunately, because Map Expectations evaluate every row of data, they can be computationally intensive. Every Map Expectation includes a mostly parameter. The mostly parameter allows you to specify a minimum percentage of rows that must validate successfully to pass the Expectation. The Expectation can still succeed when individual rows fail validation. This can be useful if you want your pipelines to have invalid data tolerance. Terminology Row Expectations was considered as an alternative name for Map Expectations, but it would have led to formulations such as ColumnRowExpectations and this might have confused users. Instead, Map Expectations was selected as a reference to map() functions and map-reduce algorithms. The naming convention requires an explanation, but conversations with users indicate that the meaning of the selected naming convention is clear.   Subclasses Beyond the [Domain][Approach]Expectation naming convention, the specialized subclasses RegexBasedColumnMapExpectation and SetBasedColumnMapExpectation are supported. These extend ColumnMapExpectation and make it easier to define Expectations based on regexes and sets. For more information, see How to create a Custom Regex-Based Column Map Expectation and How to create a Custom Set-Based Column Map Expectation. BatchExpectations BatchExpectations do not currently have a special subclass for Map Expectations. Essentially, BatchMapExpectations would apply row-by-row validation to all the columns in a Batch. When there is demand for this Expectation type, the class hierarchy will be refactored to accommodate it. QueryExpectations QueryExpectations allow you to set Expectations against the results of custom SQL or Spark queries. QueryExpectations can be useful if you\u2019re comfortable working in SQL or Spark or a specific dialect. They can also allow you to embed arbitrarily complex logic in your Expectations, such as combining data from multiple tables, or applying complex logic within a query.  QueryExpectations bypass most of the logic that GX uses for grouping queries on related Domains. As a result, QueryExpectations can increase database traffic and consume computational resources. If you\u2019re not careful when you construct your Expectation, you can also misattribute results to the wrong Domain. For most use cases, QueryDataAssets are the better option. This option allows you to separate the logic of assembling data for validation from the logic of evaluating it. The other limitation of QueryExpectations is that they are tightly bound to a specific SQL or Spark dialect. This can be a good way to get started, but if you use pandas, or more than one dialect of SQL or Spark, GX recommends that you port your QueryExpectation into a more general-purpose Expectation. Conclusion This concludes the review of GX Expectation classes. You've learned:   What Expectation Domains are and how to select one   When to use Map and Aggregate data validation approaches   What specialized subclasses are available for making set-based and regular-expression-based Expectations   What the QueryExpectation class is and when you should use it   Related documentation Now that you've learned about Expectation classes, you can use the following resources to learn how to put them into practice:   Explore Expectations   How to create and edit Expectations based on domain knowledge, without inspecting data directly   Create and manage Custom Expectations   Expectations naming conventions   Standard arguments for Expectations   Result format  ", "Great Expectations overview": " title: Great Expectations overview import CheckpointFlowchart from './images_overview/checkpoint_flowchart.png' The information provided here is intended for new users of Great Expectations (GX) and those looking for an understanding of its components and its primary workflows. This overview of GX doesn\u2019t require an in-depth understanding of the code that governs GX processes and interactions. This is an ideal place to start before moving to more advanced topics, or if you want a better understanding of GX functionality. What is GX GX is a Python library that provides a framework for describing the acceptable state of data and then validating that the data meets those criteria. GX core components When working with GX you use the following four core components to access, store, and manage underlying objects and processes:  Data Context: Manages the settings and metadata for a GX project, and provides an entry point to the GX Python API. Datasources: Connects to your source data, and organizes retrieved data for future use. Expectations: Identifies the standards to which your data should conform. Checkpoints: Validates a set of Expectations against a specific set of data.  Data Context  A Data Context manages the settings and metadata for a GX project.  In Python, the Data Context object serves as the entry point for the GX API and manages various classes to limit the objects you need to directly manage yourself.  A Data Context contains all the metadata used by GX, the configurations for GX objects, and the output from validating data. The following are the available Data Context types: - Ephemeral Data Context: Exists in memory, and does not persist beyond the current Python session. - File Data Context: Exists as a folder and configuration files. Its contents persist between Python sessions. - Cloud Data Context: Supports persistence between Python sessions, but additionally serves as the entry point for Great Expectations Cloud. For more information, see Configure Data Contexts. The GX API A Data Context object in Python provides methods for configuring and interacting with GX.  These methods and the objects and additional methods accessed through them compose the GX public API. For more information, see The GX API reference. Stores Stores contain the metadata GX uses.  This includes configurations for GX objects, information that is recorded when GX validates data, and credentials used for accessing data sources or remote environments.  GX utilizes one Store for each type of metadata, and the Data Context contains the settings that tell GX where that Store should reside and how to access it. For more information, see Configure your GX environment. Data Docs Data Docs are human-readable documentation generated by GX.  Data Docs describe the standards that you expect your data to conform to, and the results of validating your data against those standards.  The Data Context manages the storage and retrieval of this information. You can configure where your Data Docs are hosted.  Unlike Stores, you can define configurations for multiple Data Docs sites.  You can also specify what information each Data Doc site provides, allowing you to format and provide different Data Docs for different use cases. For more information, see Host and share Data Docs. Datasources  Datasources connect GX to source data such as CSV files in a folder, a PostgreSQL database hosted on AWS, or any combination of data formats and environments. Regardless of the format of your source data or where it resides, Datasources provide GX with a unified API for working with it. For more information, see Connect to data. Data Assets and Batches Data Assets are collections of records within a Data Source.  While a Data Source tells GX how to connect to your source data, Data Assets tell GX how to organize that data. Although the records in your Data Assets can correspond directly to the contents of tables or files in your source data they do not necessarily need to. For instance, you could combine multiple tables worth of records in a SQL Data Source into a single Query Data Asset that joins the tables in question.  For a File Data Source, you could use regular expressions to define a Data Asset as the contents of all of the .csv files in a specific subfolder. Data Assets can be further partitioned into Batches.  Batches are unique subsets of records within a Data Asset.  For example, say you have a Data Asset in a SQL Data Source that consists of all records from last year in a given table.  You could then partition those records into Batches of data that correspond to the records for individual months of that year. For more information, see Manage Data Assets. Batch Requests A Batch Request specifies one or more Batches within the Data Asset.  Batch Requests are the primary way of retrieving data for use in GX.  Because Batch Requests can retrieve multiple Batches they provide significant flexibility in how you utilize the data in a single Data Asset. As an example, GX can automate the process of running statistical analyses for multiple Batches of data.  This is possible because you can provide a Batch Request that corresponds to multiple Batches in a Data Asset.  Alternatively, you can specify a single Batch from that same Data Asset so that you do not need to re-run the analyses on all of your data when you are only interested in a single subset. Taking the example of a Data Asset that has been partitioned into Batches by months, this would let you build a statistical model off of each month of your existing data by providing all of the Batches in a single Batch Request.  Then, after an update, you could specify that you only wanted to run your analysis on the most recent month's data by providing a Batch Request that only indicates that one Batch. For more information, see Request data from a Data Asset. Expectations An Expectation is a verifiable assertion about source data.  Similar to assertions in traditional Python unit tests, Expectations provide a flexible, declarative language for describing expected behaviors. Unlike traditional unit tests which describe the expected behavior of code given a specific input, Expectations apply to the input data itself. For example, you can define an Expectation that a column contains no null values. When GX runs that Expectation on your data it generates a report which indicates if a null value was found. Expectations can be built directly from the domain knowledge of subject matter experts, interactively while introspecting a set of data, or through automated tools provided by GX. For a list of available Expectations, see the Expectation Gallery. For more information, see Create Expectations. Expectation Suites Expectation Suites are collections of Expectations describing your data.  When GX validates data, an Expectation Suite helps streamline the process by running all of the contained Expectations against that data.  In almost all cases, when you create an Expectation you will be creating it inside of an Expectation Suite object. You can define multiple Expectation Suites for the same data to cover different use cases.  An example could be having one Expectation Suite for raw data, and a more strict Expectation Suite for that same data post-processing.  Because an Expectation Suite is decoupled from a specific source of data, you can apply the same Expectation Suite against multiple, disparate Datasources.  For instance, you can reuse an Expectation Suite that was created around an older set of data to validate the quality of a new set of data. For more information, see Create and manage Expectations and Expectation Suite. Data Assistants A Data Assistant is a utility that automates the process of building Expectations by asking questions about your data, gathering information to describe what is observed, and then presenting Metrics and proposed Expectations based on the answers it has determined.  This can accelerate the process of creating Expectations for the provided data. For more information, see Profilers and Data Assistants. Custom Expectations A built-in library of more than 50 common expectations is included with GX.  However, you can also create custom Expectations if you need a more specialized one for your data validation.  You can also use custom Expectations that have been contributed by other GX community members by installing them as Plugins. For more information, see: - Create Custom Expectations - Use a Custom Expectation Checkpoints  A Checkpoint is the primary means for validating data in a production deployment of GX. Checkpoints provide an abstraction for bundling a Batch (or Batches) of data with an Expectation Suite (or several), and then running those Expectations against the paired data. For more information, see Validate Data. Validation Results The Validation Results returned by GX tell you how your data corresponds to what you expected of it. You can view this information in the Data Docs that are configured in your Data Context. Evaluating your Validation Results helps you identify issues with your data. If the Validation Results show that your data meets your Expectations, you can confidently use it. For more information, see Validation Result. Actions One of the most powerful features of Checkpoints is that you can configure them to run Actions. The Validation Results generated when a Checkpoint runs determine what Actions are performed. Typical use cases include sending email, Slack messages, or custom notifications. Another common use case is updating Data Docs sites. Actions can be used to do anything you are capable of programming in Python. Actions are a versatile tool for integrating Checkpoints in your pipeline's workflow. For more information, see Configure Actions.", "'Concepts'": " sidebar_label: 'Concepts' title: 'Concepts' id: learn_lp description: Longer format conceptual information about Great Expectations features and functionality.  import LinkCardGrid from '/docs/components/LinkCardGrid'; import LinkCard from '/docs/components/LinkCard'; Use longer format conceptual information to learn more about Great Expectations (GX) features and functionality.    ", "'MetricProviders'": " sidebar_label: 'MetricProviders' title: 'MetricProviders' id: metricproviders description: Learn how MetricProviders are an integral component of the Expectation software development kit (SDK).  MetricProviders generate and register Metrics to support Expectations, and they are an important part of the Expectation software development kit (SDK). Typically, MetricProviders are used to resolve complex requirements and are not implemented widely. If you're considering using MetricProviders, you're encouraged to join the Great Expectations Slack community and discuss your requirements in the #gx-community-support channel. You'll encounter references to MetricProviders in many of the topics in the Great Expectations (GX) documentation. An in-depth knowledge of MetricProvider functionality is helpful, but not required to successfully complete the tasks documented in these topics. However, some knowledge of MetricProvider functionality is useful if you're developing Custom Expectations, or you're maintaining or extending MetricProvider classes. MetricProviders let you declare all the Metrics that are needed to support an Expectation in concise, Don't Repeat Yourself (DRY) syntax. MetricProviders support the following use cases:  Conceptually grouping code for Metrics calculation across multiple backends. Incremental development. You can develop Metrics for each Execution Engine, one at a time. Sharing Metrics across Expectations. Generating Metrics that can be inferred from specific types of Expectation automatically.  Assumed knowledge To get the most out of the information provided here, you should have an understanding of:  Expectations Metrics The Metric registry ExecutionEngines  When MetricProviders are required A minimum of one supporting Metric is required by every Expectation. For example, expect_column_mean_to_be_between relies on a Metric that calculates the mean of a column. Often, an Expectation requires multiple Metrics. For example, the following Metrics are required in the expect_column_values_to_be_in_set Expectation:  column_values.in_set.unexpected_count column_values.in_set.unexpected_rows column_values.in_set.unexpected_values column_values.in_set.unexpected_value_counts  To allow Expectations to work with multiple backends, methods for calculating Metrics need to be implemented for each ExecutionEngine. For example, pandas is implemented by calling the built-in pandas .mean() method on the column, Spark is implemented with a built-in Spark mean function, and SQLAlchemy is implemented with a SQLAlchemy generic function. Metrics can help you incorporate conditional statements in Expectations that support conditional evaluations. For example, column_values.in_set.condition. Metrics such as column_values.in_set.unexpected_index_list and column_values.in_set.unexpected_index_query can help you calculate the truthiness of your data. Class hierarchy Although the class hierarchy for MetricProviders and Expectations is different, they use the same naming conventions. The following is the MetricProviders class hierarchy: text MetricProvider     QueryMetricProvider     TableMetricProvider         ColumnAggregateExpectation     MapMetricProvider         ColumnMapMetricProvider             RegexColumnMapMetricProvider             SetColumnMapMetricProvide         ColumnPairMapMetricProvider         MulticolumnMapExpectation         MulticolumnMapMetricProvider If you\u2019re not sure which MetricProvider you should use with an Expectation, you can usually infer the correct MetricProvider class name from the Expectation class name. For example: | Expectation class                 | MetricProvider class                    | | --------------------------------- | --------------------------------------- | | Expectation                       | MetricProvider                          | | BatchExpectation                  | TableMetricProvider                     | | ColumnAggregateExpectation        | ColumnAggregateMetricProvider           | | ColumnMapExpectation              | ColumnMapMetricProvider                 | | RegexBasedColumnMapExpectation    | RegexColumnMapMetricProvider (built in) | | SetBasedColumnMapExpectation      | SetColumnMapMetricProvider (built in)   | | ColumnPairMapExpectation          | ColumnPairMapMetricProvider             | | MulticolumnMapExpectation         | MulticolumnMapMetricProvider            | | QueryExpectation                  | QueryMetricProvider                     | Sometimes, the MetricProvider class is created directly from the Expectation class, so you don\u2019t need to specify a MetricProvider or methods when declaring a new Expectation. For example, the RegexBasedColumnMapExpectation automatically implements the RegexColumnMapMetricProvider, and the SetBasedColumnMapExpectation automatically implements the SetColumnMapMetricProvider. Define Metrics with a MetricProvider The API for MetricProvider classes is unusual. MetricProvider classes are never intended to be instantiated, and they don\u2019t have inputs or outputs in the normal sense of method arguments and return values. Instead, the inputs for MetricProvider classes are methods for calculating the Metric on different backend applications. Each method must be decorated with an appropriate decorator. On new, the MetricProvider class registers the decorated methods as part of the Metrics registry so that they can be invoked to calculate Metrics. The registered methods are the only output from MetricProviders. :::note Decorators invoked on new can make maintainability challenging. GX intends to address this shortcoming in future releases. ::: A typical MetricProvider class has three methods that correspond to the three primary ExecutionEngines used by GX (pandas, SQL, and SparkDF). Each of the three methods are marked with an appropriate decorator. Each MetricProvider class supports different decorators. Every MetricProvider class can use @metric_value and @metric_partial. For the MetricProvider, QueryMetricProvider, and TableMetricProvider classes, these are the only supported Metric decorators. The following table lists the MetricProvider subclasses and their associated Metrics. | Subclass                          | Supported Metrics                                              | | --------------------------------- | ---------------------------------------------------------------| | ColumnAggregateMetricProvider     | @column_aggregate_value, @column_aggregate_partial             | | ColumnMapMetricProvider           | @column_condition_partial, @column_function_partial            | | ColumnPairMapMetricProvider       | @column_pair_condition_partial                                 | | MulticolumnMapMetricProvider      | @multicolumn_condition_partial, @multicolumn_function_partial  | Metric decorator naming conventions The following Metric decorators differ in terms of the types of inputs and outputs they accept:  A metric_value returns A metric_partials condition_partials versus function_partials ", "Feature and code readiness": " title: Feature and code readiness The following are the readiness levels for Great Expectations features and code:     \u00a0 Experimental: Try, but do not rely  \u00a0 Beta: Ready for early adopters  \u00a0 Production: Ready for general use   These readiness levels allow experimentation, without the need for unnecessary changes as features and APIs evolve. These readiness levels also help streamline development by providing contributors with a clear, incremental path for creating and improving the Great Expectations code base. The following table details Great Expectations readiness levels. Great Expectations uses a cautious approach when determining if a feature or code change should be moved to the next readiness level. If you need a specific feature or code change advanced, open a GitHub issue. | Criteria                                 |  Experimental Try, but do not rely |  Beta Ready for early adopters |  Production Ready for general use | |------------------------------------------|--------------------------------------|----------------------------------|-------------------------------------| | API stability                            | Unstable \u00b9                            | Mostly Stable                    | Stable                              | | Implementation completeness              | Minimal                              | Partial                          | Complete                            | | Unit test coverage                       | Minimal                              | Partial                          | Complete                            | | Integration/Infrastructure test coverage | Minimal                              | Partial                          | Complete                            | | Documentation completeness               | Minimal                              | Partial                          | Complete                            | | Bug risk                                 | High                                 | Moderate                         | Low                                 | \u00b9 When an experimental class is initialized, the following warning message appears in the log:  Warning: great_expectations.some_module.SomeClass is experimental. Methods, APIs, and core behavior may change in the future. Expectation contributions Create and manage Custom Expectations explains how you can create an Expectation with an experimental status. The print_diagnostic_checklist() method provides you with a list of requirements that you must meet to move your Expectation from experimental to beta, and then to production. The first five requirements are required for experimental status, the following three are required for beta status, and the final two are required for production status. | Criteria                                 |  Experimental Try, but do not rely |  Beta Ready for early adopters |  Production Ready for general use | |------------------------------------------|:------------------------------------:|:--------------------------------:|:-----------------------------------:| | Has a valid library_metadata object            | \u2714 | \u2714 | \u2714 | | Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period | \u2714 | \u2714 | \u2714 | | Has at least one positive and negative example case, and all test cases pass | \u2714 | \u2714 | \u2714 | | Has core logic and passes tests on at least one Execution Engine | \u2714 | \u2714 | \u2714 | | Passes all linting checks | \u2714 | \u2714 | \u2714 | | Has basic input validation and type checking | \u2015 | \u2714 | \u2714 | | Has both Statement Renderers: prescriptive and diagnostic | \u2015 | \u2714 | \u2714 | | Has core logic that passes tests for all applicable Execution Engines and SQL dialects | \u2015 | \u2714 | \u2714 | | Has a robust suite of tests, as determined by a code owner | \u2015 | \u2015 | \u2714 | | Has passed a manual review by a code owner for code standards and style guides | \u2015 | \u2015 | \u2714 |", "Contributing Misc and CLA": " title: Contributing Misc and CLA Miscellaneous Contributor license agreement (CLA) When you contribute code, you affirm that the contribution is your original work and that you license the work to the project under the project\u2019s open source license. Whether or not you state this explicitly, by submitting any copyrighted material via pull request, email, or other means you agree to license the material under the project\u2019s open source license and warrant that you have the legal authority to do so. Please make sure you have signed our Contributor License Agreement (either Individual Contributor License Agreement v1.0 or Software Grant and Corporate Contributor License Agreement (\u201cAgreement\u201d) v1.0). We are not asking you to assign copyright to us, but to give us the right to distribute your code without restriction. We ask this of all contributors in order to assure our users of the origin and continuing existence of the code. You only need to sign the CLA once. Issue Tags We use stalebot to automatically tag issues without activity as stale, and close them if no response is received in one week. Adding the stalebot-exempt tag will prevent the bot from trying to close the issue. Additionally, we try to add tags to indicate the status of key discussion elements:   help wanted covers issues where we have not prioritized the request, but believe the feature is useful and so we would welcome community contributors to help accelerate development.   enhacement and expectation-request indicate discussion of potential new features for Great Expectations   good first issue indicates a small-ish task that would be a good way to begin making contributions to Great Expectations  ", "Contribute": " title: Contribute id: contributing  To contribute to Great Expectations documentation or code, see one of the following resources:   To request a documentation change, or a change that doesn't require local testing, see the README in the docs repository.   To submit a code change to Great Expectations for consideration, see CONTRIBUTING_CODE in the great_expectations repository.   To create and submit a Custom Expectation to Great Expectations for consideration, see CONTRIBUTING_EXPECTATIONS in the great_expectations repository.   To submit a custom package to Great Expectations for consideration, see CONTRIBUTING_PACKAGES in the great_expectations repository.   If you're not sure where to start, or you want to learn what other contributors are doing, go to the Great Expectations Slack community and introduce yourself in the #contributing channel. If you're interested in helping out, review the GitHub issues list and self-assign a help wanted issue labels. If you can't find an issue that interests you, and you want to add an improvement or change, create a new issue. If you're working on an existing or new issue, add a comment to introduce yourself and to let everyone know you\u2019re working on the issue.", "\"Integrate Great Expectations with AWS\"": " title: \"Integrate Great Expectations with AWS\" sidebar_label: \"Amazon Web Services (AWS)\" description: \"Integrate Great Expectations with AWS\" id: aws_lp  import LinkCardGrid from '/docs/components/LinkCardGrid'; import LinkCard from '/docs/components/LinkCard'; Integrate Great Expectations (GX) with AWS Glue, AWS with S3 and Pandas, AWS with S3 and Spark, AWS with Athena, AWS with Redshift, AWS EMR Serverless, and EMR Spark.         ", "\"Deploy Great Expectations in hosted environments without a file system\"": " title: \"Deploy Great Expectations in hosted environments without a file system\" sidebar_label: \"Hosted environments\" description: \"Use Great Expectations in hosted environments\" id: how_to_instantiate_a_data_context_hosted_environments sidebar_custom_props: { icon: 'img/integrations/hosted_icon.svg' }  The components in the great_expectations.yml file define the Validation Results Stores, Data Source connections, and Data Docs hosts for a Data Context. These components might be inaccessible in hosted environments, such as Databricks, Amazon EMR, and Google Cloud Composer. The information provided here is intended to help you use Great Expectations in hosted environments. Configure your Data Context To use code to create a Data Context, see Instantiate an Ephemeral Data Context. To configure a Data Context for a specific environment, see one of the following resources:  How to instantiate a Data Context on an EMR Spark cluster How to use Great Expectations in Databricks  Create Expectation Suites and add Expectations To add a Data Source and an Expectation Suite, see How to connect to a PostgreSQL database. To add Expectations to your Suite individually, use the following code: validator.expect_column_values_to_not_be_null(\"my_column\") validator.save_expectation_suite(discard_failed_expectations=False) To configure your Expectation store to load a Suite at a later time, see one of the following resources:  How to configure an Expectation store to use Amazon S3 How to configure an Expectation store to use Azure Blob Storage How to configure an Expectation store to use GCS How to configure an Expectation store to use a filesystem How to configure an Expectation store to use PostgreSQL  Run validation To create and run a Checkpoint in code, see How to create a new Checkpoint.  In a hosted environment you will not be able to store the Checkpoint for repeated use across Python sessions, but you can recreate it each time your scripts run. Use Data Docs To build and view Data Docs in your environment, see Options for hosting Data Docs.", "Instantiate a Data Context on an EMR Spark cluster": " title: Instantiate a Data Context on an EMR Spark cluster description: \"Instantiate a Data Context on an EMR Spark cluster\" sidebar_label: \"Amazon EMR Spark cluster\" sidebar_custom_props: { icon: 'img/integrations/spark_icon.png' }  import Prerequisites from './components/deployment_pattern_prerequisites.jsx' Use the information provided here to instantiate a Data Context on an EMR Spark cluster without a full configuration directory. Prerequisites   Install Great Expectations on your EMR Spark cluster  Copy this code snippet into a cell in your EMR Spark notebook and then run it:  python   sc.install_pypi_package(\"great_expectations\") Configure a Data Context in code   Create an in-code Data Context. See Instantiate an Ephemeral Data Context.   Copy the Python code at the end of How to instantiate an Ephemeral Data Context into a cell in your EMR Spark notebook, or use the other examples to customize your configuration. The code instantiates and configures a Data Context for an EMR Spark cluster.   Test your configuration   Execute the cell with the snippet you copied in the previous step.   Copy the code snippet into a cell in your EMR Spark notebook.   Run the following command to verify that an error isn't returned:   python       context.list_datasources()", "Use Great Expectations with AWS Glue": " title: Use Great Expectations with AWS Glue description: AWS Glue sidebar_label: \"AWS Glue\" sidebar_custom_props: { icon: 'img/integrations/aws_glue_icon.svg' }  AWS Glue is a serverless data integration service by Amazon.  It makes it easier to prepare, move, and integrate data from multiple sources.  It has applications for analytics, machine learning, and application development. :::info  The most recent version of GX that supports this integration is GX 0.15.50 To use Great Expectations (GX) with AWS Glue, see How to use GX in AWS Glue in version 0.15.50 of the documentation. For more information about implementing and using AWS Glue, see the AWS Glue site. :::", "Use Great Expectations in Deepnote": " title: Use Great Expectations in Deepnote description: \"Use Great Expectations with Deepnote\" sidebar_label: \"Deepnote\" sidebar_custom_props: { icon: 'img/integrations/deepnote_icon.png' }  Deepnote is a Jupyter compatible, cloud-hosted, data science notebook that helps teams collaborate to discover, understand and share findings in their data stack. Integrating Great Expectations with Deepnote allows documentation to be hosted and notebooks to be scheduled.  :::info  The most recent version of GX that supports this integration is GX 0.15.50 To use Great Expectations (GX) with Deepnote, see How to use GX in Deepnote in version 0.15.50 of the documentation. For more information about implementing and using Deepnote, see the Deepnote site. :::", "Use Great Expectations with EMR Serverless": " title: Use Great Expectations with EMR Serverless description: \"Use Great Expectations with Amazon EMR Serverless\" sidebar_label: \"Amazon EMR Serverless\" sidebar_custom_props: { icon: 'img/integrations/emr_serverless_icon.png' }  EMR Serverless is a deployment option for Amazon EMR that provides a serverless runtime environment.  It simplifies the operation of analytics applications using the latest open source frameworks, such as Apache Hive or Apache Spark.  EMR Serverless removes the need to configure, secure, optimize, and operate clusters for running applications with supported frameworks. :::info  The most recent version of GX that supports this integration is GX 0.15.50 To use Great Expectations (GX) with Amazon EMR Serverless, see How to use Great Expectations in EMR Serverless in version 0.15.50 of the documentation. For more information about implementing and using Amazon EMR Serverless, see the Amazon EMR documentation. :::", "Use Great Expectations in Flyte": " title: Use Great Expectations in Flyte description: \"Use Great Expectations with Flyte\" sidebar_label: \"Flyte\" sidebar_custom_props: { icon: 'img/integrations/flyte_icon.png' }  Flyte is a structured programming and distributed processing platform that enables highly concurrent, scalable, and maintainable workflows for Machine Learning and Data Processing. It is a fabric that connects disparate computation backends using a type-safe data dependency graph. It records all changes to a pipeline, making it possible to rewind time. It also stores a history of all executions and provides an intuitive UI, CLI, and REST/gRPC API to interact with the computation. The power of data validation in Great Expectations can be integrated with Flyte to validate the data moving in and out of the pipeline entities you may have defined in Flyte. This helps establish stricter boundaries around your data to ensure that everything works as expected and data does not crash your pipelines unexpectedly! :::info  The most recent version of GX that supports this integration is GX 0.15.50 To use Great Expectations (GX) in Flyte, see How to use GX in Flyte in version 0.15.50 of the documentation. For more information about implementing and using Flyte, see the Flyte site. :::", "How to Use Great Expectations with Airflow": " title: How to Use Great Expectations with Airflow sidebar_label: \"Airflow\" description: \"Run a Great Expectations checkpoint in Apache Airflow\" id: how_to_use_great_expectations_with_airflow sidebar_custom_props: { icon: 'img/integrations/airflow_icon.png' }  Airflow is a data orchestration tool for creating and maintaining data pipelines through DAGs written in Python. DAGs complete work through operators, which are templates that encapsulate a specific type of work. :::info  Consult Astronomer's Orchestrate Great Expectations with Airflow guide for more information on how to set up and configure the GreatExpectationsOperator in an Airflow DAG. For more information about implementing and using Airflow, see Astronomer's documentation. :::", "Use Great Expectations with Google Cloud Platform and BigQuery": " title: Use Great Expectations with Google Cloud Platform and BigQuery description: \"Use Great Expectations with Google Cloud Platform and BigQuery\" sidebar_label: \"Google Cloud Platform and BigQuery\" sidebar_custom_props: { icon: 'img/integrations/google_cloud_icon.png' }  import Prerequisites from './components/deployment_pattern_prerequisites.jsx' import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; import Congratulations from '../guides/connecting_to_your_data/components/congratulations.md' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; This guide will help you integrate Great Expectations (GX) with Google Cloud Platform (GCP) using our recommended workflow. Prerequisites   A working local installation of Great Expectations version 0.13.49 or later. Familiarity with Google Cloud Platform features and functionality. Have completed the set-up of a GCP project with a running Google Cloud Storage container that is accessible from your region, and read/write access to a BigQuery database if this is where you are loading your data. Access to a GCP Service Account with permission to access and read objects in Google Cloud Storage, and read/write access to a BigQuery database if this is where you are loading your data.   :::caution Note on Installing Great Expectations in Google Cloud Composer Currently, Great Expectations will only install in Composer 1 and Composer 2 environments with the following packages pinned. [tornado]==6.2 [nbconvert]==6.4.5 [mistune]==0.8.4 We are currently investigating ways to provide a smoother deployment experience in Google Composer, and will have more updates soon. ::: We recommend that you use Great Expectations in GCP by using the following services:   - Google Cloud Composer (GCC) for managing workflow orchestration including validating your data. GCC is built on Apache Airflow.   - BigQuery or files in Google Cloud Storage (GCS) as your Data Source   - GCS for storing metadata (Expectation Suites, Validation Results, Data Docs)   - Google App Engine (GAE) for hosting and controlling access to Data Docs. We also recommend that you deploy Great Expectations to GCP in two steps: 1. Developing a local configuration for GX that uses GCP services to connect to your data, store Great Expectations metadata, and run a Checkpoint. 2. Migrating the local configuration to Cloud Composer so that the workflow can be orchestrated automatically on GCP. The following diagram shows the recommended components for a Great Expectations deployment in GCP:  Relevant documentation for the components can also be found here:  How to configure an Expectation store to use GCS How to configure a Validation Result store in GCS How to host and share Data Docs on GCS Optionally, you can also use a Secret Manager for GCP Credentials  Part 1: Local Configuration of Great Expectations that connects to Google Cloud Platform 1. If necessary, upgrade your Great Expectations version The current guide was developed and tested using Great Expectations 0.13.49. Please ensure that your current version is equal or newer than this. A local installation of Great Expectations can be upgraded using a simple pip install command with the --upgrade flag. bash pip install great-expectations --upgrade 2. Get DataContext: One way to create a new Data Context  is by using the create() method. From a Notebook or script where you want to deploy Great Expectations run the following command. Here the full_path_to_project_directory  can be an empty directory where you intend to build your Great Expectations configuration. YAML name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_gcs.py get_context\" 3. Connect to Metadata Stores on GCP The following sections describe how you can take a basic local configuration of Great Expectations and connect it to Metadata stores on GCP. The full configuration used in this guide can be found in the great-expectations repository and is also linked at the bottom of this document. :::note Note on Trailing Slashes in Metadata Store prefixes When specifying prefix values for Metadata Stores in GCS, please ensure that a trailing slash / is not included (ie prefix: my_prefix/ ). Currently this creates an additional folder with the name / and stores metadata in the / folder instead of my_prefix. ::: Add Expectations Store By default, newly profiled Expectations are stored in JSON format in the expectations/ subdirectory of your great_expectations/ folder. A new Expectations Store can be configured by adding the following lines into your great_expectations.yml file, replacing the project, bucket and prefix with your information. YAML name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_gcs.py expected_expectation_store\" Great Expectations can then be configured to use this new Expectations Store, expectations_GCS_store, by setting the expectations_store_name value in the great_expectations.yml file. YAML name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_gcs.py new_expectation_store\" For additional details and example configurations, please refer to How to configure an Expectation store to use GCS. Add Validations Store By default, Validations are stored in JSON format in the uncommitted/validations/ subdirectory of your great_expectations/ folder. A new Validations Store can be configured by adding the following lines into your great_expectations.yml file, replacing the project, bucket and prefix with your information. YAML name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_gcs.py expected_validations_store\" Great Expectations can then be configured to use this new Validations Store, validations_GCS_store, by setting the validations_store_name value in the great_expectations.yml file. YAML name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_gcs.py new_validations_store\" For additional details and example configurations, please refer to  How to configure an Validation Result store to use GCS. Add Data Docs Store To host and share Datadocs on GCS, we recommend using the following guide, which will explain how to host and share Data Docs on Google Cloud Storage using IP-based access. Afterwards, your great-expectations.yml will contain the following configuration under data_docs_sites,  with project, and bucket being replaced with your information. YAML name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_gcs.py new_data_docs_store\" You should also be able to view the deployed DataDocs site by running the following CLI command: bash gcloud app browse If successful, the gcloud CLI will provide the URL to your app and launch it in a new browser window, and you should be able to view the index page of your Data Docs site. 4. Connect to your Data The remaining sections in Part 1 contain descriptions of how to connect to your data in Google Cloud Storage (GCS) or BigQuery and build a Checkpoint that you'll migrate to Google Cloud Composer.  More details can be found in the corresponding How to Guides, which have been linked.   Using the  Data Context that was initialized in the previous section, add the name of your GCS bucket to the add_pandas_gcs function. python name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_gcs.py datasource\" In the example, we have added a Data Source that connects to data in GCS using a Pandas dataframe. The name of the new datasource is gcs_datasource and it refers to a GCS bucket named test_docs_data. For more details on how to configure the Data Source, and additional information on authentication, please refer to How to connect to data on GCS using Pandas    :::note In order to support tables that are created as the result of queries in BigQuery, Great Expectations previously asked users to define a named permanent table to be used as a \"temporary\" table that could later be deleted, or set to expire by the database. This is no longer the case, and Great Expectations will automatically set tables that are created as the result of queries to expire after 1 day. ::: Using the  Data Context that was initialized in the previous section, create a Data Source that will connect to data in BigQuery, python name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_bigquery.py add_bigquery_datasource\" In the example, we have created a Data Source named my_bigquery_datasource, using the add_or_update_sql method and passing in a connection string. To configure the BigQuery Data Source, see How to connect to a BigQuery database.   4. Create Assets   Add a CSV Asset to your Datasource by using the add_csv_asset function. First, configure the prefix and batching_regex. The prefix is for the path in the GCS bucket where we can find our files. The batching_regex is a regular expression that indicates which files to treat as Batches in the Asset, and how to identify them. python name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_gcs.py prefix_and_batching_regex\" In our example, the pattern r\"data/taxi_yellow_tripdata_samples/yellow_tripdata_sample_(?P<year>\\d{4})-(?P<month>\\d{2})\\.csv\" is intended to build a Batch for each file in the GCS bucket, which are: bash test_docs_data/data/taxi_yellow_tripdata_samples/yellow_tripdata_sample_2019-01.csv test_docs_data/data/taxi_yellow_tripdata_samples/yellow_tripdata_sample_2019-02.csv test_docs_data/data/taxi_yellow_tripdata_samples/yellow_tripdata_sample_2019-03.csv The batching_regex pattern will match the 4 digits of the year portion and assign it to the year domain, and match the 2 digits of the month portion and assign it to the month domain. Next we can add an Asset named csv_taxi_gcs_asset to our Data Source by using the add_csv_asset function.  python name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_gcs.py asset\"   Add a BigQuery Asset into your Datasource either as a table asset or query asset. In the first example, a table Asset named my_table_asset is built by naming the table in our BigQuery Database, which is taxi_data in our case.  python name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_bigquery.py add_bigquery_table_asset\" In the second example, a query Asset named my_query_asset is built by submitting a query to the same table taxi_data. Although we are showing a simple operation here, the query can be arbitrarily complicated, including any number of JOIN, SELECT operations and subqueries.  python name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_bigquery.py add_bigquery_query_asset\"   5. Get Batch and Create ExpectationSuite   For our example, we will be creating an ExpectationSuite with instant feedback from a sample Batch of data, which we will describe in our BatchRequest. For additional examples on how to create ExpectationSuites, either through domain knowledge or using a DataAssistant or a Custom Profiler, please refer to the documentation under How to Guides -> Creating and editing Expectations for your data -> Core skills. First create an ExpectationSuite by using the add_or_update_expectation_suite method on our DataContext. Then use it to get a Validator.  python name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_gcs.py add_expectation_suite\" Next, use the Validator to run expectations on the batch and automatically add them to the ExpectationSuite. For our example, we will add expect_column_values_to_not_be_null and expect_column_values_to_be_between (passenger_count and congestion_surcharge are columns in our test data, and they can be replaced with columns in your data). python name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_gcs.py validator_calls\" Lastly, save the ExpectationSuite, which now contains our two Expectations. python name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_gcs.py save_expectation_suite\" For more details on how to configure the RuntimeBatchRequest, as well as an example of how you can load data by specifying a GCS path to a single CSV, please refer to How to connect to data on GCS using Pandas   For our example, we will be creating our ExpectationSuite with instant feedback from a sample Batch of data, which we will describe in our RuntimeBatchRequest. For additional examples on how to create ExpectationSuites, either through domain knowledge or using a DataAssistant or a Custom Profiler, please refer to the documentation under How to Guides -> Creating and editing Expectations for your data -> Core skills. Using the table_asset from the previous step, build a BatchRequest.  python name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_bigquery.py batch_request\" Next, create an ExpectationSuite by using the add_or_update_expectation_suite method on our DataContext. Then use it to get a Validator.  python name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_bigquery.py add_or_update_expectation_suite\" Next, use the Validator to run expectations on the batch and automatically add them to the ExpectationSuite. For our example, we will add expect_column_values_to_not_be_null and expect_column_values_to_be_between (passenger_count and congestion_surcharge are columns in our test data, and they can be replaced with columns in your data). python name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_bigquery.py validator_calls\" Lastly, save the ExpectationSuite, which now contains our two Expectations. python name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_bigquery.py save_expectation_suite\" To configure the BatchRequest and learn how you can load data by specifying a table name, see How to connect to a BigQuery database   5. Build and Run a Checkpoint For our example, we will create a basic Checkpoint configuration using the SimpleCheckpoint class. For additional examples, information on how to add validations, data, or suites to existing checkpoints, and more complex configurations please refer to the documentation under How to Guides -> Validating your data -> Checkpoints.   Add the following Checkpoint gcs_checkpoint to the DataContext.  Here we are using the same BatchRequest and ExpectationSuite name that we used to create our Validator above. python name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_gcs.py checkpoint\" Next, you can run the Checkpoint directly in-code by calling checkpoint.run(). python name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_gcs.py run_checkpoint\" At this point, if you have successfully configured the local prototype, you will have the following:  An ExpectationSuite in the GCS bucket configured in expectations_GCS_store (ExpectationSuite is named test_gcs_suite in our example). A new Validation Result in the GCS bucket configured in validation_GCS_store. Data Docs in the GCS bucket configured in gs_site that is accessible by running gcloud app browse.  Now you are ready to migrate the local configuration to Cloud Composer.   Add the following Checkpoint bigquery_checkpoint to the DataContext.  Here we are using the same BatchRequest and ExpectationSuite name that we used to create our Validator above. python name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_bigquery.py checkpoint\" Next, you can run the Checkpoint directly in-code by calling checkpoint.run(). python name=\"tests/integration/docusaurus/deployment_patterns/gcp_deployment_patterns_file_bigquery.py run_checkpoint\" At this point, if you have successfully configured the local prototype, you will have the following:  An ExpectationSuite in the GCS bucket configured in expectations_GCS_store (ExpectationSuite is named test_bigquery_suite in our example). A new Validation Result in the GCS bucket configured in validation_GCS_store. Data Docs in the GCS bucket configured in gs_site that is accessible by running gcloud app browse.  Now you are ready to migrate the local configuration to Cloud Composer.   Part 2: Migrating our Local Configuration to Cloud Composer We will now take the local GX configuration from Part 1 and migrate it to a Cloud Composer environment so that we can automate the workflow. There are a number of ways that Great Expectations can be run in Cloud Composer or Airflow.  Running a Checkpoint in Airflow using a bash operator Running a Checkpoint in Airflow using a python operator Running a Checkpoint in Airflow using a Airflow operator  For our example, we are going to use the bash operator to run the Checkpoint. This portion of the guide can also be found in the following Walkthrough Video. 1. Create and Configure a Service Account Create and configure a Service Account on GCS with the appropriate privileges needed to run Cloud Composer. Please follow the steps described in the official Google Cloud documentation to create a Service Account on GCP. In order to run Great Expectations in a Cloud Composer environment, your Service Account will need the following privileges:  Composer Worker Logs Viewer Logs Writer Storage Object Creator Storage Object Viewer  If you are accessing data in BigQuery, please ensure your Service account also has privileges for:  BigQuery Data Editor BigQuery Job User BigQuery Read Session User  2. Create Cloud Composer environment Create a Cloud Composer environment in the project you will be running Great Expectations. Please follow the steps described in the official Google Cloud documentation to create an environment that is suited for your needs. :::info Note on Versions. The current Deployment Guide was developed and tested in Great Expectations 0.13.49, Composer 1.17.7 and Airflow 2.0.2. Please ensure your Environment is equivalent or newer than this configuration. ::: 3. Install Great Expectations in Cloud Composer Installing Python dependencies in Cloud Composer can be done through the Composer web Console (recommended), gcloud or through a REST query.  Please follow the steps described in Installing Python dependencies in Google Cloud to install great-expectations in Cloud Composer. If you are connecting to data in BigQuery, please ensure sqlalchemy-bigquery is also installed in your Cloud Composer environment. :::info Troubleshooting Installation If you run into trouble while installing Great Expectations in Cloud Composer, the official Google Cloud documentation offers the following guide on troubleshooting PyPI package installations. ::: 4. Move local configuration to Cloud Composer Cloud Composer uses Cloud Storage to store Apache Airflow DAGs (also known as workflows), with each Environment having an associated Cloud Storage bucket (typically the name of the bucket will follow the pattern [region]-[composer environment name]-[UUID]-bucket). The simplest way to perform the migration is to move the entire local great_expectations/ folder from Part 1 to the Cloud Storage bucket where Composer can access the configuration. First open the Environments page in the Cloud Console, then click on the name of the environment to open the Environment details page. In the Configuration tab, the name of the Cloud Storage bucket can be found to the right of the DAGs folder. This will take you to the folder where DAGs are stored, which can be accessed from the Airflow worker nodes at: /home/airflow/gcsfuse/dags. The location we want to uploads great_expectations/ is one level above the /dags folder. Upload the local great_expectations/ folder either dragging and dropping it into the window, using gsutil cp, or by clicking the Upload Folder button. Once the great_expectations/ folder is uploaded to the Cloud Storage bucket, it will be mapped to the Airflow instances in your Cloud Composer and be accessible from the Airflow Worker nodes at the location: /home/airflow/gcsfuse/great_expectations. 5. Write DAG and Add to Cloud Composer   We will create a simple DAG with a single node (t1) that runs a BashOperator, which we will store in a file named: ge_checkpoint_gcs.py. python name=\"tests/integration/fixtures/gcp_deployment/ge_checkpoint_gcs.py full\" The BashOperator will first change directories to /home/airflow/gcsfuse/great_expectations, where we have uploaded our local configuration. Then we will run the Checkpoint using same CLI command we used to run the Checkpoint locally: ```bash great_expectations checkpoint run gcs_checkpoint ```` To add the DAG to Cloud Composer, move ge_checkpoint_gcs.py to the environment's DAGs folder in Cloud Storage. First, open the Environments page in the Cloud Console, then click on the name of the environment to open the Environment details page. On the Configuration tab, click on the name of the Cloud Storage bucket that is found to the right of the DAGs folder. Upload the local copy of the DAG you want to upload. For more details, please consult the official documentation for Cloud Composer   We will create a simple DAG with a single node (t1) that runs a BashOperator, which we will store in a file named:  ge_checkpoint_bigquery.py. python name=\"tests/integration/fixtures/gcp_deployment/ge_checkpoint_bigquery.py full\" The BashOperator will first change directories to /home/airflow/gcsfuse/great_expectations, where we have uploaded our local configuration. Then we will run the Checkpoint using same CLI command we used to run the Checkpoint locally: bash great_expectations checkpoint run bigquery_checkpoint To add the DAG to Cloud Composer, move ge_checkpoint_bigquery.py to the environment's DAGs folder in Cloud Storage. First, open the Environments page in the Cloud Console, then click on the name of the environment to open the Environment details page. On the Configuration tab, click on the name of the Cloud Storage bucket that is found to the right of the DAGs folder. Upload the local copy of the DAG you want to upload. For more details, please consult the official documentation for Cloud Composer   6. Run DAG / Checkpoint Now that the DAG has been uploaded, we can trigger the DAG using the following methods:  Trigger the DAG manually. Trigger the DAG on a schedule, which we have set to be once-per-day in our DAG Trigger the DAG in response to events.  In order to trigger the DAG manually, first open the Environments page in the Cloud Console, then click on the name of the environment to open the Environment details page. In the Airflow webserver column, follow the Airflow link for your environment. This will open the Airflow web interface for your Cloud Composer environment. In the interface, click on the Trigger Dag button on the DAGs page to run your DAG configuration. 7. Check that DAG / Checkpoint has run successfully If the DAG run was successful, we should see the Success status appear on the DAGs page of the Airflow Web UI. We can also check so check that new Data Docs have been generated by accessing the URL to our gcloud app. 8. Congratulations! You've successfully migrated your Great Expectations configuration to Cloud Composer! There are many ways to iterate and improve this initial version, which used a bash operator for simplicity. For information on more sophisticated ways of triggering Checkpoints, building our DAGs, and dividing our Data Assets into Batches using DataConnectors, please refer to the following documentation:  How to run a Checkpoint in Airflow using a python operator. How to run a Checkpoint in Airflow using a Great Expectations Airflow operator(recommended). How to trigger the DAG on a schedule. How to trigger the DAG on a schedule. How to trigger the DAG in response to events. How to use the Google Kubernetes Engine (GKE) to deploy, manage and scale your application. How to configure a DataConnector to introspect and partition tables in SQL. How to configure a DataConnector to introspect and partition a file system or blob store.  Also, the following scripts and configurations can be found here:  Local GX configuration used in this guide can be found in the great-expectations GIT repository. Script to test BigQuery configuration. Script to test GCS configuration. ", "Use Great Expectations with Meltano": " title: Use Great Expectations with Meltano description: \"Use Great Expectations with Meltano\" sidebar_label: \"Meltano\" sidebar_custom_props: { icon: 'img/integrations/meltano_icon.png' }  Meltano is an Open Source DataOps OS that's used to install and configure data applications (Great Expectations, Singer, dbt, Airflow, etc.) that your team's data platform is built on top of, all in one central repository. Using Meltano enables teams to easily implement DataOps best practices like configuration as code, code reviews, isolated test environments, CI/CD, etc.  A common use case is to manage ELT pipelines with Meltano and as part of ensuring the quality of the data in those pipelines, teams bring in Great Expectations. :::info  The most recent version of GX that supports this integration is GX 0.15.50 To use GX with Meltano, see How to use GX with Meltano in version 0.15.50 of the documentation. For more information about implementing and using Meltano, see the Meltano site. :::", "Use Great Expectations with Prefect": " title: Use Great Expectations with Prefect description: \"Use Great Expectations with Prefect\" sidebar_label: \"Prefect\" sidebar_custom_props: { icon: 'img/integrations/prefect_icon.svg' }  Prefect is a workflow orchestration and observation platform that enables data engineers, ML engineers, and data scientists to stop wondering about their workflows. The Prefect open source library allows users to create workflows using Python and add retries, logging, caching, scheduling, failure notifications, and much more. Prefect Cloud offers all that goodness plus a hosted platform, automations, and enterprise features for users who need them. Prefect Cloud provides free and paid tiers. Prefect can be used with Great Expectations validations so that you can be confident about the state of your data. With a Prefect deployment, you can productionize your workflow and run data quality checks in reaction to the arrival of new data or on a schedule.  :::info  Consult Prefect's Great Expectations + Prefect documentation for more information on using GX with Prefect.  For more information about how to use Prefect, refer to the Prefect documentation. :::", "Use Great Expectations with YData-Synthetic": " title: Use Great Expectations with YData-Synthetic description: \"Use Great Expectations with YData-Synthetic\" sidebar_label: \"YData-Synthetic\" sidebar_custom_props: { icon: 'img/integrations/ydata_synthetic_icon.png' }  YData-Synthetic is an open-source synthetic data engine. Using different kinds of Generative Adversarial Networks (GANS), the engine learns patterns and statistical properties of original data. It can create endless samples of synthetic data that resemble the original data. :::info  To use Great Expectations (GX) with YData-Synthetic, see YData-Synthetic's documentation on GX. For more information about implementing and using YData-Synthetic, see the YData-Synthetic site. :::", "'Integrate'": " sidebar_label: 'Integrate' title: 'Integrate' id: integrations_and_howtos_overview description: Integrate third party products and services with Great Expectations (GX).  This where you'll find information about integrating third party products and services with Great Expectations (GX). The content is a collaborative effort between the teams who maintain the products and GX. To collaborate on integration documentation for a third party product or service, submit your request on Slack.", "\"Integrations: Index\"": " title: \"Integrations: Index\"  Sample Integration ZenML Integration ", "Integrate DataHub with Great Expectations": " title: Integrate DataHub with Great Expectations authors:     name: Mayuri Nehate, John Joyce, Maggie Hays      url: https://datahubproject.io description: \"Integrate DataHub with Great Expectations\" sidebar_label: \"DataHub\" sidebar_custom_props: { icon: 'img/integrations/datahub_icon.png' }  DataHub is a modern data catalog built to enable end-to-end data discovery, observability, and governance.  DataHub is built on an extensible metadata platform that helps manage the complexity of diverse and evolving data ecosystems. :::info  Consult DataHub's metadata ingestion documentation for GX for more information on how to set up and configure DataHub's DataHubValidationAction for GX. For more information about implementing and using DataHub, see the DataHub site. :::", "Integrate ZenML with Great Expectations": " title: Integrate ZenML with Great Expectations authors:     name: Stefan Nica     url: https://zenml.io description: \"Integrate ZenML with Great Expectations\" sidebar_label: \"ZenML\" sidebar_custom_props: { icon: 'img/integrations/zenml_icon.png' }  ZenML is an extensible, open-source framework for creating production-ready, portable machine learning (ML) pipelines.  Integrating Great Expectations (GX) with ZenML helps data scientists and ML engineers to make data profiling and validation an integral part of their production ML toolset and workflows. :::info  The most recent version of GX that supports this integration is GX 0.15.50 To use GX with ZenML, see Integrating ZenML with Great Expectations in version 0.15.50 of the documentation. For more information about implementing and using ZenML, see the ZenML site. :::", "API reference": " title: API reference Great Expectations (GX) API reference content is generated from classes and methods docstrings. If the code or documentation needs improvement, see the contributor docs, or submit your feedback on Slack.", "Customize your deployment": " title: Customize your deployment Customizing your deployment by upgrading specific components of your deployment is a straight forward task. Data Contexts make this modular, so that you can add or swap out one component at a time. Most of these changes are quick, incremental steps\u2014so you can upgrade from a basic demo deployment to a full production deployment at your own pace and be confident that your Data Context will continue to work at every step along the way. This reference guide is designed to present you with clear options for upgrading your deployment. For specific implementation steps, please check out the linked How-to guides. Components Here\u2019s an overview of the components of a typical Great Expectations deployment:  Great Expectations configs and metadata Options for storing Great Expectations configuration Options for storing Expectations Options for storing Validation Results  Options for customizing generated notebooks   Integrations to related systems  Connecting to Data Options for hosting Data Docs Additional Checkpoints and Actions How to update Data Docs as a Validation Action  Options for storing Great Expectations configuration The simplest way to manage your Great Expectations configuration is usually by committing great_expectations/great_expectations.yml to Git. However, it\u2019s not usually a good idea to commit credentials to source control. In some situations, you might need to deploy without access to source control (or maybe even a file system). Here\u2019s how to handle each of those cases:  Configure credentials Instantiate an Ephemeral Data Context  Options for storing Expectations Many teams find it convenient to store Expectations in Git. Essentially, this approach treats Expectations like test fixtures: they live adjacent to code and are stored within version control. Git acts as a collaboration tool and source of record. Alternatively, you can treat Expectations like configs, and store them in a blob store. Finally, you can store them in a database.  How to configure an Expectation store in Amazon S3 How to configure an Expectation store in GCS How to configure an Expectation store in Azure Blob Storage How to configure an Expectation store to PostgreSQL How to configure an Expectation store on a filesystem  Options for storing Validation Results By default, Validation Results are stored locally, in an uncommitted directory. This is great for individual work, but not good for collaboration. The most common pattern is to use a cloud-based blob store such as S3, GCS, or Azure blob store. You can also store Validation Results in a database.  How to configure a Validation Result store on a filesystem How to configure a Validation Result store in Amazon S3 How to configure a Validation Result store in GCS How to configure a Validation Result store in Azure Blob Storage How to configure a Validation Result store to PostgreSQL  Reference Architectures  How to instantiate a Data Context on an EMR Spark cluster How to use Great Expectations in Databricks  Connecting to Data Great Expectations allows you to connect to data in a wide variety of sources, and the list is constantly getting longer. If you have an idea for a source not listed here, please speak up in the public discussion forum.  How to connect to a Athena database How to connect to a BigQuery database How to connect to a MSSQL database How to connect to a MySQL database How to connect to a Postgres database How to connect to a Redshift database How to connect to a Snowflake database How to connect to a SQLite database How to connect to data on a filesystem using Spark How to connect to data on S3 using Spark How to connect to data on GCS using Spark  Options for hosting Data Docs By default, Data Docs are stored locally, in an uncommitted directory. This is great for individual work, but not good for collaboration. A better pattern is usually to deploy to a cloud-based blob store (S3, GCS, or Azure Blob Storage), configured to share a static website.  How to host and share Data Docs on a filesystem How to host and share Data Docs on Azure Blob Storage How to host and share Data Docs on GCS How to host and share Data Docs on Amazon S3  Additional Checkpoints and Actions Most teams will want to configure various Checkpoints and Validation Actions as part of their deployment. There are two primary patterns for deploying Checkpoints. Sometimes Checkpoints are executed during data processing (e.g. as a task within Airflow). From this vantage point, they can control program flow. Sometimes Checkpoints are executed against materialized data. Great Expectations supports both patterns. There are also some rare instances where you may want to validate data without using a Checkpoint.  How to trigger Slack notifications as a Validation Action How to trigger Opsgenie notifications as a Validation Action How to trigger Email as a Validation Action How to deploy a scheduled Checkpoint with cron How to get Data Docs URLs for custom Validation Actions How to pass an in-memory DataFrame to a Checkpoint How to run a Checkpoint in Airflow  Not interested in managing your own configuration or infrastructure? Learn more about Great Expectations Cloud \u2014 our fully managed SaaS offering. Sign up for our weekly cloud workshop! You\u2019ll get to see our newest features and apply for our private Alpha program!", "Expectation Suite Operations": " title: Expectation Suite Operations A Great Expectations Expectation Suite enables you to perform Create, Read, Update, and Delete (CRUD) operations on a Suite's Expectations without needing to re-run them. Each of the methods that support a CRUD operation relies on two main parameters - expectation_configuration and match_type.  expectation_configuration - an ExpectationConfiguration object that is used to determine whether and where this   Expectation already exists within the Suite. It can be a complete or a partial ExpectationConfiguration. match_type - a string with the value of domain, success, or runtime which determines the criteria used for   matching: domain checks whether two Expectation Configurations apply to the same data. It results in the loosest match,   and can use the least complete ExpectationConfiguration object. For example, for a column map   Expectation, domain_kwargs will include the Expectation_type, the column, and any row_conditions that may affect   which rows are evaluated by the Expectation. success criteria are more exacting - in addition to the domain kwargs, these include those kwargs used when   evaluating the success of an Expectation, like mostly, max, or value_set. runtime are the most specific - in addition to domain_kwargs and success_kwargs, these include kwargs used   for runtime configuration. Currently these include result_format, include_config, and catch_exceptions    Adding or updating Expectations To add an Expectation to a Suite, you can use python suite.add_expectation(     expectation_configuration,      match_type,      overwrite_existing )  If a matching Expectation is not found on the Suite, this function will add the Expectation to the Suite. If a matching Expectation is found on the Suite, add_expectation will throw an error, unless overwrite_existing   is set to True, in which case the found Expectation will be updated with expectation_configuration. If more than one Expectation is found, this will throw an error, and you will be prompted to be more specific with   your matching criteria.  Removing Expectations To remove an Expectation from an Expectation Suite, you can use python suite.remove_expectation(     expectation_configuration,      match_type,      remove_multiple_matches )  If this finds one matching Expectation, it will remove it. If it finds more than one matching Expectation, it will throw an error, unless remove_multiple_matches is set to   True, in which case it will remove all matching Expectations. If this finds no matching Expectations, it will throw an error.  If you are interactively working with an Expectation Suite using a Validator, you can access this functionality by directly calling  python validator.remove_expectation(     expectation_configuration,      match_type,      remove_multiple_matches )", "'Reference'": " sidebar_label: 'Reference' title: 'Reference' id: reference_overview description: Supplemental information that will help you get the most out of Great Expectations.  import LinkCardGrid from '/docs/components/LinkCardGrid'; import LinkCard from '/docs/components/LinkCard'; This is where you'll find supplemental information that will help you get the most out of Great Expectations (GX).        ", "Usage statistics": " title: Usage statistics To help us improve the tool, by default we track event data when certain Data Context-enabled commands are run. Our blog post from April 2020 explains a little bit more about what we want to capture with usage statistics and why! The usage statistics include things like the OS and python version, and which GX features are used. You can see the exact schemas for all of our messages here. While we hope you'll leave them on, you can easily disable usage statistics for a Data Context by adding the following to your data context configuration: yaml     anonymous_usage_statistics:       data_context_id: <randomly-generated-uuid>       enabled: false You can also disable usage statistics system-wide by setting the GE_USAGE_STATS environment variable to FALSE or adding the following code block to a file called great_expectations.conf located in /etc/ or ~/.great_expectations: ini     [anonymous_usage_statistics]     enabled=FALSE As always, please reach out on Slack if you have any questions or comments.", "<!-- Add the topic title here -->": " title:    First task Assumed knowledge   An understanding of Great Expectations functionality How to connect to source data How to create an Expectation Suite  First task     Run the following command in an empty base directory inside a Python virtual environment: bash title=\"Terminal input\" pip install great_expectations It can take several minutes for the installation to complete. Jupyter Notebook is included with Great Expectations, and it lets you edit code and view the results of code runs.   Open Jupyter Notebook, a command line, or a terminal and then run the following command to import the great_expectations module: python name=\"tutorials/quickstart/quickstart.py import_gx\"   Run the following command to import the DataContext object: python name=\"tutorials/quickstart/quickstart.py get_context\" 4. Add a comment when the prompt appears:     Second task    Run the following command to create two Expectations. The first Expectation uses domain knowledge (the pickup_datetime shouldn't be null), and the second Expectation uses auto=True to detect a range of values in the passenger_count column.  python name=\"tutorials/quickstart/quickstart.py create_expectation\" The Expectation assumes the pickup_datetime column always contains data.  None of the column's values are null. To analyze Validator data, you can create multiple Expectations that call multiple methods with the validator.expect_* syntax.   Third task    Run the following command to define a Checkpoint and examine the data to determine if it matches the defined Expectations:  python name=\"tutorials/quickstart/quickstart.py create_checkpoint\"   Run the following command to return the Validation results: python name=\"tutorials/quickstart/quickstart.py run_checkpoint\"   Additional tasks  Related documentation   How to install Great Expectations locally How to set up GX to work with data on AWS S3 How to set up GX to work with data in Azure Blob Storage How to set up GX to work with data on GCS How to set up GX to work with SQL databases  How to instantiate a Data Context on an EMR Spark Cluster How to use Great Expectations in Databricks  Next steps   Install Great Expectations locally Set up GX to work with data on AWS S3 Set up GX to work with data in Azure Blob Storage Set up GX to work with data on GCS ", "TEMPLATE How to guide {stub}": " title: TEMPLATE How to guide {stub} import Prerequisites from '/docs/components/_prerequisites.jsx' import NextSteps from '/docs/guides/connecting_to_your_data/components/next_steps.md' import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; This guide will help you {do something.} {That something is important or useful, because of some reason.}   Additional guide-specific prerequisites go here.   :::warning Do not introduce additional heading hierarchy or headers. - Guides must have a single h2 ## Steps heading with numbered h3 ### 1. xxx headings below it. This ensures consistency and supports a built in TOC seen on the upper right of all guides. - Guides may have an optional ## Additional Notes and ## Next Steps headings as shown at the end of this template. ::: Steps :::tip What qualifies as a step? To qualify as a step, the things within it must either: - require user input (change a name, add credentials, etc) - or require user to run something and view output ::: 1. First do this Run this code to {do a thing}. python name=\"tests/integration/docusaurus/template/script_example.py first import\" 2. Next do this. {Concise description of what the user is going to do}. python name=\"tests/integration/docusaurus/template/script_example.py assert\" 3. Finally, do this. :::tip Use tabs to represent choices. When using Great Expectations there are sometimes choices to be made that do not warrant a separate how to guide. Use tabs to represent these. ::: Next, do {concise description of what the user is going to do} using either {choice in tab 1} or {choice in tab 2}.   Run this command in the CLI. console great_expectations suite new   Run this code in Python. python name=\"tests/integration/docusaurus/template/script_example.py imports\"   Congratulations! You successfully {did the thing this guide is about}. Additional Notes :::tip Additional Notes are optional and may contain: - links to any scripts used in this guide to facilitate user feedback via GitHub - additional context (use extremely sparingly) ::: To view the full scripts used in this page, see them on GitHub:  postgres_yaml_example.py postgres_python_example.py  Next Steps :::tip Next steps are optional Include them only if it makes sense. ::: Now that you've {done a thing}, you'll want to work on these {other things}:  How to xxx How to yyy ", "(Template) Integrating { PRODUCT NAME } With Great Expectations": " title: (Template) Integrating { PRODUCT NAME } With Great Expectations authors:     name: AUTHOR NAME     url: https://superurl.com  :::info * Maintained By: Company Name * Status: Alpha | Beta | Production/Mature * Testing: https://URL-to-CI-If-Public.io (otherwise remove) * Support/Contact: https://yoururl.com ::: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Viverra nam libero justo laoreet. Hendrerit gravida rutrum quisque non. Mauris pellentesque pulvinar pellentesque habitant morbi. Sed enim ut sem viverra aliquet eget. A diam maecenas sed enim ut sem viverra aliquet eget. Non sodales neque sodales ut etiam sit amet nisl. Fermentum iaculis eu non diam. Technical background Prerequisites  Req1  Req2  Req3  Deeper and longer information goes here. Dev loops unlocked by integration  Deploy magnetic interfaces Optimize distributed interfaces Embrace granular bandwidth  Setup Consequat interdum varius sit amet mattis vulputate enim nulla. Ipsum a arcu cursus vitae congue mauris rhoncus aenean. Nunc sed velit dignissim sodales ut eu. 1. Step1 2. Step2 3. Run following command python import step from steps Usage Dicant homero intellegebat vel ei, in qui summo iriure. Nec malorum vivendum ut. Ne nobis invenire pri, qui minim phaedrum hendrerit ad, te mel deseruisse reprehendunt. Nec at percipit reprimique. Te nemore dictas option per, aeterno albucius pro ea. Example doing this Qui eu quot modus singulis, qui forensibus comprehensam ut. Facer deterruisset ne eos. Eu cum lorem omnes. Ex pro dico partem mucius, no sed recusabo principes. Vel brute abhorreant te. :::tip Stand up and take a breath ::: 1. Step one 2. Step two 3. Step three 4. Step four 5. Result bash  It worked! Example doing that Mea affert temporibus in, nec ei paulo praesent instructior. Eum ex voluptua postulant, nec te ullum simul omittam. :::danger This process is destructive, so proceed with extreme care. ::: 1. Stufe eins 2. Stufe zwei 3. Stufe drei 4. Ergebnis shell Vollst\u00e4ndig! Further discussion Things to consider Sed ut perspiciatis, unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam eaque ipsa, quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt, explicabo. Nemo enim ipsam voluptatem, quia voluptas sit, aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos, qui ratione voluptatem sequi nesciunt When things don't work :::info If you're a premium paid customer, feel free to open a support ticket in your support portal :::  Try rebooting your machine Try rebooting again Try with laptop Try with your phone Try reaching out to our community support at (email or Slack URL)  FAQs  What is it even? Quite frankly, we're not sure My computer won't turn on? Is it plugged in? How do you beat Starscourge Radhan? We don't know but let us know if you find out!  (Optional) Other resources  Possibly a video walk-through Link to a relevant tutorial ", "Action": " title: Action import TechnicalTag from '../term_tags/_tag.mdx'; An Action is a Python class with a run() method that takes a Validation Result and does something with it. Actions are customizable.  Great Expectations comes with common Actions for such things as sending email or Slack notifications, updating Data Docs, and storing Validation Results out of the box.  However, it is easy to create custom Actions by creating a subclass of Great Expectations' ValidationAction class and overwriting its _run() method. This means that you can configure an Action to do literally anything you are capable of programming in Python in response to a Checkpoint Validation completing. Relationship to other objects Actions are configured inside the action_list parameter of a Checkpoint's configuration, and execute every time the Checkpoint finishes running a Validation.  When an Action is run, it will have access to the Validation Results and configured Metrics that were generated by the Checkpoint. Use cases Configuring, implementing, and executing an Action (custom or otherwise) takes place in the Validate Data step. Creating custom Actions is a process that falls outside the workflow of using Great Expectations. Actions are configured when Checkpoints are created in the Validate Data step of working with Great Expectations.  After that, Checkpoints that have a populated action_list in their configuration will execute the indicated Actions every time they finish running a Validation. Versatility The features of a specific Action depend on what that Action is designed to do.  An Action can perform anything that can be done with Python code, making them phenomenally versatile. Convenience Since some Actions are common, Great Expectations implements them out of the box: You don't have to write every Action as a custom one.  These Actions are subclasses of the ValidationAction class, and you can view them in the great_expectations.checkpoint.actions module. The following are some of the available pre-built Actions:  EmailAction: sends an email to a given list of email addresses. MicrosoftTeamsNotificationAction: sends a Microsoft Teams notification to a given webhook. SlackNotificationAction: sends a Slack notification to a given webhook. StoreEvaluationParametersAction: extracts Evaluation Parameters from a Validation Result and stores them in the Store configured for this Action. StoreMetricsAction: extracts Metrics from a Validation Result and stores them in a Metrics Store. StoreValidationResultAction: stores a Validation Result in the ValidationsStore. UpdateDataDocsAction: notifies the site builders of all the data docs sites of the Data Context that a validation result should be added to the data docs.  Access Actions are not intended to be manually instantiated or accessed.  Instead, they are included in the action_list parameter of a Checkpoint's configuration, and when the Checkpoint finishes running a Validation it will then run the Actions in its action_list in order of appearance. Classes that implement Actions can be found in the great_expectations.checkpoint.actions module, which you can view on GitHub: - great_expectations.checkpoint.actions Create Custom actions need to be created in Python code, and can be implemented as Plugins.  In order for a Python class to be a valid Action, it must conform to the Action API.  Specifically, it must implement a run() method which accepts three required parameters, two named optional parameters, and any number of **kwargs. Required parameters  validation_result_suite: an instance of the ExpectationSuiteValidationResult class. validation_result_suite_identifier: an instance of either the ValidationResultIdentifier class (for open source Great Expectations) or the GeCloudIdentifier (from Great Expectations Cloud). data_asset: an instance of the Validator class.  Optional parameters  expectation_suite_identifier checkpoint_identifier  Additional parameters  **kwargs: named parameters that are specific to a given Action, and need to be assigned a value in the Action's configuration in a Checkpoint's action_list.  The required and optional named parameters will be automatically passed to the Action from the Checkpoint that the Action is included with, after the Checkpoint completes Validation.  This means you can configure your custom Actions to behave conditionally based on the Validation Results your Checkpoint generates, or the values passed along with any of those named parameters.  Additional **kwargs parameters can be included, but they cannot be passed automatically by the Checkpoint.  Therefore, you will have to specify the name and value for these parameters in the configuration for their Action, in the Checkpoint's action_list. The best practice when creating a custom Action is to create a subclass of the ValidationAction class found in the great_expectations.checkpoint.actions module.  Leave the inherited run() method as its default, and overwrite the _run() method to contain your functionality.  There are numerous examples of this practice in the subclasses of ValidationAction located in the great_expectations.checkpoint.actions module, which you can view on GitHub: - great_expectations.checkpoint.actions If you develop a custom Action, consider making it a contribution in the Great Expectations open source GitHub project.  You can also reach out to us on Slack if you need additional guidance in your efforts. Configure Actions are configured inside of the action_list parameter for Checkpoints.  In general, you will need to provide at least a name (which is user defined and does not need to correspond to anything specific) and, in the in the parameters under action a class_name (which should correspond to the name of the Action's Python class).  If you are implementing a custom Action in a Plugin, you will also need to include a module_name in the parameters under action which references where your Plugin is located.  Any other keys placed under action will be passed to the Action's class as additional key word arguments. The following is an example of an action_list configuration that performs some common Actions that are built in to the Great Expectations code base: yaml action_list: - name: store_validation_result   action:     class_name: StoreValidationResultAction - name: store_evaluation_params   action:     class_name: StoreEvaluationParametersAction - name: update_data_docs   action:     class_name: UpdateDataDocsAction To configure Actions in a Checkpoint, see Configure Actions.", "Batch": " title: Batch id: batch hoverText: A selection of records from a Data Asset.  import TechnicalTag from '../term_tags/_tag.mdx'; A Batch is a selection of records from a Data Asset. A Batch provides a consistent interface for describing specific data from any Data Source, to support building Metrics, Validation, and Profiling. Batches are designed to be \"MECE\" -- mutually exclusive and collectively exhaustive partitions of Data Assets. However, in many cases the same underlying data could be present in multiple batches, for example if an analyst runs an analysis against an entire table of data each day, with only a fraction of new records being added. Consequently, the best way to understand what \"makes a Batch a Batch\" is the act of attending to it. Once you have defined how a Data Source's data should be sliced (even if that is to define a single slice containing all of the data in the Data Asset), you have determined what makes those particular Batches \"a Batch.\"  The Batch is the fundamental unit that Great Expectations will validate and about which it will collect metrics. Relationship to other objects A Batch is generated by providing a Batch Request to a Data Asset. It provides a reference to interact with the data through the Data Asset and adds metadata to precisely identify the specific data included in the Batch. Profilers use Batches to generate Metrics and potential Expectations based on the data. Batches make it possible for the Profiler to compare data over time and sample from large datasets to improve performance. Metrics are always associated with a Batch of data. The identifier for the Batch is the primary way that Great Expectations identifies what data to use when computing a Metric and how to store that Metric. Batches are also used by Validators when they run an Expectation Suite against data. Use Cases When creating Expectations interactively, a Validator needs access to a specific Batch of data against which to check Expectations. The how to guide on interactively creating expectations covers using a Batch in this use case. During Validation, a Checkpoint checks a Batch of data against Expectations from an Expectation Suite. You must specify a Batch Request for the Checkpoint to run. Consistency A Batch is always part of a Data Asset. A Data Asset can be configured to slice its data into batches in many ways. For example, it can be based on an arbitrary field, including datetimes, from the data. A Batch is always built using a Batch Request. See Batch Request or Connect to source data. Once a Data Asset identifies the specific data that will be included in a Batch based on the Batch Request, it creates a reference to the data and adds metadata to including the parameters used in the Batch Request. Access You can access a Batch through the Data Asset get_batch_list_from_batch_request method. You typically will not need to access the Batch directly. Instead, you will pass a Batch Request to a Expectations object such as a Profiler, Validator, or Checkpoint, which will then do something in response to the Batch's data. Create The BatchRequest object is the primary API used to construct Batches. You construct a Batch Request that corresponds to a batch via the Data Asset's method build_batch_request.  For more information, see our documentation on Batch Requests.  :::note Instantiating a Batch does not necessarily \u201cfetch\u201d the data by immediately running a query or pulling data into memory. Instead, think of a Batch as a wrapper that includes the information that you will need to fetch the right data when it\u2019s time to Validate. :::", "\"Batch Request\"": " title: \"Batch Request\" import TechnicalTag from '../term_tags/_tag.mdx'; A Batch Request specifies a Batch of data. It can be created by using the build_batch_request method found on a Data Asset. A Batch Request contains all the necessary details to query the appropriate underlying data.  The relationship between a Batch Request and the data returned as a Batch is guaranteed.  If a Batch Request identifies multiple Batches that fit the criteria of the user provided options argument to the build_batch_request method on a Data Asset, the Batch Request will return all of the matching Batches. If you are using an interactive session, you can inspect the allowed keys for the options argument for a Data Asset by printing the batch_request_options attribute. Relationship to other objects A Batch Request is always used when Great Expectations builds a Batch. Any time you interact with something that requires a Batch of Data (such as a Profiler, Checkpoint, or Validator) you will use a Batch Request to create the Batch that is used. Use cases If you are using a Custom Profiler or the interactive method of creating Expectations, you will need to provide a Batch of data for the Profiler to analyze or your manually defined Expectations to test against.  For both of these processes, you will therefore need a Batch Request to get the Batch. For more information, see:  How to create Expectations interactively in Python  When Validating data with a Checkpoint, you will need to provide one or more Batch Requests and one or more Expectation Suites.  You can do this at runtime, or by defining Batch Request and Expectation Suite pairs in advance, in the Checkpoint's configuration. For more information on setting up Batch Request/Expectation Suite pairs in a Checkpoint configuration, see How to add validations data or suites to a Checkpoint. Guaranteed relationships The relationship between a Batch and the Batch Request that generated it is guaranteed.  A Batch Request includes all the information necessary to identify a specific Batch or Batches. Batches are always built using a Batch Request.  When the Batch is built metadata is attached to the Batch object and is available via the Batch metadata attribute.  This metadata contains all the option values necessary to recreate the Batch Request that corresponds to the Batch.  Access You will rarely need to access an existing Batch Request.  Instead, you will often build a Batch Request from a Data Asset.  A Batch Request can also be saved to a configuration file when you save an object that required a Batch Request for setup, such as a Checkpoint.  Once you receive a Batch back, it is unlikely you will need to reference to the Batch Request that generated it.  Indeed, if the Batch Request was part of a configuration, Great Expectations will simply initialize a new copy rather than load an existing one when the Batch Request is needed.  Create You can create a Batch Request from a Data Asset by calling build_batch_request.  Here is an example of configuring a Pandas Filesystem Asset and creating a Batch Request: python name=\"tests/integration/docusaurus/reference/glossary/batch_request batch_request\" The options one passes in to specify a batch will vary depending on how the specific Data Asset was configured.  To look at the keys for the options dictionary, you can do the following: python name=\"tests/integration/docusaurus/reference/glossary/batch_request options\"", "Checkpoint": " id: checkpoint title: Checkpoint hoverText: The primary means for validating data in a production deployment of Great Expectations.  import TechnicalTag from '../term_tags/_tag.mdx'; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; A Checkpoint is the primary means for validating data in a production deployment of Great Expectations. Checkpoints provide a convenient abstraction for bundling the Validation of a Batch (or Batches) of data against an Expectation Suite (or several), as well as the Actions that should be taken after the validation. Like Expectation Suites and Validation Results, Checkpoints are managed using a Data Context, and have their own Store which is used to persist their configurations to YAML files. These configurations can be committed to version control and shared with your team. Relationships to other objects  A Checkpoint uses a Validator to run one or more Expectation Suites against one or more Batches provided by one or more Batch Requests. Running a Checkpoint produces Validation Results and will result in optional Actions being performed if they are configured to do so. Use cases In the Validate Data step of working with Great Expectations, there are two points in which you will interact with Checkpoints in different ways: First, when you create them.  And secondly, when you use them to actually Validate your data. Reusable You do not need to re-create a Checkpoint every time you Validate data.  If you have created a Checkpoint that covers your data Validation needs, you can save and re-use it for your future Validation needs.  Since you can set Checkpoints up to receive some of their required information (like Batch Requests) at run time, it is easy to create Checkpoints that can be readily applied to multiple disparate sources of data. Actions One of the most powerful features of Checkpoints is that they can be configured to run Actions, which will do some process based on the Validation Results generated when a Checkpoint is run.  Typical uses include sending email, slack, or custom notifications.  Another common use case is updating Data Docs sites.  However, Actions can be created to do anything you are capable of programing in Python.  This gives you an incredibly versatile tool for integrating Checkpoints in your pipeline's workflow! To set up common Action use cases, see Configure Actions. The classes that implement Checkpoints are in the great_expectations.checkpoint module. Create Creating a Checkpoint is part of the initial setup for data validation.  Checkpoints are reusable and only need to be created once, although you can create multiple Checkpoints to cover multiple Validation use cases. For more information about creating Checkpoints, see How to create a new Checkpoint. After you create a Checkpoint, you can use it to Validate data by running it against a Batch or Batches of data.  The Batch Requests used by a Checkpoint during this process may be pre-defined and saved as part of the Checkpoint's configuration, or the Checkpoint can be configured to accept one or more Batch Request at run time. For more information about data validation, see How to validate data by running a Checkpoint. In its most basic form, a Checkpoint accepts an expectation_suite_name identfying the test suite to run, and a batch_request identifying the data to test. Checkpoint can be directly directly in Python as follows: python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_create_a_new_checkpoint.py create checkpoint batch_request\" For an in-depth guide on Checkpoint creation, see our guide on how to create a new Checkpoint. Configure A Checkpoint uses its configuration to determine what data to Validate against which Expectation Suite(s), and what actions to perform on the Validation Results - these validations and Actions are executed by calling a Checkpoint's run method (analogous to calling validate with a single Batch). Checkpoint configurations are very flexible. At one end of the spectrum, you can specify a complete configuration in a Checkpoint's YAML file, and simply call my_checkpoint.run(). At the other end, you can specify a minimal configuration in the YAML file and provide missing keys as kwargs when calling run. At runtime, a Checkpoint configuration has three required and three optional keys, and is built using a combination of the YAML configuration and any kwargs passed in at runtime: Required keys  name: user-selected Checkpoint name (e.g. \"staging_tables\") config_version: version number of the Checkpoint configuration  validations: a list of dictionaries that describe each validation that is to be executed, including any actions.    Each validation dictionary has three required and three optional keys:     #### Required keys         - batch_request: a dictionary describing the batch of data to validate (learn more about specifying Batches            here: Batches)         - expectation_suite_name: the name of the Expectation Suite to validate the batch of data against         - action_list: a list of actions to perform after each batch is validated Optional keys - `name`: providing a name will allow referencing the validation inside the run by name (e.g. \"    user_table_validation\") - `evaluation_parameters`: used to define named parameters using Great    Expectations [Evaluation Parameter syntax](../terms/evaluation_parameter.md) - `runtime_configuration`: provided to the Validator's `runtime_configuration` (e.g. `result_format`)    Optional keys  class_name: the class of the Checkpoint to be instantiated, defaults to Checkpoint template_name: the name of another Checkpoint to use as a base template run_name_template: a template to create run names, using environment variables and datetime-template syntax (e.g. \"%Y-%M-staging-$MY_ENV_VAR\")  Configure defaults and parameter override behavior Checkpoint configurations follow a nested pattern, where more general keys provide defaults for more specific ones. For instance, any required validation dictionary keys (e.g. expectation_suite_name) can be specified at the top-level (i.e. at the same level as the validations list), serving as runtime defaults. Starting at the earliest reference template, if a configuration key is re-specified, its value can be appended, updated, replaced, or cause an error when redefined. Replaced  name module_name class_name run_name_template expectation_suite_name  Updated  batch_request: at runtime, if a key is re-defined, an error will be thrown action_list: actions that share the same user-defined name will be updated, otherwise a new action will be appended evaluation_parameters runtime_configuration  Appended  action_list: actions that share the same user-defined name will be updated, otherwise a new action will be appended validations  Checkpoint configuration default and override behavior   This configuration specifies full validation dictionaries - no nesting (defaults) are used. When run, this Checkpoint will perform one validation of a single batch of data, against a single Expectation Suite (\"my_expectation_suite\"). YAML: yaml name=\"tests/integration/docusaurus/reference/core_concepts/checkpoints_and_actions.py no_nesting just the yaml\" runtime: python name=\"tests/integration/docusaurus/reference/core_concepts/checkpoints_and_actions.py run_checkpoint\"   This configuration specifies four top-level keys (\"expectation_suite_name\", \"action_list\", \"evaluation_parameters\", and \"runtime_configuration\") that can serve as defaults for each validation, allowing the keys to be omitted from the validation dictionaries. When run, this Checkpoint will perform two Validations of two different Batches of data, both against the same Expectation Suite (\"my_expectation_suite\"). Each Validation will trigger the same set of Actions and use the same Evaluation Parameters and runtime configuration. YAML: yaml name=\"tests/integration/docusaurus/reference/core_concepts/checkpoints_and_actions.py nesting_with_defaults just the yaml\" Runtime: python name=\"tests/integration/docusaurus/reference/core_concepts/checkpoints_and_actions.py run_checkpoint_2\" Results: python name=\"tests/integration/docusaurus/reference/core_concepts/checkpoints_and_actions.py validation_results_suites_data_assets\"   This configuration omits the \"validations\" key from the YAML, which means a \"validations \" list must be provided when the Checkpoint is run. Because \"action_list\", \"evaluation_parameters\", and \"runtime_configuration\" appear as top-level keys in the YAML configuration, these keys may be omitted from the validation dictionaries, unless a non-default value is desired. When run, this Checkpoint will perform two validations of two different batches of data, with each batch of data validated against a different Expectation Suite (\"my_expectation_suite\" and \"my_other_expectation_suite\", respectively). Each Validation will trigger the same set of actions and use the same Evaluation Parameters and runtime configuration. YAML: yaml name=\"tests/integration/docusaurus/reference/core_concepts/checkpoints_and_actions.py keys_passed_at_runtime just the yaml\" Runtime: python name=\"tests/integration/docusaurus/reference/core_concepts/checkpoints_and_actions.py run_checkpoint_3\" Results: python name=\"tests/integration/docusaurus/reference/core_concepts/checkpoints_and_actions.py validation_results_suites_data_assets_2\"   This configuration references the Checkpoint detailed in the previous example (\"Keys passed at runtime\"), allowing the runtime call to be much slimmer. YAML: yaml name=\"tests/integration/docusaurus/reference/core_concepts/checkpoints_and_actions.py using_template just the yaml\" Runtime: python name=\"tests/integration/docusaurus/reference/core_concepts/checkpoints_and_actions.py run_checkpoint_4\" Results: python name=\"tests/integration/docusaurus/reference/core_concepts/checkpoints_and_actions.py validation_results_suites_data_assets_3\"   This configuration specifies the SimpleCheckpoint class under the \"class_name\" key, allowing for a much slimmer configuration. YAML, using SimpleCheckpoint: yaml name=\"tests/integration/docusaurus/reference/core_concepts/checkpoints_and_actions.py using_simple_checkpoint just the yaml\" Equivalent YAML, using Checkpoint: yaml name=\"tests/integration/docusaurus/reference/core_concepts/checkpoints_and_actions.py equivalent_using_checkpoint just the yaml\" Runtime: python name=\"tests/integration/docusaurus/reference/core_concepts/checkpoints_and_actions.py run_checkpoint_6\" Results: python name=\"tests/integration/docusaurus/reference/core_concepts/checkpoints_and_actions.py assert_suite_2\"   SimpleCheckpoint class For many use cases, the SimpleCheckpoint class can be used to simplify the process of specifying a Checkpoint configuration. SimpleCheckpoint provides a basic set of actions - store Validation Result, store Evaluation Parameters, update Data Docs, and optionally, send a Slack notification - allowing you to omit an action_list from your configuration and at runtime. Configurations using the SimpleCheckpoint class can optionally specify four additional top-level keys that customize and extend the basic set of default actions:  site_names: a list of Data Docs site names to update as part of the update Data Docs action - defaults to \"all\" slack_webhook: if provided, an action will be added that sends a Slack notification to the provided webhook notify_on: used to define when a notification is fired, according to Validation Result outcome - all, failure, or success. Defaults to all. notify_with: a list of Data Docs site names for which to include a URL in any notifications - defaults to all  CheckpointResult The return object of a Checkpoint run is a CheckpointResult object. The run_results attribute forms the backbone of this type and defines the basic contract for what a Checkpoint's run method returns. It is a dictionary where the top-level keys are the ValidationResultIdentifiers of the Validation Results generated in the run. Each value is a dictionary having at minimum, a validation_result key containing an ExpectationSuiteValidationResult and an actions_results key containing a dictionary where the top-level keys are names of Actions performed after that particular Validation, with values containing any relevant outputs of that action (at minimum and in many cases, this would just be a dictionary with the Action's class_name). The run_results dictionary can contain other keys that are relevant for a specific Checkpoint implementation. For example, the run_results dictionary from a WarningAndFailureExpectationSuiteCheckpoint might have an extra key named \"expectation_suite_severity_level\" to indicate if the suite is at either a \"warning\" or \"failure\" level. CheckpointResult objects include many convenience methods (e.g. list_data_asset_names) that make working with Checkpoint results easier. You can learn more about these methods in the documentation for class: great_expectations.checkpoint.types.checkpoint_result.CheckpointResult. Below is an example of a CheckpointResult object which itself contains ValidationResult, ExpectationSuiteValidationResult, and CheckpointConfig objects. Example CheckpointResult python name=\"tests/integration/docusaurus/reference/core_concepts/checkpoints_and_actions.py results\" Example script To view the full script used in this page, see checkpoints_and_actions.py", "\"Checkpoint Store\"": " title: \"Checkpoint Store\" import TechnicalTag from '../term_tags/_tag.mdx'; A Checkpoint Store is a connector to store and retrieve information about means for validating data in a production deployment of Great Expectations. The Checkpoint Store manages storage and retrieval of Checkpoint configurations for the Data Context.  Checkpoint configurations can be added through the Data Context's add_checkpoint() method and retrieved with its get_checkpoint method. A configured Checkpoint Store is not required in order to work with Great Expectations, however a local configuration for a Checkpoint Store will be added automatically to great_expectations.yml when you store a Checkpoint configuration for the first time. When working with Great Expectations to Validate data you will not need to interact with a Checkpoint Store directly outside configuring the Store.  Instead, your Data Context will use the Checkpoint Store to store and retrieve Checkpoints behind the scenes, and the objects you will directly work with will be those Checkpoints. Relationship to other objects The Data Context will use your Checkpoint Store to store Checkpoint configurations and retrieve those configurations when initializing Checkpoints.  Typically, the Checkpoint Store will not need to interact with anything other than the Data Context and the configuration information being stored. Use cases When you save your first Checkpoint, a Checkpoint Store configuration will automatically be added to great_expectations.yml.  If you wish, you can adjust this configuration but in most cases the default will suffice.  Whenever you request an existing Checkpoint from your Data Context it will use the Checkpoint Store behind the scenes to retrieve that Checkpoint's configuration and initialize it.  Likewise, when you instruct your Data Context to store a newly created or edited Checkpoint it will use the Checkpoint Store behind the scenes to store that Checkpoint's configuration. Access Your Data Context will handle accessing your Checkpoint Store behind the scenes when you use it to store or retrieve a Checkpoint's configuration.  Rather than interacting with the Checkpoint Store itself, you will generally be interacting with your Data Context and a Checkpoint.", "CLI (Command Line Interface)": " title: CLI (Command Line Interface) Great Expectations has outgrown the CLI. With improvements to the Great Expectations API, you can now perform most operations with Python code and minimal setup. For help with the CLI, see an earlier version of the docunentation.", "\"Custom Expectation\"": " title: \"Custom Expectation\" import TechnicalTag from '../term_tags/_tag.mdx'; A Custom Expectation is an extension of the Expectation class, developed outside the Great Expectations library.  When you create a Custom Expectation, you can tailor it your specific needs. Custom Expectations are intended to allow you to create Expectations tailored to your specific data needs. Relationship to other objects Other than the development of Custom Expectations, which takes place outside the usual Great Expectations workflow for Validating data, Custom Expectations should interact with Great Expectations in the same way as any other Expectation would. Use cases For details on when and how you would use a Custom Expectation to Validate Data, see the corresponding documentation on Expectations. Access If you are using a Custom Expectation to validate data, you will typically access it exactly as you would any other Expectation.  However, if your Custom Expectation has not yet been contributed or merged into the Great Expectations codebase, you may want to set your Custom Expectation up to be accessed as a Plugin.  This will allow you to continue using your Custom Expectation while you wait for it to be accepted and merged. If you are still working on developing your Custom Expectation, you will access it by opening the python file that contains it in your preferred editing environment. Create We provide extensive documentation on how to create Custom Expectations.  If you are interested in doing so, we advise you reference our guides on how to create Custom Expectations. Contribute Community contributions are a great way to help Great Expectations grow!  If you've created a Custom Expectation that you would like to share with others, we have a guide on how to contribute a new Expectation to Great Expectations, just waiting for you!", "\"Data Asset\"": " title: \"Data Asset\" import TechnicalTag from '../term_tags/_tag.mdx'; A Data Asset is a collection of records within a Data Source which is usually named based on the underlying data system and sliced to correspond to a desired specification. Data Assets are used to specify how Great Expectations will organize data into Batches. Data Assets are usually tied to existing data that already has a name (e.g. \u201cthe UserEvents table\u201d). In many cases, Data Assets slice the data one step further (e.g. \u201cnew records for each day within the UserEvents table.\u201d) To further illustrate with some examples: in a SQL database, rows from a table grouped by the week they were delivered may be a data asset; in an S3 bucket or filesystem, files matching a particular regex pattern may be a data asset.  The specifics of a filesystem Data Asset are defined by the parameters provided when it is created. With a SQL Data Asset, you can also add splitters after you have initially created the Data Asset. You can define multiple Data Assets built from the same underlying source data system to support different workflows such as interactive exploration and creation of Expectations, the use of Profilers to analyze data, and ongoing Validation through Checkpoints. Great Expectations is designed to help you think and communicate clearly about your data. To do that, we need to rely on some specific ideas about what we're protecting with our Expectations. You usually do not need to think about these nuances to use Great Expectations, and many users never think about what exactly makes a Data Asset or Batch. But we think it can be extremely useful to understand the design decisions that guide Great Expectations. Great Expectations protects the quality of Data Assets. A Data Asset is a logical collection of records. Great Expectations consumes and creates metadata about Data Assets.  For example, a Data Asset might be a user table in a database, monthly financial data, a collection of event log   data, or anything else that your organization uses.  How do you know when a collection of records is one Data Asset instead of two Data Assets or when two collections of records are really part of the same Data Asset? In Great Expectations, we think the answer lies in the user. Great Expectations opens insights and enhances communication while protecting against pipeline risks and data risks, but that revolves around a purpose in using some data (even if that purpose starts out as \"I want to understand what I have here\"!). We recommend that you call a collection of records a Data Asset when you would like to track metadata (and especially, * Expectations) about it. A collection of records is a Data Asset when it's worth giving it a name.* Since the purpose is so important for understanding when a collection of records is a Data Asset, it immediately follows that Data Assets are not necessarily disjoint. The same data can be in multiple Data Assets. You may have different Expectations of the same raw data for different purposes or produce documentation tailored to specific analyses and users.  Similarly, it may be useful to describe subsets of a Data Asset as new Data Assets. For example, if we have a Data Asset   called the \"user table\" in our data warehouse, we might also have a different Data Asset called the \"Canadian User   Table\" that includes data only for some users.  Not all records in a Data Asset need to be available at the same time or place. A Data Asset could be built from * streaming data that is never stored, incremental deliveries, analytic queries, incremental updates, replacement deliveries, or from a one-time* snapshot. That implies that a Data Asset is a logical concept. Not all of the records may be accessible at the same time. That highlights a very important and subtle point: no matter where the data comes from originally, Great Expectations validates batches of data. A batch is a discrete subset of a Data Asset that can be identified by a some collection of parameters, like the date of delivery, value of a field, time of validation, or access control permissions. Relationship to other objects A Data Asset is a collection of records that you care about, which a Data Source is configured to interact with.  Batches are subsets of Data Asset records.  When a Batch Request is provided to a Data Source, the Data Source will reference its Data Asset in order to translate the Batch Request into a query for a specific Batch of data. For the most part, Data Assets are utilized by Great Expectations behind the scenes.  Other than configuring them, you will rarely have to interact with them directly (if at all). Use cases When connecting to your data you will define at least one Data Asset for each Data Source you create.  From these Data Assets you will be able to create Batch Requests, which will specify the data that you provide to your Expectations. When using the interactive workflow for creating Expectations, it is often useful to utilize a simple Data Asset configuration for purposes of exploring the data.  This configuration can then be replaced when it is time to Validate data going forward. When using a Profiler, Great Expectations can take advantage of as much information about your data as you can provide.  It may even be useful to configure multiple Data Assets to support complex Profiler configurations. When you are Validating new Batches of data you'll be using a Data Asset and Batch Request to indicate the Batch or Batches of data to Validate. Versatility Data Assets can be configured to cover a variety of use cases.  For example:  For append-only datasets such as logs, you configure a Data Asset to split data into batches corresponding to the intervals that you want the profiler to evaluate. For example, you could configure a log-style Data Asset to divide the data into Batches corresponding to single months. When you are working with dynamic datasets such as database tables, you will configure a Data Asset that defines a Batch as the state of the table at the time of Validation. Because you usually cannot access previous states of the such a table, only a single Batch from that Data Asset will be available at any given time. In that case, you have a few options: You can profile the data using a single Batch of data.  While you may not have as much statistical power in the estimated Expectations as if you had historical data available, this allows you to get started immediately, and you can build up a collection of Metrics over time that Great Expectations can use to refine the Expectations. You can configure another Data Asset from the same source Data that randomly assigns data to different batches to bootstrap the refinement of Expectations by providing a random sampling of records. You can configure another Data Asset from the same source data that splits the data into batches based on some keys in the data, such as load times or update times.   For working with streaming data, we recommend that you pull a sample of data into a permanent store and first profile your data using that data sample.  You will be able to configure a Data Asset for the data sample just as you would for an append-only dataset.  When it is time to Validate the data, you will use a second Data Asset that is configured to directly connect to your streaming data platform.  Flexibility Data Assets are logical constructs based around how you want to define and subdivide your data.  Generally speaking, a collection of data that you want to collect metadata about can be considered a Data Asset.  As a rule of thumb: a set of data is a Data Asset when it is worth giving a name.  Because of this, it is entirely possible to define multiple Data Assets to subdivide the same set of data in different ways, or even that exclude some of the source data. For instance, imagine you are doing analysis on sales data for cars.  You could create a Data Asset named \"recent_sales\" that only provides a batch for the most recent group of sold cars.  Or a Data Asset named \"monthly_reports\" that groups your source data system's records into Batches by month.  That same data could also be included in a Data Asset called \"yearly_reports\" that groups the data by year, and another Data Asset called \"make_and_model\" that groups records by those criteria, or a Data Asset called \"local_sales\" that groups records by dealership.  However it is that you want to analyze your data, you can configure a Data Asset to do so. Access You will not typically need to directly access a Data Asset.  Great Expectations validates Batches, and your Data Assets will be used behind the scenes in conjunction with Datasources and Batch Requests to provide those Batches.  However, you yourself will not need to work with the Data Asset beyond configuring it in a Data Source. Create You will not need to manually create a Data Asset.  Instead, they will be created from the configuration you provide for a Data Source when that Data Source needs them. Configure Data Assets are configured by providing parameters when they are created.  SQL-based Data Assets can be further configured after creation by calling their methods for adding splitters. To configure Data Assets for various environments and source data systems, see Connect to source data.", "Data Assistant": " id: data_assistant title: Data Assistant  import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; A Data Assistant is a pre-configured utility that simplifies the creation of Expectations. A Data Assistant can help you determine a starting point when working with a large, new, or complex dataset by asking questions and then building a list of relevant Metrics from the answers to those questions. Branching question paths based on your responses ensure that additional, relevant Metrics are not missed. The result is a comprehensive collection of Metrics that can be saved, reviewed as graphical plots, or used by the Data Assistant to generate a set of proposed Expectations. Data Assistants allow you to introspect multiple Batches and create an Expectation Suite from the aggregated Metrics of those Batches.  They provide convenient, visual representations of the generated Expectations to assist with identifying outliers in the corresponding parameters.  They can be accessed from your Data Context, and provide a good starting point for building Expectations or performing initial data exploration. Relationships to other objects A Data Assistant implements a pre-configured Profiler in order to gather Metrics and propose an Expectation Suite based on the introspection of the Batch or Batches contained in a provided Batch Request. Use cases Data Assistants are an ideal starting point for creating Expectations.  When you're working with unfamiliar data, a Data Assistant can provide an overview by introspecting the data and generating a series of relevant Expectations using estimated parameters for you to review. When you use the \"flag_outliers\" value for the estimation parameter, your generated Expectations have parameters that disregard values that the Data Assistant identifies as outliers. To create a graphical representation of the generated Expectations, use the Data Assistant's plot_metrics() method. Reviewing the Data Assistant's results can help you identify outliers in your data. When you're working with familiar, good data, a Data Assistant can use the \"exact\" value for the estimation parameter to provide comprehensive Expectations that reflect the values found in the provided data. Profiling To provide a representative analysis of the provided data, a Data Assistant can automatically process multiple Batches from a single Batch Request. Multi-Batch introspection Data Assistants leverage the ability to process multiple Batches from a single Batch Request to provide a representative analysis of the provided data.  With previous Profilers you would only be able to introspect a single Batch at a time.  This meant that the Expectation Suite generated would only reflect a single Batch.  If you had many Batches of data that you wanted to build inter-related Expectations for, you would have needed to run each Batch individually and then manually compare and update the Expectation parameters that were generated.  With a Data Assistant, that process is automated.  You can provide a Data Assistant multiple Batches and get back Expectations that have parameters based on, for instance, the mean or median value of a column on a per-Batch basis.  Visual plots for Metrics When working in a Jupyter Notebook you can use the plot_metrics() method of a Data Assistant's result object to generate a visual representation of your Expectations, the values that were assigned to their parameters, and the Metrics that informed those values.  This assists in exploratory data analysis and fine-tuning your Expectations, while providing complete transparency into the information used by the Data Assistant to build your Expectations. Data Assistants can be accessed from your Data Context. To select a Data Assistant in a Jupyter Notebook, enter context.assistants. and use code completion.  All Data Assistants have a run(...) method that takes in a Batch Request and numerous optional parameters, and then loads the results into an Expectation Suite for future use. To access the Onboarding Data Assistant, use context.assistants.onboarding. :::note For more information about the Onboarding Data Assistant, see How to create an Expectation Suite with the Onboarding Data Assistant. ::: Configure Data Assistants are pre-configured. You provide the Batch Request, and some optional parameters in the Data Assistant's run(...) method. Related documentation To learn more about working with the Onboarding Data Assistant, see How to create an Expectation Suite with the Onboarding Data Assistant.", "Data Context": " title: Data Context id: data_context hoverText: The primary entry point for a GX deployment, with configurations and methods for all supporting components.  import TechnicalTag from '../term_tags/_tag.mdx'; A Data Context is the primary entry point for a Great Expectations (GX) deployment, and it provides the configurations and methods for all supporting GX components. As the primary entry point for the GX API, the Data Context provides a convenient method for accessing common objects based on untyped input or common defaults. A Data Context also allows you to configure top-level components, and you can use different storage methodologies to back up your Data Context configuration. After you instantiate your DataContext and store its configurations, it always behaves the same way. Relationships to other objects Your Data Context provides you with the methods to configure your Stores, plugins, and Data Docs.  It also provides the methods needed to create, configure, and access your Data Sources, Expectations, Profilers, and Checkpoints.  In addition, a Data Context helps you manage your Metrics, Validation Results, and the contents of your Data Docs . Use Cases  When you configure your GX environment, you'll instantiate a Data Context. See Instantiate a Data Context. You can also use the Data Context to manage optional configurations for your Stores, Plugins, and Data Docs.  To configure Stores, see Configure your GX environment. To host and share Data Docs, see Host and share Data Docs. When you connect to data, you use your Data Context to create and configure Data Sources.  For more information on how to create and configure Data Sources, see Connect to source data. When creating Expectations, you'll use your Data Context to create Expectation Suites and Expectations, and then save them to an Expectations Store. The Data Context also provides a starting point for creating Custom Profilers, and manages the Metrics and Validation Results needed to run a Profiler automatically. The Data Context manages the content of your Data Docs (displaying such things as the Validation Results and Expectations generated by a Profiler).  For more information about creating Expectations, see Create Expectations.  When Validating data, the Data Context provides your entry point for creating, configuring, saving, and accessing Checkpoints.  For more information on using your Data Context to create a Checkpoint, see Validate Data.  Access to APIs The Data Context provides a primary entry point to the GX API.  Your Data Context provides a convenient method for accessing common objects.  While internal workflows of GX are strongly typed, the convenience methods available from the Data Context are exceptions, allowing access based on untyped input or common defaults. Configuration management A Data Context includes basic create, read, update, and delete (CRUD) operations for the core components of a GX deployment. This includes Datasources, Expectation Suites, and Checkpoints. In addition, a Data Context allows you to access and integrate Data Docs, Stores, Plugins, and so on. Component management and config storage The Data Context helps you create components such as Data Sources, Checkpoints, and Expectation Suites and manage where the information about those components is stored.   For production deployments, you will want to define these components according to your source data systems and production environment. This may include storing information about those components in something other than your local environment. To view implementation examples for specific environments and source data systems, see Integrations. GX Cloud compatability Because your Data Context contains the entirety of your GX project, GX Cloud can reference it to permit seamless upgrading from open source GX to GX Cloud. Instantiating a DataContext After you've created a Data Context, you'll likely start future work by instantiating a DataContext in Python. For example: python title=\"Import GX\" name=\"tests/integration/docusaurus/connecting_to_your_data/filesystem/pandas_yaml_example.py import gx\" python name=\"tests/integration/docusaurus/connecting_to_your_data/filesystem/pandas_yaml_example.py get_context\" Alternatively, you can use the context_root_dir parameter if you want to specify a specific directory If you\u2019re using GX Cloud, you set up the cloud environment variables before calling get_context. Untyped inputs The code standards for GX strive for strongly typed inputs.  However, the Data Context's convenience functions are a noted exception to this standard.  For example, to get a Batch with typed input, you run the following code: python name=\"tests/integration/docusaurus/connecting_to_your_data/filesystem/pandas_yaml_example.py import BatchRequest\" python name=\"tests/integration/docusaurus/connecting_to_your_data/filesystem/pandas_yaml_example.py context.get_batch with batch request\" If you prefer untyped inputs, you run code similar to the following examples: python name=\"tests/integration/docusaurus/connecting_to_your_data/filesystem/pandas_yaml_example.py context.get_batch with parameters data_asset_name\" python name=\"tests/integration/docusaurus/connecting_to_your_data/filesystem/pandas_yaml_example.py context.get_batch with parameters\" In the example code, the get_batch() method is responsible for inferring your intended types, and passing it through to the correct internal methods. This distinction between untyped and typed inputs reflects an important architectural decision within the GX codebase. Internal workflows are strongly typed, but  exceptions are allowed for a handful of convenience methods on the DataContext. Stronger type-checking allows the building of cleaner code, with stronger guarantees and a better understanding of error states. It also allows GX to take advantage of tools such as static type checkers, cyclometric complexity analysis, and so on. Requiring typed inputs can make getting started with GX challenging. For example, the first method shown in the previous code examples can be intimidating if you don't know what a BatchRequest is. It also requires you to know that a Batch Request is imported from great_expectations.core.batch. Allowing untyped inputs makes it possible to get started with GX much more quickly. However, there is always a risk is that untyped inputs will lead to confusion. To reduce or eliminate the risk, GX uses these guidleines:   Type inference is conservative. If inferring types require guessing, the method returns an error.   Informative errors are provided to help you determine an alternative input that does not require guessing to infer.  ", "Data Docs": " id: data_docs title: Data Docs  import TechnicalTag from '../term_tags/_tag.mdx'; Data Docs translate Expectations, Validation Results, and other metadata into human-readable documentation. Automatically compiling your data documentation from your data tests in the form of Data Docs keeps your documentation current. Relationship to other objects Data Docs can be used to view Expectation Suites and Validation Results.  With a customized Renderer, you can extend what they display and how.  You can issue a command to update your Data Docs from your Data Context.  Alternatively, you can include the UpdateDataDocsAction Action in a Checkpoint's action_list to trigger an update of your Data Docs with the Validation Results that were generated by that Checkpoint being run.  Use cases You can configure multiple Data Docs sites while setting up your Great Expectations project.  This allows you to tailor the information that is displayed by Data Docs as well as how they are hosted.  To host and share your Data Docs, see Host and share Data Docs. You can view your saved Expectation Suites in Data Docs.   Saved Validation Results will be displayed in any Data Docs site that is configured to show them.  If you build your Data Docs from the Data Context, the process will render Data Docs for all of your Validation Results.  Alternatively, you can use the UpdateDataDocsAction Action in a Checkpoint's action_list to update your Data Docs with just the Validation Results generated by that checkpoint. Versatility Multiple sites can be configured inside a project, each suitable for a particular data documentation use case.  Visualize all Great Expectations artifacts from the local repository of a project as HTML: Expectation Suites,    Validation Results and profiling results. Maintain a \"shared source of truth\" for a team working on a data project. Such documentation renders all the    artifacts committed in the source control system (Expectation Suites and profiling results) and a continuously    updating data quality report, built from a chronological list of validations by run id. Share a spec of a dataset with a client or a partner. This is similar to API documentation in software development.    This documentation would include profiling results of the dataset to give the reader a quick way to grasp what the    data looks like, and one or more Expectation Suites that encode what is expected from the data to be considered    valid.  Access Data Docs are rendered as HTML files.  As such, you can open them with any browser. Create If your Data Docs have not yet been rendered, you can create them from your Data Context. From the root folder of your project (where you initialized your Data Context), you can build your Data Docs with the CLI command: bash title=\"Terminal command\" great_expectations docs build Alternatively, you can use your Data Context to build your Data Docs in python with the command: python title=\"Python code\" import great_expectations as gx context = gx.get_context() context.build_data_docs() Configure Data Docs sites are configured under the data_docs_sites key in your deployment's great_expectations.yml file. Users can specify:  which Datasources to document (by default, all) whether to include Expectations, validations and profiling results sections where the Expectations and validations should be read from (filesystem, S3, Azure, or GCS) where the HTML files should be written (filesystem, S3, Azure, or GCS) which Renderer and view class should be used to render each section  For more information, see Host and share Data Docs.", "\"Data Docs Store\"": " title: \"Data Docs Store\" import TechnicalTag from '../term_tags/_tag.mdx'; A Data Docs Store is a connector to store and retrieve information pertaining to human-readable documentation generated from Great Expectations metadata detailing Expectations, Validation Results, etc. The Data Docs Store provides an easy way to configure where and how to have your Data Docs rendered. Relationship to other objects Your Data Docs Store will be used behind the scenes by any Action that updates your Data Docs.  Great Expectations includes the UpdateDataDocsAction subclass of the ValidationAction class for this express purpose.  Including the UpdateDataDocsAction in the action_list of a Checkpoint will cause your Data Docs to be updated accordingly. Use cases When you configure your Data Docs, one of the items you will need to include is a data_docs_store_backend for each entry in the data_docs_sites key located in the great_expectations.yml file.  This will provide Great Expectations with the necessary information to access the Data Docs being rendered for each given site.  That process will generally be handled behind the scenes; after configuring your Data Docs sites to include a Data Docs Store, you will generally not have to directly interact with the Data Docs Store yourself. When Profilers are run to create Expectations their results are made available through Data Docs.  Additionally, most Profiler workflows include a step where the Profiler's generated Expectation Suite is Validated against a Batch of data.  These processes use a Data Docs Store behind the scenes. When Checkpoints are run to Validate data, their results are generally updated into Data Docs.  Any Checkpoint that is configured to do so will use a Data Docs Store behind the scenes.  Including the UpdateDataDocsAction in the action_list of a Checkpoint will cause your Data Docs to be updated accordingly.  This update will only include the Validation Results from the Checkpoint itself and will not re-render all of your existing Data Docs.  If a Checkpoint does not include an UpdateDataDocsAction the Validation Results will not be rendered into Data Docs. Ease of use Once your Data Docs sites are configured with a Data Docs Store backend, the Great Expectations Objects that access Data Docs will handle the rest: you won't have to directly interact with the Data Docs Store. Access Your Data Context and other objects that interact with Data Docs will access your Data Docs Stores behind the scenes without you needing to directly manage them yourself beyond configuring your Data Docs sites during Setup.   In the Validate Data step, including UpdateDataDocsAction in the action_list of a Checkpoint will cause your Data Docs to be updated with the Checkpoint's Validation Results; this process will use your Data Docs Stores behind the scenes. :::note - To ensure that the Validation Results are included in the updated Data Docs, UpdateDataDocsAction should be present after StoreValidationResultAction in the Checkpoint's action_list. ::: You can also render your Data Docs outside a Checkpoint by utilizing the build_data_docs() method of your Data Context.  This will re-render all of your Data Docs according to the contents of your Data Docs Store. Configure Unlike other Stores, Data Docs stores are configured in the store_backend section under data_doc_sites in the great_expectations.yml config file.  For detailed information on how to configure a Data Docs Store for a given backend, please see the corresponding how-to guide:  How to host and share Data Docs on a filesystem How to host and share Data Docs on Azure Blob Storage How to host and share Data Docs on GCS How to host and share Data Docs on Amazon S3 ", "Data Source": " title: Data Source id: datasource hoverText: Provides a standard API for accessing and interacting with data from a wide variety of source systems.  import TechnicalTag from '../term_tags/_tag.mdx'; A Data Source provides a standard API for accessing and interacting with data from a wide variety of source systems. Datasources provide a standard API across multiple backends: the Data Source API remains the same for PostgreSQL, CSV Filesystems, and all other supported data backends. :::note Important:  Datasources do not modify your data. ::: Relationship to other objects Datasources function by bringing together a way of interacting with Data (an Execution Engine) with a definition of the data to access (a Data Asset).  Batch Requests utilize a Datasources' Data Assets to return a Batch of data. Use Cases When connecting to data the Data Source is your primary tool. At this stage, you will create Datasources to define how Great Expectations can find and access your Data Assets.  Under the hood, each Data Source uses an Execution Engine (ex: SQLAlchemy, Pandas, and Spark) to connect to and query data. Once a Data Source is configured you will be able to operate with the Data Source's API rather than needing a different API for each possible data backend you may be working with. When creating Expectations you will use your Datasources to obtain Batches for Profilers to analyze.  Datasources also provide Batches for  Expectation Suites, such as when you use the interactive workflow to create new Expectations. Datasources are also used to obtain Batches for Validators to run against when you are validating data. Standard API Datasources support connecting to a variety of different data backends. No matter which source data system you employ, the Data Source's API will remain the same. No unexpected modifications Datasources do not modify your data during profiling or validation, but they may create temporary artifacts to optimize computing Metrics and Validation (this behavior can be configured). Create and access Datasources can be created and accessed using Python code, which can be executed from a script, a Python console, or a Jupyter Notebook. To access a Data Source all you need is a Data Context and the name of the Data Source. The below snippet shows how to create a Pandas Data Source for local files: python name=\"tests/integration/docusaurus/connecting_to_your_data/connect_to_your_data_overview add_datasource\" This next snippet shows how to retrieve the Data Source from the Data Context. python name=\"tests/integration/docusaurus/connecting_to_your_data/connect_to_your_data_overview config\" For detailed instructions on how to create Datasources that are configured for various backends, see our documentation on Connecting to Data.", "Evaluation Parameter": " id: evaluation_parameter title: Evaluation Parameter hoverText: A dynamic value used during Validation of an Expectation which is populated by evaluating simple expressions or by referencing previously generated metrics.  import TechnicalTag from '../term_tags/_tag.mdx'; An Evaluation Parameter is a dynamic value used during Validation of an Expectation which is populated by evaluating simple expressions or by referencing previously generated Metrics. You can use Evaluation Parameters to configure Expectations to use dynamic values, such as a value from a previous step in a pipeline or a date relative to today.  Evaluation Parameters can be simple expressions such as math expressions or the current date, or reference Metrics generated from a previous Validation run.  During interactive development, you can even provide a temporary value that should be used during the initial evaluation of the Expectation. Relationship to other objects Evaluation Parameters are used in Expectations when Validating data.  Checkpoints use Actions to store Evaluation Parameters in the Evaluation Parameter Store. Use cases When creating Expectations based on introspection of Data, it can be useful to reference the results of a previous Expectation Suite's Validation.  To do this, you would use an URN directing to an Evaluation Parameter store.  An example of this might look something like the following: python title=\"Python code\" eval_param_urn = 'urn:great_expectations:validations:my_expectation_suite_1:expect_table_row_count_to_be_between.result.observed_value' validator.expect_table_row_count_to_equal(     value={         '$PARAMETER': eval_param_urn, # this is the actual parameter we're going to use in the validation     } ) The core of this is a $PARAMETER : URN pair. When Great Expectations encounters a $PARAMETER flag during validation, it will replace the URN with a value retrieved from an Evaluation Parameter Store or Metrics Store. If you do not have a previous Expectation Suite's Validation Results to reference, however, you can instead provide Evaluation Parameters with a temporary initial value. For example, the interactive method of creating Expectations is based on Validating Expectations against a previous run of the same Expectation Suite.  Since a previous run has not been performed when Expectations are being created, Evaluation Parameters cannot reference a past Validation and will require a temporary value instead.  This will allow you to test Expectations that are meant to rely on values from previous Validation runs before you have actually used them to Validate data.   Say you are creating additional Expectations for the data that you used in the Quickstart guide.  You want to create an expression that asserts that the row count for each Validation remains the same as the previous upstream_row_count, but since there is no previous upstream_row_count you need to provide a value that matches what the Expectation you are creating will find. To do so, you would first edit your existing (or create a new) Expectation Suite using a Validator object, as shown in our guide on How to create Expectations interactively in Python. The Expectation you will want to add to solve the above problem is the expect_table_row_count_to_equal Expectation, and this Expectation uses an evaluation parameter: upstream_row_count.  Therefore, when using the validator to add the expect_table_row_count_to_equal Expectation you will have to define the parameter in question (upstream_row_count) by assigning it to the $PARAMETER value in a dictionary.  Then, you would provide the temporary value for that parameter by setting it as the value of the $PARAMETER.<parameter_in_question> key in the same dictionary.  Or, in this case, the $PARAMETER.upstream_row_count. For an example of this, see below: python title=\"Python code\" validator.expect_table_row_count_to_equal(     value={\"$PARAMETER\": \"upstream_row_count\", \"$PARAMETER.upstream_row_count\": 10000},     result_format={'result_format': 'BOOLEAN_ONLY'} ) This returns {'success': True}. An alternative method of defining the temporary value for an Evaluation Parameter is the set_evaluation_parameter() method, as shown below: ```python title=\"Python code\" validator.set_evaluation_parameter(\"upstream_row_count\", 10000) validator.expect_table_row_count_to_equal(     value={\"$PARAMETER\": \"upstream_row_count\"},     result_format={'result_format': 'BOOLEAN_ONLY'} ) ``` This returns {'success': True}. Additionally, if the Evaluation Parameter's value is set in this way, you do not need to set it again (or define it alongside the use of the $PARAMETER key) for future Expectations that you create with this Validator. It is also possible for advanced users to create Expectations using Evaluation Parameters by turning off interactive evaluation and adding the Expectation configuration directly to the Expectation Suite.  For more information on this, see our guide on how to create and edit Expectations based on domain knowledge without inspecting data directly. More typically, when validating Expectations, you will provide Evaluation Parameters that are only available at runtime. Evaluation Parameters that are configured as part of a Checkpoint's Expectations will be used without further interaction from you.  Additionally, Evaluation Parameters will be stored by having the StoreEvaluationParametersAction subclass of the ValidationAction class defined in a Checkpoint configuration's action_list. However, if you want to provide specific values for Evaluation Parameters when running a Checkpoint (for instance, when you are testing a newly configured Checkpoint) you can do so by either defining the value of the Evaluation Parameter as an environment variable, or by passing the Evaluation Parameter value in as a dictionary assigned to the named parameter evaluation_parameters in the Data Context's run_checkpoint() method. For example, say you have a Checkpoint named my_checkpoint that is configured to use the Evaluation Parameter upstream_row_count.  To associate this Evaluation Parameter with an environment variable, you would edit the Checkpoint's configuration like this: yaml title=\"YAML configuration\" name: my_checkpoint ... evaluation_parameters:     upstream_row_count: $MY_ENV_VAR If you would rather pass the value of the Environment Variable upstream_row_count in as a dictionary when the Checkpoint is run, you can do so like this: ```python title=\"Python code\" import great_expectations as gx test_row_count = 10000 context = gx.get_context() context.run_checkpoint(my_checkpoint, evaluation_parameters={\"upstream_row_count\":test_row_count}) ``` Dynamic values Evaluation Parameters are defined by expressions that are evaluated at run time and replaced with the corresponding values.  These expressions can include such things as: - Values from previous Validation runs, such as the number of rows in a previous Validation. - Values modified by basic arithmetic, such as a percentage of rows in a previous Validation. - Temporal values, such as \"now\" or \"timedelta.\" - Complex values, such as lists. :::note Although complex values like lists can be used as the value of an Evaluation Parameter, you cannot currently combine complex values with arithmetic expressions. ::: Create An Evaluation Parameter is defined when an Expectation is created.  The Evaluation Parameter at that point will be a reference, either indicating a Metric from the results of a previous Validation, or an expression which will be evaluated prior to a Validation being run on the Expectation Suite. The Evaluation Parameter references take the form of a dictionary with the $PARAMETER key.  The value for this key will be directions to the desired Metric or the Evaluation Parameter's expression.  In either case, it will be evaluated at run time and replaced with the value described by the reference dictionary's value.  If the reference is pointing to a previous Validation's Metrics, it will be in the form of a $PARAMETER: URN pair, rather than a $PARAMETER: expression pair. To store Evaluation Parameters, define a StoreEvaluationParametersAction subclass of the ValidationAction class in a Checkpoint configuration's action_list, and run that Checkpoint. It is also possible to dynamically load Evaluation Parameters from a database. Evaluation Parameter expressions Evaluation Parameters can include basic arithmetic and temporal expressions.  For example, we might want to specify that a new table's row count should be between 90 - 110 % of an upstream table's row count (or a count from a previous run). Evaluation parameters support basic arithmetic expressions to accomplish that goal: ```python title=\"Python code\" validator.set_evaluation_parameter(\"upstream_row_count\", 10000) validator.expect_table_row_count_to_be_between(     min_value={\"$PARAMETER\": \"trunc(upstream_row_count * 0.9)\"},     max_value={\"$PARAMETER\": \"trunc(upstream_row_count * 1.1)\"},      result_format={'result_format': 'BOOLEAN_ONLY'} ) `` This returns{'success': True}`. We can also use the temporal expressions \"now\" and \"timedelta\". This example states that we expect values for the \"load_date\" column to be within the last week. python title=\"Python code\" validator.expect_column_values_to_be_between(     column=\"load_date\",     min_value={\"$PARAMETER\": \"now() - timedelta(weeks=1)\"} ) Evaluation Parameters are not limited to simple values, for example you could include a list as a parameter value.  Going back to our taxi data, let's say that we know there are only two types of accepted payment: Cash or Credit Card, which are represented by a 1 or a 2 in the payment_type column.  We could verify that these are the only values present by using a list, as shown below: ```python title=\"Python code\" validator.set_evaluation_parameter(\"runtime_values\", [1,2]) validator.expect_column_values_to_be_in_set(     \"payment_type\",      value_set={\"$PARAMETER\": \"runtime_values\"} ) ``` This Expectation will fail (the NYC taxi data allows for four types of payments), and now we are aware that what we thought we knew about the payment_type column wasn't accurate, and that now we need to research what those other two payment types are! :::note - You cannot currently combine complex values with arithmetic expressions. ::: The expressions and the corresponding functions that you can use in Evaluation Parameters are listed in the following table: | Evaluation Parameter Expression    | Python Function Call                                      | |----------------------------------------|---------------------------------------------------------------| | sin                                  | math.sin                                                    |  | cos                                  | math.cos                                                    | | tan                                  | math.tan                                                    | | exp                                  | math.exp                                                    |  | abs                                  | abs                                                         | | trunc                                | lambda a: int(a)                                            | | round                                | round                                                       | | sgn                                  | lambda a: -1 if a < -_epsilon else 1 if a > _epsilon else 0 | | now                                  | datetime.datetime.now                                       | | datetime                             | datetime.datetime                                           | | timedelta                            | datetime.timedelta                                          |", "\"Evaluation Parameter Store (Metric Store)\"": " title: \"Evaluation Parameter Store (Metric Store)\" import TechnicalTag from '../term_tags/_tag.mdx'; An Evaluation Parameter Store is a connector to store and retrieve information about parameters used during Validation of an Expectation which reference simple expressions or previously generated metrics. Evaluation Parameter Stores provide a method of storing and retrieving the Metrics associated with Evaluation Parameters without also storing the additional information that accompanies Validation Results. Relationship to other objects Evaluation Parameter Stores are an alternative way to store Metrics without the accompanying Validation Results.  The process of storing information in an Evaluation Parameter Store is also typically executed by an Action in a Checkpoint's action_list.  Metrics stored in Evaluation Parameter Stores are also available as Evaluation Parameters when defining Expectations. Use cases When you initialize your Data Context for the first time, a configuration for an in-memory Evaluation Parameter Store will automatically be added to great_expectations.yml. You may change this configuration to work with different environments in the same fashion as you would configure any other store, by adding a store_backend key to the Evaluation Parameter Store's entry. When some Expectations are created they require you to define an Evaluation Parameter.  One of the ways this can be done is to define a '$PARAMETER': URN pair in the Expectation's value dictionary.  When Great Expectations encounters a $PARAMETER flag during Validation, it will replace the URN with a value retrieved from an Evaluation Parameter Store or Metric Store. If an Evaluation Parameter is used in an Expectation that is being Validated, and the Evaluation Parameter is configured to reference a value in your Evaluation Parameter Store, the Evaluation Parameter Store will be used behind the scenes to retrieve that value.  You will not need to directly interact with it. If a Checkpoint is run,and it contains the StoreEvaluationParametersAction Action in its action_list, then your Evaluation Parameter Store will be used to store the Evaluation Parameters that were generated by the Checkpoint as well as their values. Reference previous results The Evaluation Parameter Store permits you to store the Metrics generated by Evaluation Parameters when Expectations containing them are Validated.  It also allows you to retrieve these Evaluation Parameters and values at a later point.  This permits you to create Expectations that rely on comparing the data of your current Batch with the metrics generated from a previous Batch of data. Convenience Reading from and writing to the Evaluation Parameter store is handled behind the scenes.  All you need to do is provide the correct URN path to the parameter you want to retrieve, or include the StoreEvaluationParametersAction Action in the action_list of any Checkpoint that generates Metrics from Evaluation Parameters that you want to store. Access The Evaluation Parameter Store uses an URN schema for identifying dependencies between Expectation Suites.  A valid URN must begin with the string: urn:great_expectations.  Valid URNs for Evaluation Parameter Stores must have one of the following structures to be recognized by your Data Context: yaml title: \"Replace names in <> with the desired name.\" urn:great_expectations:validations:<expectation_suite_name>:<metric_name> urn:great_expectations:validations:<expectation_suite_name>:<metric_name>:<metric_kwargs_id>", "Execution Engine": " id: execution_engine title: Execution Engine hoverText: A system capable of processing data to compute Metrics  import TechnicalTag from '../term_tags/_tag.mdx'; An Execution Engine is a system capable of processing data to compute Metrics. An Execution Engine provides the computing resources that will be used to actually perform Validation. Great Expectations can take advantage of different Execution Engines, such as Pandas, Spark, or SqlAlchemy, and even translate the same Expectations to validate data using different engines. Data is always viewed through the lens of an Execution Engine in Great Expectations. When we obtain a Batch of data, that Batch contains metadata that wraps the native Data Object of the Execution Engine -- for example, a DataFrame in Pandas or Spark, or a table or query result in SQL. Relationship to other objects Execution Engines are components of Datasources.  They accept Batch Requests and deliver Batches.  The Execution Engine is an underlying component of the Data Source, and when you interact with the Data Source it will handle the Execution Engine for you. Use cases You define the Execution Engine that you want to use to process data to compute Metrics in the Data Source configuration.  After you define the Execution Engine, you don't need to interact with it because the Data Source it is configured for uses it automatically. If a Profiler is used to create Expectations, or if you use the interactive workflow for creating Expectations, an Execution Engine will be involved as part of the Data Source used to provide data from a source data system for introspection. When a Checkpoint Validates data, it uses a Data Source (and therefore an Execution Engine) to execute one or more Batch Requests and acquire the data that the Validation is run on. When creating Custom Expectations and Metrics, often Execution Engine-specific logic is required for that Expectation or Metric. See Custom Expectations for more information. Standardized data and Expectations Execution engines handle the interactions with the source data.  They also wrap data from those source data systems with metadata that allows Great Expectations to read it regardless of its native format. Additionally, Execution Engines enable the calculations of Metrics used by Expectations so that they can operate in a format appropriate to their associated source data system.  Because of this, the same Expectations can be used to validate data from different Datasources, even if those Datasources interact with source data systems so different in nature that they require different Execution Engines to access their data.  Deferred Metrics SqlAlchemyExecutionEngine and SparkDFExecutionEngine provide an additional feature that allows deferred resolution of Metrics, making it possible to bundle the request for several metrics into a single trip to the backend. Additional Execution Engines may also support this feature in the future. The resolve_metric_bundle() method of these engines computes values of a bundle of Metrics; this function is used internally by resolve_metrics() on Execution Engines that support bundled metrics. Access You will not need to directly access an Execution Engine. When you interact with a Data Source it will handle the Execution Engine's operation under the hood. Create You will not need to directly instantiate an Execution Engine.  Instead, they are automatically created as a component in a Data Source. If you are interested in using and accessing data with an Execution Engine that Great Expectations does not yet support, consider making your work a contribution to the Great Expectations open source GitHub project.  This is a considerable undertaking, so you may also wish to reach out to us on Slack as we will be happy to provide guidance and support. Execution Engine init arguments  name caching batch_spec_defaults batch_data_dict validator  Execution Engine Properties  loaded_batch_data active_batch_data_id  Execution Engine Methods  load_batch_data(batrch_id, batch_data) resolve_metrics: computes metric values get_compute_domain: gets the compute domain for a particular type of intermediate metric.  Configure Execution Engines are not configured directly, but determined based on the Data Source you choose.", "Expectation": " title: Expectation id: expectation hoverText: A verifiable assertion about data.  import TechnicalTag from '../term_tags/_tag.mdx'; An Expectation is a verifiable assertion about data. Expectations enhance communication about your data and improve quality for data applications. They help you take the implicit assumptions about your data and make them explicit. Using Expectations helps reduce trips to domain experts and avoids leaving insights about data on the \"cutting room floor.\" Great Expectations' built-in library includes more than 50 common Expectations, such as:  expect_column_values_to_not_be_null expect_column_values_to_match_regex expect_column_values_to_be_unique expect_column_values_to_match_strftime_format expect_table_row_count_to_be_between expect_column_median_to_be_between  For a full list of available Expectations, see the Expectation Gallery. Great Expectations is a framework for defining Expectations and running them against your data. Like assertions in traditional Python unit tests, Expectations provide a flexible, declarative language for describing expected behavior. Unlike traditional unit tests, Great Expectations applies Expectations to data instead of code. For example, you could define an Expectation that a column contains no null values, and Great Expectations would run that Expectation against your data, and report if a null value was found. Relationship to other objects Expectations are grouped into Expectation Suites, which are in turn stored and retrieved through an Expectation Store.  Profilers will analyze data in order to generate Expectations, and Checkpoints rely on Expectation Suites (and the Expectations contained therein) to Validate data. Use cases Expectations are obviously a fundamental component of the Create Expectations step in working with Great Expectations.  There are two points at which you will have direct interaction with them.  The first is when you are creating new Expectations.  The second is when you are editing them.  Expectations are not meant to be static: the recommended best practice is an iterative process where your Expectations are edited as your data, and your understanding of that data, change.  For further information on this process, please see our overview on the Create Expectations process, and our related how-to guides. When you create your Checkpoints, you will be able to configure them to use specific Expectation Suites.  Other than setting up this configuration (or arranging to pass Expectation Suites at runtime) you will not need to directly interact with the Expectations themselves.  Instead, when you run your Checkpoint it will handle the use of the Expectations in any of its Expectation Suites to Validate the data indicated in its Batch Request/s.  This will be done under the hood, with the Validation Results that are generated being passed along to the Checkpoint's (optional) Actions for further processing. Customization Expectations are especially useful when they capture critical aspects of data understanding that analysts and practitioners know based on its semantic meaning. It's common to want to extend Great Expectations with application or domain specific Expectations. For example: bash expect_column_text_to_be_in_english expect_column_value_to_be_valid_icd_code These Expectations aren't included in the default set, but could be very useful for specific applications. Fear not! Great Expectations is designed for customization and extensibility. Building custom Expectations is easy and allows your custom logic to become part of the validation, documentation, and even profiling workflows that make Great Expectations stand out. See the guides on creating custom Expectations for more information on building Expectations and updating Data Context configurations to automatically load Batches of data with custom Data Assets. Distributional Expectations Distributional Expectations rely on expected distributions or \"partition objects\", which are built from intervals for continuous data or categorical classes and their associated weights, in order to help identify when new datasets or samples may be different from expected.  Distributional Expectations represent specific Expectation types, such as expect_column_kl_divergence_to_be_less_than.  You should use Distributional Expectations in the same way as other Expectations: to help accelerate identification of risks and changes to a modeled system or disruptions to a complex upstream data feed. For more information, see reference guide on Distributional Expectations. Conditional Expectations Conditional Expectations are those that are intended to be applied not to an entire dataset, but to a particular subset of the data.  Additionally, Conditional Expectations include those where what one expects of one variable depends on the value of another.  An example of this would be an Expectation that a column not have a null value only if another column's value falls into a specific subset.  Conditional Expectations represent a facet of map Expectations, including such things as expect_column_values_to_be_in_set. Conditional Expectations are experimentally available for Pandas, Spark, and SQLAlchemy backends. For more information on these Expectations, please see our reference guide for Conditional Expectations. Limitations Unfortunately, not all Expectations are implemented for all source data systems. To view a list of available Expectations, see the Expectations Gallery. Likewise, Conditional Expectations are considered experimental and may exhibit unexpected behavior when utilized with Spark and SQLAlchemy backends.  You can reference our documentation on Conditional Expectations for more information. Access You may directly access Expectations as part of the interactive workflow for creating new Expectations.  For further details on this process, see guide on how to create and edit Expectations with instant feedback from a sample Batch of data.   Create Generating Expectations is one of the most important parts of using Great Expectations effectively, and there are a variety of methods for generating and encoding Expectations. When Expectations are encoded in the Great Expectations format, they become shareable and persistent sources of truth about how data was expected to behave-and how it actually did. There are several paths to generating Expectations:  Automated inspection of datasets. Currently, the profiler mechanism in Great Expectations produces Expectation Suites that can be used for validation. In some cases, the goal is profiling your data, and in other cases automated inspection can produce Expectations that will be used in validating future batches of data. Expertise. Rich experience from subject-matter experts, Analysts, and data owners is often a critical source of Expectations. Interviewing experts and encoding their tacit knowledge of common distributions, values, or failure conditions can be can excellent way to generate Expectations. Exploratory Analysis. Using Great Expectations in an exploratory analysis workflow (e.g. within Jupyter Notebooks)is an important way to develop experience with both raw and derived datasets and generate useful and testable Expectations about characteristics that may be important for the data's eventual purpose, whether reporting or feeding another downstream model or data system.  For more information on these methods, see overview guide for creating Expectations. Configure Most of the time you will not need to directly interact with an Expectation's configurations.  However, advanced users may have circumstances in which it is desirable to define Expectations based purely on domain knowledge, without comparing against underlying data.  To do this, you will need to directly write an Expectation's configuration.  For details on how to do this, please reference our guide on how to create and edit Expectations based on domain knowledge, without inspecting data directly. The other occasion when you may want to edit an Expectation's configuration is when you need to edit the result_format of a Custom Expectation  The result_format parameter may be either a string or a dictionary which specifies the fields to return in result.  For further details, please see our reference guide on the result_format parameter. Results All Expectations return a JSON-serializable dictionary when evaluated, which consists of four standard (though optional, depending on the type of Expectation in question) arguments.  These are: result_format, include_config, catch_exceptions, and meta.  For a more detailed explanation as to what each of these arguments consists of and which Expectations use them, please see our reference guide on standard arguments. Domain and Success Keys A domain makes it possible to address a specific set of data, such as a table, query result, column in a table or dataframe, or even a Metric computed on a previous Batch of data.  It does this by describing the locale of data to which a Metric or Expectation applies. A domain is defined by a set of key-value pairs. The domain keys are the keys that uniquely define the domain for an Expectation. They vary depending on the Expectation; for example, many Expectations apply to data in a single column, but others apply to data from multiple columns or to properties that do not apply to a column at all. An Expectation also defines success keys that specify the values of metrics that determine when the Expectation will succeed. For example, the expect_column_values_to_be_in_set Expectation relies on the batch_id, table, column, and row_condition domain keys to determine what data are described by a particular configuration, and the value_set and mostly success keys to evaluate whether the Expectation is actually met for that data. Note: The batch_id and table domain keys are often omitted when running a validation, because the Expectation is being applied to a single batch and table. However, they must be provided in cases where they could be ambiguous. Metrics use a similar concept: they also use the same kind of domain keys as Expectations, but instead of success keys, we call the keys that determine a Metric's value its value keys.", "\"Expectation Store\"": " title: \"Expectation Store\" import TechnicalTag from '../term_tags/_tag.mdx'; An Expectation Store is a connector to store and retrieve information about collections of verifiable assertions about data. Expectation Stores allow you to store and retrieve Expectation Suites.  These Stores can be accessed and configured through the Data Context, but entries are added to them when you save an Expectation Suite (typically through a convenience method available from your Data Context).  A configured Expectation Store is required in order to work with Great Expectations.  A local configuration for an Expectation Store will be added automatically to great_expectations.yml when you initialize your Data Context for the first time. Generally speaking, while working with Great Expectations to Validate data you will not need to interact with an Expectation Store directly outside configuring the Store.  Instead, your Data Context will use an Expectation Store to store and retrieve Expectation Suites behind the scenes, and the objects you will directly work with will be those suites. Relationship to other objects Expectation Stores can be used to store and retrieve Expectation Suites.  This means that Expectation Stores may also come into play when working with Checkpoints and Validators, which can take Expectation Suites as input (rather than having a set Expectation Suite pre-defined in their configuration). Use cases When you initialize your Data Context for the first time, a configuration for a local Expectation Store will automatically be added to great_expectations.yml. You may change this configuration to work with different environments.   For more information on configuring an Expectation Store for a specific environment, see Configure Expectation Stores.  Most workflows for creating Expectations automatically bundle them into an Expectation Suite.  When you save the resultant Expectation Suite, your Data Context will use an Expectation Store to do so.  Likewise, to edit an existing Expectation Suite, you will use your Data Context to retrieve it through an Expectation Store.  This process is available through a convenience method in the Data Context, so you will not have to directly instantiate and interact with the Expectation Store. When Validating Data it is possible to have Checkpoints configured to require an Expectation Suite as a parameter, or to have an Expectation Suite pre-defined in the Checkpoint's configuration.  In either case, your Expectation Store will be used behind the scenes to retrieve the Expectation Suite in question (unless you are providing an Expectation Suite that still exists in memory from the Create Expectations step). Access You will not typically need direct access to your Expectation Store.  Instead, your Data Context will use your Expectation Store behind the scenes when storing or retrieving Expectation Suites.  Most of your direct interaction with the Expectation Store will take place during Setup, if you have need to configure your Expectation Store beyond the default that is provided when you initialize your Data Context.  The rest of the time you will most likely use convenience methods in your Data Context to retrieve Expectation Suites, and the save_expectation_suite method of a Validator to save newly created Expectation Suites.  Both of these methods will make use of your Expectation Store behind the scenes. Configure For details on how to configure an Expectation Store, please reference the relevant how-to guide:  How to configure an Expectation store to use Amazon S3 How to configure an Expectation store to use Azure Blob Storage How to configure an Expectation store to use GCS How to configure an Expectation store to use a filesystem How to configure an Expectation store to use PostgreSQL ", "Expectation Suite": " id: expectation_suite title: Expectation Suite hoverText: A collection of verifiable assertions about data.  import TechnicalTag from '../term_tags/_tag.mdx'; An Expectation Suite is a collection of verifiable assertions about data. Expectation Suites combine multiple Expectations into an overall description of data. For example, you can group all the Expectations about a given table in a given database into an Expectation Suite and name it my_database.my_table. Expectation Suite names are customizable, and the only constraint is that it must be unique to a given project. Relationship to other objects Expectation Suites are stored in an Expectation Store.  They are generated interactively using a Validator or automatically using Profilers, and are used by Checkpoints to Validate data. Use cases The lifecycle of an Expectation Suite starts with creating it. Then it goes through an iterative loop of Review and Edit as the team's understanding of the data described by the suite evolves. Expectation Suites are largely managed automatically in the workflows for creating Expectations.  When the Expectations are created, an Expectation Suite is created to contain them.  In the Profiling workflow, this Expectation Suite will contain all the Expectations generated by the Profiler.  In the interactive workflow, an Expectation Suite will be configured to include Expectations as they are defined, but will not be saved to an Expectation Store until you issue the command for it to be. For more information on these processes, please see: - Our overview on the process of Creating Expectations - Our guide on how to create and edit Expectations with instant feedback from a sample Batch of data Expectation Suites are used during the Validation of data.  In this step, you will need to provide one or more Expectation Suites to a Checkpoint.  This can either be done by configuring the Checkpoint to use a preset list of one or more Expectation Suites, or by configuring the Checkpoint to accept a list of one or more Expectation Suites at runtime. Reusability Expectation Suites are primarily used by Checkpoints, which can accept a list of one or more Expectation Suite and Batch Request pairs.  Because they are stored independently of the Checkpoints that use them, the same Expectation Suite can be included in the list for multiple Checkpoints, provided the Expectation Suite contains a list of Expectations that describe the data that Checkpoint will Validate.  You can even use the same Expectation Suite multiple times within the same Checkpoint by pairing it with different Batch Requests. CRUD operations A Great Expectations Expectation Suite enables you to perform Create, Read, Update, and Delete (CRUD) operations on the Suite's Expectations without needing to re-run them.  Each of the Expectation Suite methods that support a Create, Read, Update, or Delete (CRUD) operation relies on two main parameters - expectation_configuration and match_type.  expectation_configuration - an ExpectationConfiguration object that is used to determine whether and where this Expectation already exists within the Suite. It can be a complete or a partial ExpectationConfiguration. match_type - a string with the value of domain, success, or runtime which determines the criteria used for matching: domain checks whether two Expectation Configurations apply to the same data. It results in the loosest match, and can use the least complete ExpectationConfiguration object. For example, for a column map Expectation, a domain match_type will check that the expectation_type matches, and that the column and any row_conditions that affect which rows are evaluated by the Expectation match. success criteria are more exacting - in addition to the domain kwargs, these include those kwargs used when evaluating the success of an Expectation, like mostly, max, or value_set. -runtime are the most specific - in addition to domain_kwargs and success_kwargs, these include kwargs used for runtime configuration. Currently, these include result_format, include_config, and catch_exceptions    Access You will rarely need to directly access an Expectation Suite.  If you do need to edit one, the simplest way is through the CLI.  To do so, run the command: markdown title=\"Terminal command\" great_expectations suite edit NAME_OF_YOUR_SUITE_HERE This will open a Jupyter Notebook where each Expectation in the Expectation Suite is loaded as an individual cell.  You can edit, remove, and add Expectations in this list.  Running the cells will create the Expectations in a new Expectation Suite, which you can then save over the old Expectation Suite or save under a new name.  The Expectation Suite and any changes made will not be stored until you give the command for it to be saved, however. In almost all other circumstances you will simply pass the name of any relevant Expectation Suites to an object such as a Checkpoint that will manage accessing and using it for you. Save Expectation Suites Each Expectation Suite is saved in an Expectation Store, as a JSON file in the great_expectations/expectations subdirectory of the Data Context. Best practice is for users to check these files into the version control each time they are updated, in the same way they treat their source files. This discipline allows data quality to be an integral part of versioned pipeline releases. You can save an Expectation Suite by using a Validator's save_expectation_suite() method.  This method will be included in the last cell of any Jupyter notebook launched from the CLI for the purpose of creating or editing Expectations.", "Metric": " id: metric title: Metric hoverText: A computed attribute of data such as the mean of a column.  import TechnicalTag from '../term_tags/_tag.mdx'; A Metric is a computed attribute of data such as the mean of a column. Metrics are values derived from one or more Batches that can be used to evaluate Expectations or to summarize the result of Validation. It can be helpful to think of a Metric as the answer to a question.  A Metric could be a statistic, such as the minimum value of the column, or a more complex object, such as a histogram. Metrics are a core part of Validating data. Relationship to other objects Metrics are generated as part of running Expectations against a Batch (and can be referenced as such). For example, if you have an Expectation that the mean of a column falls within a certain range, the mean of the column must first be computed to see if its value is as expected.  The generation of Metrics involves Execution Engine specific logic.  These Metrics can be included in Validation Results, based on the result_format configured for them.  In memory Validation Results can in turn be accessed by Actions, including the StoreValidationResultAction which will store them in the Validation Results Store.  Therefore, Metrics from previously run Expectation Suites can also be referenced by accessing stored Validation Results that contain them. Use cases Metrics are generated in accordance with the requirements of an Expectation when an Expectation is evaluated.  This includes Expectations that are evaluated as part of the interactive process for creating Expectations and the DataAssistant or Custom Profiler based process for creating Expectations. Past Metrics can also be accessed by some Expectations through Evaluation Parameters.  However, when you are creating Expectations there may not be past Metrics to provide.  In these cases, it is possible to define a temporary value that the Evaluation Parameter can use in place of the missing past Metric. Checkpoints Validate data by running the Expectations in one or more Expectation Suite.  In the process, Metrics will be generated.  These Metrics can be passed to the Actions in the Checkpoint's action_list as part of the Validation Results for the Expectations (depending on the Validation Result's result_format), and will be stored in a Metric Store if the StoreMetricsAction is one of the Actions in the Checkpoint's action_list. Metrics are core to the Validation of data When an Expectation should be evaluated, Great Expectations collects all the Metrics requested by the Expectation and provides them to the Expectation's validation logic. Most validation is done by comparing values from a column or columns to a Metric associated with the Expectation being evaluated. Past Metrics are available to other Expectations and Data Docs An Expectation can also expose Metrics, such as the observed value of a useful statistic via an Expectation Validation Result, where Data Docs -- or other Expectations -- can use them.  This is done through an Action (to which the Expectation's Validation Result has been passed) which will save them to a Metric Store.  The Action in question is the StoreMetricsAction.  You can view the implementation of this Action in our GitHub. Access Validation Results can expose Metrics that are defined by specific Expectations that have been validated, called \"Expectation Defined Metrics.\" To access those values, you address the Metric as a dot-delimited string that identifies the value, such as expect_column_values_to_be_unique.successor expect_column_values_to_be_between.result.unexpected_percent. These Metrics may be stored in a Metrics Store. A metric_kwargs_id is a string representation of the Metric Kwargs that can be used as a database key. For simple cases, it could be easily readable, such as column=Age, but when there are multiple keys and values or complex values, it will most likely be a md5 hash of key/value pairs. It can also be None in the case that there are no kwargs required to identify the Metric. The following examples demonstrate how Metrics are defined: python title=\"Python code\" res = validator.expect_column_values_to_be_in_set(     \"color\",     [\"red\", \"green\"] ) res.get_metric(     \"expect_column_values_to_be_in_set.result.missing_count\",     column=\"color\" ) See the How to configure a MetricsStore guide for more information. Create Metrics are produced using logic specific to the Execution Engine associated with the Data Source that provides the data for the Batch Request/s that the Metric is calculated for.  That logic that is defined in a MetricProvider. When a MetricProvider class is first encountered, Great Expectations will register the Metric and any methods that it defines as able to produce Metrics.  The registered metric will then be able to be used with validator.get_metric() or validator.get_metrics().  Configure Configuration of Metrics is applied when they are defined as part of an Expectation. Naming conventions Metrics can have any name. However, for the \"core\" Great Expectations Metrics, we use the following conventions:  For aggregate Metrics, such as the mean value of a column, we use the domain and name of the statistic, such as column.mean or column.max. For map Metrics, which produce values for individual records or rows, we define the domain using the prefix \"column_values\" and use several consistent suffixes to provide related Metrics. For example, for the Metric that defines whether specific column values fall into an expected set, several related Metrics are defined: column_values.in_set.unexpected_count provides the total number of unexpected values in the domain. column_values.in_set.unexpected_values provides a sample of unexpected_values; \"result_format\" is one of its   value_keys to determine how many values should be returned. column_values.in_set.unexpected_rows provides full rows for which the value in the domain column was unexpected column_values.in_set.unexpected_value_counts provides a count of how many times each unexpected value occurred   ", "Metric Store": " id: metric_store title: Metric Store hoverText: A connector to store and retrieve information about computed attributes of data, such as the mean of a column.  import TechnicalTag from '../term_tags/_tag.mdx'; A Metric Store is a connector to store and retrieve information about computed attributes of data, such as the mean of a column. The Metric Store is tailored for storing and retrieving Metrics.  Although it can be used within Great Expectations as a reference to populate Evaluation Parameters when an Expectation Suite is run, the fact that it doesn't include other information found in Validation Results means you can also use it to more easily examine trends in your Metrics over time.  To help with this, a Metric Store will track the run_id of a Validation and the Expectation Suite name in addition to the metric name and metric kwargs. For information on how to create and use a Metric Store, see How to configure a Metric Store. The Metric Store differs from an Evaluation Parameter Store in how it formats its data.  Information stored in a Metric Store is kept in a format that more easily converted into tables which can be used for reports or to analyze trends.  It can also be referenced as values for Evaluation Parameters.   Relationship to other objects A Metric Store can be referenced by an Expectation Suite to populate values for Evaluation Parameters used by Expectations within that suite.  Metric Stores are also used in Checkpoints to store Metrics that are included in the Validation Results which are passed to the Checkpoints' action_list. Use cases If you intend to use a Metric Store, you can configure it in your great_expectations.yml file when you configure other Stores.  A Metric Store is an optional addition to great_expectations.yml, and one will not be included by default when you first initialize your Data Context. For more information, please see our guide on how to configure a Metric Store. When creating Expectations, you can configure them to use your Metric Store to retrieve values for Evaluation Parameters. When a Checkpoint is run, it will use a Metric Store to populate values for Evaluation Parameters if the Expectation Suite being run is configured to do so.  The StoreMetricsAction Action can also be included in a Checkpoint's action_list.  This will cause the Checkpoint to use the Metric Store and store any Metrics that were included in the Validation Results passed to the action_list.", "Plugin": " id: plugin title: Plugin  import TechnicalTag from '../term_tags/_tag.mdx'; Plugins extend Great Expectations' components and/or functionality. Python files that are placed in the plugins directory in your project (which is created automatically when you initialize your Data Context) can be used to extend Great Expectations.  Modules added there can be referenced in configuration files or imported directly in Python interpreters, scripts, or Jupyter Notebooks.  If you contribute a feature to Great Expectations, implementing it as a Plugin will allow you to start using that feature even before it has been merged into the open source Great Expectations code base and included in a new release. Relationships to other objects Due to their nature as extensions of Great Expectations, it can be generally said that any given Plugin can interact with any other object in Great Expectations that it is written to interact with.  However, best practices are to not interact with any objects not needed to achieve the Plugin's purpose. Use cases Plugins can be relevant to any point in the process of working with Great Expectations, depending on what any given Plugin is meant to do or extend.  Developing a Plugin is a process that exists outside the standard four-step workflow for using Great Expectations.  However, you can generally expect to actually use a Plugin in the same step as whatever object it is extending would be used, and to configure a Plugin in the same step as you would configure whatever object is extended by the Plugin. Customization Plugins can be anything from entirely custom code to subclasses inheriting from existing Great Expectations classes.  This versatility allows you to extend and tailor Great Expectations to your specific needs.  The use of Plugins can also allow you to implement features that have been submitted to Great Expectations but not yet integrated into the code base.  For instance, if you contributed code for a new feature to Great Expectations, you could implement it in your production environment as a plugin even if it had not yet been merged into the official Great Expectations code base and released as a new version. Component specific functionality Because Plugins often extend the functionality of existing Greate Expectations components, it is impossible to classify all of their potential features in a few generic statements.  In general, best practices are to include thorough documentation if you are developing or contributing code for use as a Plugin.  If you are using code that was created by someone else, you will have to reference their documentation (and possibly their code itself) in order to determine the features of that specific Plugin. The API of any given Plugin is determined by the individual or team that created it.  That said, if the Plugin is extending an existing Great Expectations component, then best practices are for the Plugin's API to mirror that of the object it extends as closely as possible. Import Plugins Any Plugin dropped into the plugins folder can be imported with a standard Python import statement.  In some cases, this will be all you need to do in order to make use of the Plugin's functionality.  For example, a Custom Expectation Plugin could be imported and used the same as any other Expectation in the interactive process for creating Expectations.", "Profiler": " id: profiler title: Profiler hoverText: Generates Metrics and candidate Expectations from data.  import TechnicalTag from '../term_tags/_tag.mdx'; A Profiler generates Metrics and candidate Expectations from data. A Profiler creates a starting point for quickly generating Expectations. There are several Profilers included with Great Expectations; conceptually, each Profiler is a checklist of questions which will generate an Expectation Suite when asked of a Batch of data. Relationship to other objects A Profiler builds an Expectation Suite from one or more Data Assets. Many Profiler workflows will also include a step that Validates the data against the newly-generated Expectation Suite to return a Validation Result. Use cases Profilers come into use when it is time to configure Expectations for your project.  At this point in your workflow you can configure a new Profiler, or use an existing one to generate Expectations from a Batch of data. For details on how to configure a customized Profiler, see our guide on how to create a new expectation suite using a Profiler. Profiler types There are multiple types of Profilers built in to Great Expectations.  Below is a list with overviews of each one.  For more information, you can view their docstrings and source code in the great_expectations\\profile folder on our GitHub. Profiler The Custom Profiler allows you to directly configure a customized Profiler through a YAML configuration.  Profilers allow you to integrate organizational knowledge about your data into the profiling process. For example, a team might have a convention that all columns named \"id\" are primary keys, whereas all columns ending with the suffix \"_id\" are foreign keys. In that case, when the team using Great Expectations first encounters a new dataset that followed the convention, a Profiler could use that knowledge to add an expect_column_values_to_be_unique Expectation to the \"id\" column (but not, for example an \"address_id\" column). For details on how to configure a customized Profiler, see our guide on how to create a new expectation suite using a Profiler. Create It is unlikely that you will need to create a customized Profiler by extending an existing Profiler with a subclass.  Instead, you should work with a Profiler which can be fully configured in a YAML configuration file. Configuring a custom Profiler is covered in the following section.  See also How to create a new expectation suite using a Profiler, or the full source code for that guide on our GitHub as an example. Configure Profilers Profilers allow users to provide a highly configurable specification which is composed of Rules to use in order to build an Expectation Suite by profiling existing data. Imagine you have a table of Sales that comes in every month. You could profile last month's data, inspecting it in order to automatically create a number of expectations that you can use to validate next month's data.   A Rule in a Profiler could say something like \"Look at every column in my Sales table, and if that column is numeric, add an expect_column_values_to_be_between Expectation to my Expectation Suite, where the min_value for the Expectation is the minimum value for the column, and the max_value for the Expectation is the maximum value for the column.\" Each rule in a Profiler has three types of components:  DomainBuilders: A DomainBuilder will inspect some data that you provide to the Profiler, and compile a list of Domains for which you would like to build expectations ParameterBuilders: A ParameterBuilder will inspect some data that you provide to the Profiler, and compile a dictionary of Parameters that you can use when constructing your ExpectationConfigurations ExpectationConfigurationBuilders: An ExpectationConfigurationBuilder will take the Domains compiled by the DomainBuilder, and assemble ExpectationConfigurations using Parameters built by the ParameterBuilder  In the above example, imagine your table of Sales has twenty columns, of which five are numeric: * Your DomainBuilder would inspect all twenty columns, and then yield a list of the five numeric columns * You would specify two ParameterBuilders: one which gets the min of a column, and one which gets a max. Your Profiler would loop over the Domain (or column) list built by the DomainBuilder and use the two ParameterBuilders to get the min and max for each column. * Then the Profiler loops over Domains built by the DomainBuilder and uses the ExpectationConfigurationBuilders to add a expect_column_values_to_between column for each of these Domains, where the min_value and max_value are the values that we got in the ParameterBuilders. In addition to Rules, a Profiler enables you to specify Variables, which are global and can be used in any of the Rules. For instance, you may want to reference the same BatchRequest or the same tolerance in multiple Rules, and declaring these as Variables will enable you to do so.  Below is an example configuration based on this discussion: yaml title=\"YAML configuration\" variables:   my_last_month_sales_batch_request: # We will use this BatchRequest in our DomainBuilder and both of our ParameterBuilders so we can pinpoint the data to Profile     datasource_name: my_sales_datasource     data_connector_name: monthly_sales     data_asset_name: sales_data     data_connector_query:       index: -1   mostly_default: 0.95 # We can set a variable here that we can reference as the `mostly` value for our expectations below rules:   my_rule_for_numeric_columns: # This is the name of our Rule     domain_builder:       batch_request: $variables.my_last_month_sales_batch_request # We use the BatchRequest that we specified in Variables above using this $ syntax       class_name: SemanticTypeColumnDomainBuilder # We use this class of DomainBuilder so we can specify the numeric type below       semantic_types:         - numeric     parameter_builders:       - parameter_name: my_column_min         class_name: MetricParameterBuilder         batch_request: $variables.my_last_month_sales_batch_request         metric_name: column.min # This is the metric we want to get with this ParameterBuilder         metric_domain_kwargs: $domain.domain_kwargs # This tells us to use the same Domain that is gotten by the DomainBuilder. We could also put a different column name in here to get a metric for that column instead.       - parameter_name: my_column_max         class_name: MetricParameterBuilder         batch_request: $variables.my_last_month_sales_batch_request         metric_name: column.max         metric_domain_kwargs: $domain.domain_kwargs     expectation_configuration_builders:       - expectation_type: expect_column_values_to_be_between # This is the name of the expectation that we would like to add to our suite         class_name: DefaultExpectationConfigurationBuilder         column: $domain.domain_kwargs.column         min_value: $parameter.my_column_min.value # We can reference the Parameters created by our ParameterBuilders using the same $ notation that we use to get Variables         max_value: $parameter.my_column_max.value         mostly: $variables.mostly_default", "Renderer": " title: Renderer import TechnicalTag from '../term_tags/_tag.mdx'; A Renderer is a class for converting Expectations, Validation Results, etc. into Data Docs or other output such as email notifications or Slack messages. Every Expectation has its own Renderer(s) that allows that Expectation to be expressed in a human-readable format in Data Docs.  From an architectural/contributor standpoint, Expectations are rendered into an intermediate canonical format and then from there turned into docs and other output (think of it more of a rendering pipeline).  Likewise, Renderers can be used to send Slack messages and email alerts.  Renderers for these purposes are already available in Great Expectations, but you can create custom ones and use them by specifying them in the Action that sends the message after Validation. Relationship to other objects The most common use of Renderers is to render Expectations and Validation Results into output for Data Docs.  Renderers can also be used to send Slack messages and email alerts.  Great Expectations makes these Renderers available by default.  If you create a Custom Expectation, you may also want to create a custom Renderer for it. Use cases Creating Custom Expectations, including their custom Renderers, is a process that falls outside the Great Expectations Validation workflow. Great Expectations will use a Renderer behind the scenes if you build Data Docs to view your Expectations.  If you want to send Slack, email, or other alerts after a Checkpoint runs you will do so by specifying a Renderer in an Action in the Checkpoint's action_list.  Renderers will also be used behind the scenes if you update your Data Docs through a Checkpoint Action, or if you rebuild your Data Docs outside the Validation process. Access When looking for Renderers with Expectations, you will find them specified as part of the Python class defining the Expectation.  With alerts and messaging, however, you will specify the Renderer to use in an Action's configuration.  To do this, you will specify a module_name and class_name indicating the Renderer's Python class under the renderer key (found nested under the action key) in an Action entry in a Checkpoint's action_list. Configure If you need instructions on how to configure the renderer portion of an Action to send a message or alert, please see the relevant guide:  How to trigger email as a Validation Action How to trigger Slack notifications as a Validation Action How to trigger Opsgenie notifications as a Validation Action ", "Store": " title: Store id: store hoverText: A connector to store and retrieve information about metadata in Great Expectations.  import TechnicalTag from '../term_tags/_tag.mdx'; A Store is a connector to store and retrieve information about metadata in Great Expectations. Stores are available from your Data Context. Great Expectations supports a variety of Stores for different purposes, but the most common Stores are Expectation Stores, Validations Stores, Checkpoint Stores, Metric Stores, and Evaluation Parameter Stores.  Data Docs Stores can also be configured for Data Doc Sites.  Each of these Stores is tailored to a specific type of information.  Expectation Store: a connector to store and retrieve information about collections of verifiable assertions about data.  These are Stores for Expectation Suites. Validation Result Store: a connector to store and retrieve information about objects generated when data is Validated against an Expectation Suite. Checkpoint Store: a connector to store and retrieve information about means for validating data in a production deployment of Great Expectations. Evaluation Parameter Store: a connector to store and retrieve information about parameters used during Validation of an Expectation which reference simple expressions or previously generated Metrics. Data Docs Store: a connector to store and retrieve information pertaining to Human readable documentation generated from Great Expectations metadata detailing Expectations, Validation Results, etc. Metric Stores: a connector to store and retrieve information about computed attributes of data, such as the mean of a column.  These differ from the Evaluation Parameter Store in how they are formatted.  The data in a Metrics Store is intended to be used for generating reports and analyzing trends, rather than as Evaluation Parameter values.  Relationship to other objects Each type of Store is designed to interact with a specific subset of information, and thus interacts with a specific subset of objects in Great Expectations.  However, all Stores can be listed and modified through your Data Context.  For further information on how a given type of Store relates to other objects, please see the corresponding Store type's technical term page:  Expectation Store technical term page: Relationship to other objects Checkpoint Store technical term page: Relationship to other objects Validation Result Store technical term page: Relationship to other objects Evaluation Parameter Store (or Metric Store) technical term page: Relationship to other objects Data Docs Store technical term page: Relationship to other objects Metric Store technical term page: Relationship to other objects  Use cases All Stores are configured during the Setup step of working with Great Expectations.  For the most part, the default configurations will permit you to work with Great Expectations in a local environment.  You can, however, configure your Stores to be hosted elsewhere. When creating Expectations, you will use Expectation Stores to store your Expectation Suites.  You may also use Validation Result Stores or Evaluation Parameter Stores to configure some Expectations that require Evaluation Parameters as part of their definition, provided you have previously stored relevant Metrics in those Stores and then come back to create a new Expectation Suite that references them. When Validating data, you may store new Checkpoints (or retrieve existing ones) from your Checkpoint Store.  Checkpoints may also use Expectation Suites retrieved by an Expectation Store, and Expectations that are run by a Checkpoint may retrieve Metrics as input (Evaluation Parameters) from Validation Results in a Validation Results Store, or from values stored in an Evaluation Parameter Store.  Checkpoints may also write information about Validation Results into a Validation Results Store, or information about Metrics into an Evaluation Parameter Store (which is why they are also known as Metric Stores). Access The stores key in your great_expectations.yml file lists your available Stores. The data_docs_sites key in your great_expectations.yml file defines your Data Docs Store configurations. store_backend keys below data_docs_sites identify the Data Doc Store used by the Data Docs site. Run the following Python command to view your Stores: ```python import great_expectations as gx context = gx.get_context() context.stores ``` Data Docs Stores are the one exception to the above.  They are instead configured under the data_docs_sites key in the great_expectations.yml file.  Each entry under data_docs_sites will have a store_backend key.  The information under store_backend will correspond to the Data Doc Store used by that site. Data Doc Stores are not listed by the great_expectations store list command. Configure Your Stores configurations are located in your great_expectations.yml file below the stores key.  Three stores are required: an Expectation Store, a Validation Result Store, and an Evaluation Parameter Store must always have a valid entry in the great_expectations.yml file.  Additional Stores can be configured for uses such as Data Docs and Checkpoints (and a Checkpoint Store configuration entry will automatically be added if one does not exist when you attempt to save a Checkpoint for the first time.) Note that unlike other Stores, your Data Docs Stores are configured under each individual site under the data_docs_sites key in the great_expectations.yml file. Related documentation For more information about a specific type of Store, see the Store type's technical term definition page:  Expectation Store technical term page Checkpoint Store technical term page Validation Result Store technical term page Evaluation Parameter Store technical term page Data Docs Store technical term page Metric Store technical term page ", "Supporting Resources": " id: supporting_resource title: Supporting Resources hoverText: A resource external to the Great Expectations code base which Great Expectations utilizes.  import TechnicalTag from '../term_tags/_tag.mdx'; A Supporting Resource is a resource external to the Great Expectations code base which Great Expectations utilizes. Great Expectations requires a Python compute environment and access to data, either locally or through a database or distributed cluster. In addition, developing with Great Expectations relies heavily on tools in the Python engineering ecosystem: pip, virtual environments, and Jupyter Notebooks. We also assume some level of familiarity with Git and version control. See Related documentation section for tutorials for these tools. Use cases Supporting Resources are used throughout Great Expectations workflows.  Some are necessities for Great Expectations to operate.  Others provide convenience when performing certain tasks, and some are recommended for project security and stability.  Below you will find an outline of the major Supporting Resources, and a brief description of how and when they are used.  Source data systems: Without data to Validate, Great Expectations has no purpose.  That data can exist natively in a variety of source data systems, including filesystems, databases, and distributed clusters. Python: Great Expectations is built in Python.  The objects that you create and methods that you use while working with Great Expectations will be generally be Python objects, YAML files, or JSON files.  Great Expectations cannot run without a Python environment. pip: Pip is the standard package manager for Python.  You will use pip to install Great Expectations in your Python environment. Virtual Environments: Virtual environments are isolated environments for projects.  The recommended best practice for using Great Expectations is to install it into a Python Virtual Environment to ensure that your Great Expectations deployment is not impacted by any other projects you may be working on, and vice versa. Jupyter Notebooks: A Jupyter Notebook is a web-based interactive computing platform.  A Jupyter Notebook can combine live code, explanatory text, and visualizations into a single location.  Great Expectations' CLI frequently uses Jupyter Notebooks to provide you with code boilerplate alongside additional explanations and guidance to assist you in various tasks. Git and version control: Version control is an important part of any project.  It allows you to track changes in your files and easily rollback to earlier versions of them if needed.  Best practices for Great Expectations  is to maintain your Data Context under version control.  Related documentation If you need to know more about a Supporting Resource, it is best to reference that resource's documentation.  Below are links to tutorials that can get you started with these tools. pip  https://pip.pypa.io/en/stable/ https://www.datacamp.com/community/tutorials/pip-python-package-manager  Virtual Environments  https://virtualenv.pypa.io/en/latest/ https://python-guide-cn.readthedocs.io/en/latest/dev/virtualenvs.html https://www.dabapps.com/blog/introduction-to-pip-and-virtualenv-python/  Jupyter Notebooks and Jupyter Lab  https://jupyter.org/ https://jupyterlab.readthedocs.io/en/stable/ https://towardsdatascience.com/jupyter-lab-evolution-of-the-jupyter-notebook-5297cacde6b  Git  https://git-scm.com/ https://www.atlassian.com/git/tutorials ", "Validation": " title: Validation id: validation hoverText: The act of applying an Expectation Suite to a Batch.  Validation is the act of applying an Expectation Suite to a Batch.", "Validation Action": " title: Validation Action  id: validation_action  hoverText: A Python class with a run method that takes the result of validating a Batch against an Expectation Suite and does something with it  A Validation Action is a Python class with a run method that takes the result of validating a Batch against an Expectation Suite and does something with it.", "Validation Result": " id: validation_result title: Validation Result hoverText: An object generated when data is Validated against an Expectation or Expectation Suite.  import TechnicalTag from '../term_tags/_tag.mdx'; A Validation Result is an object generated when data is validated against an Expectation or Expectation Suite. Validation Results can be saved for every time that a Checkpoint Validates data, permitting you to maintain records of your data's adherence to Expectations and track trends in the quality of the validated data. This can help you determine if failed Expectations are due to outliers in new data, or a more systemic problem. Validation Results are also among the information passed to Actions when a Checkpoint is run.  This allows them to be used for any purpose that an Action can be created to handle. Examples of this might be sending a Slack or email notification if the Validation fails, only launching a secondary process if the Validation Results report that Validation passed, or updating your Data Docs with specific information pulled from the Validation Results. Relationship to other objects Validation Results are generated by a Checkpoint when a Validator runs an Expectation Suite against its paired Batch Request.  The Validation Result will be passed to the Checkpoint's action_list where it will be available to any Actions that may need it.  The StoreValidationResultAction subclass of ValidationAction will save Validation Results to the Validation Result Store if it is present in a Checkpoint's action_list. Use cases A Validation Result is generated when you use the interactive workflow to create Expectations and the newly created Expectation is run against the sample Batch of data you are working with.  These individual Validation Results will be created as ExpectationValidationResult instances. Validation Results are generated when you run Checkpoints to Validate data.  Because Checkpoints operate on Expectation Suites rather than individual Expectations, the Validation Results generated by Checkpoints will be created as ExpectationSuiteValidationResult instances. The difference between an ExpectationValidationResult instance and an ExpectationSuiteValidationResult instance is much like the difference between an Expectation and an Expectation Suite: One contains more of the other, along with some additional metadata. Specifically, an ExpectationSuiteValidationResult will contain additional information pertaining to the Expectation Suite that was Validated, and will contain the data of an ExpectationValidationResult for each Expectation in the Expectation Suite.  These ExpectationValidationResult instances will be in a list accessed by the results attribute of the ExpectationSuiteValidationResult instance. The Validation Results generated as ExpectationSuiteValidationResult instances can be saved in JSON format in a Validation Result Store, and rendered in Data Docs.  Both of these activities are handled by Actions in the Checkpoint's action_list.  In most cases, when you see \"Validation Results\" in Great Expectations, it will be in reference to the output generated as an ExpectationSuiteValidationResult when a Checkpoint is run. Access Inside of Actions, Validation Results are a parameter that is passed to the Action when it is run.  The other place where you can access your Validation Results is in your Validation Result Store.  In both of these cases, you will be working with an ExpectationSuiteValidationResult instance, or the serialization of such an instance into a JSON file. Outside of Actions and your Validation Result Store, you will generally only encounter Validation Results when validating a single expectation against some sample data in the interactive workflow for creating Expectations.  When an individual Validation Result is generated, it is generated as an ExpectationValidationResult instance. Create Validation Results are created automatically when data is Validated in Great Expectations.  You will not need to manually create them; they are a product of the Validation process. A single Validation Result is created when an Expectation is created with the interactive workflow and the newly created Expectation validates itself against the sample data that is used in that process.  This is generated as an ExpectationValidationResult object. A Validation Result is also created for each Expectation in an Expectation Suite when a Checkpoint is run.  These are generated as values in the results list of an ExpectationSuiteValidationResult object. Configure Expectation Validation results ExpectationValidationResult instances represent the Validation Result of a single Expectation.  If you are accessing the instance itself (rather than the serialized JSON of an instance found in your Validation Results Store) you will find that the instance provides information about the Expectation and the outcome of its Validation through the following attributes:  success: A true or false indicator of whether the Expectation passed. expectation_config: The config used by the Expectation, including such things as type of Expectation and key word arguments that were passed to the Expectation. result: The observed values generated when the Expectation was run. meta: Provides additional information about the Validation Result of some Expectations. exception_info: This is a dictionary with three keys.  raised_exception indicates if an exception was raised during the Validation.  exception_traceback contains the traceback of the raised exception, if an exception was raised. exception_message contains the message associated with the raised exception, if an exception was raised.  Configure Expectation Suite Validation results ExpectationSuiteValidationResult instances are generated when an Expectation Suite is validated.  These instances contain a list of Validation Results for individual Expectations in the Expectation Suite, as well as some additional information.  This information can be accessed through the following attributes:  success: A true or false indicator of whether all the Expectations in the Expectation Suite passed. evaluation_parameters: The Evaluation Parameters and their values at the time when the Expectation Suite was Validated. results: A list of ExpectationValidationResult results for each Expectation in the Expectation Suite. meta: Additional information about the Validation Results, such as the name of the Expectation Suite that was run, information about the Batch that was Validated, when the Validation took place, and what version of Great Expectations was used to run the Validation. statistics: Some statistics to summarize the results list, including things like the number of evaluated Expectations and the percentage of those Expectations that passed successfully.  The attributes described above for ExpectationValidationResult and ExpectationSuiteValidationResult also correspond to the keys that you will find in the serialized JSON that is created when Validation Results in the form of an ExpectationSuiteValidationResult instance are saved to the Validation Results Store.", "\"Validation Result Store\"": " title: \"Validation Result Store\" import TechnicalTag from '../term_tags/_tag.mdx'; A Validation Result Store is a connector to store and retrieve information about objects generated when data is Validated against an Expectation Suite. Validation Result Stores allow you to store and retrieve Validation Results that have been generated by Checkpoints.  These Stores can be accessed and configured through the Data Context, but entries are added to them automatically when a Checkpoint with the appropriate Action in its action_list completes running.  A configured Validation Result Store is required in order to work with Great Expectations.  A local configuration for a Validation Result Store will be added automatically to great_expectations.yml when you initialize your Data Context for the first time. Validation Result Stores ensure that Validation Results are accessible via the Data Context for review and rendering into Data Docs.  Generally speaking, while working with Great Expectations to Validate data you will not need to interact with a Validation Result Store directly outside configuring the Store or configuring Evaluation Parameters that reference Metrics which can be found in a stored Validation Result.  Instead, your Data Context will use your Validation Result Store behind the scenes when Validation Results are retrieved or stored. Relationship to other objects Validation Result Stores can be used to store and retrieve Validation Results and associated metadata.  The process of storing this information is typically executed by an Action in a Checkpoint's action_list.  Metrics found in the Validation Results stored in Validation Result Stores can also be accessed as Evaluation Parameters when defining Expectations. Use cases When you initialize your Data Context for the first time, a configuration for a local Validation Result Store will automatically be added to great_expectations.yml. You may change this configuration to work with different environments.   For more information on configuring a Validation Results Store for a specific environment, see in-depth guides on Validation Result Store configurations.  When interactively creating Expectations that rely on Evaluation Parameters, it is possible to use existing Validation Results that have stored Metrics to populate those parameters.  If these Validation Results are not available in memory, then the only way to access them is by retrieving them through a Validation Result Store (if they were stored previously). When validating Data with a Checkpoint, Validation Results will be stored by the StoreValidationResultAction if it exists in the Checkpoint's action_list.  This will use a Validation Result Store to store the Validation Result and associated metadata.  Some Expectations may also use Evaluation Parameters which are configured to pull Metrics from Validation Results that are available through the Validation Result Store. Access You will not typically need direct access to your Validation Result Store.  Instead, your Data Context will use your Validation Result Store behind the scenes when storing or retrieving Validation Results.  Most of your interaction with the Validation Results Store will take place during Setup, if you have need to configure your Validation Result Store beyond the default that is provided when you initialize your Data Context. Configure For details on how to configure a Validation Result Store, please reference the relevant how-to guide:  How to configure a Validation Result store in Amazon S3 How to configure a Validation Result store in Azure Blob Storage How to configure a Validation Result store in GCS How to configure a Validation Result store on a filesystem How to configure a Validation Result store to PostgreSQL ", "Validator": " title: Validator import TechnicalTag from '../term_tags/_tag.mdx'; A Validator is the object responsible for running an Expectation Suite against data. The Validator is the core functional component of Great Expectations. Validators don't require additional configuration.  Provide one with an Expectation Suite and a Batch Request, and it will function. Relationship to other objects Validators are responsible for running an Expectation Suite against a Batch Request.  Checkpoints, in particular, use them for this purpose.  However, you can also use your Data Context to get a Validator to use outside a Checkpoint.  Use cases When connecting to Data, it is often useful to verify that you have configured your Data Source correctly.  To verify a new Data Source, you can load data from it into a Validator using a Batch Request.  Fore examples of this workflow, see Connect to source data. When creating Expectations for an Expectation Suite, most workflows will have you use a Validator.  You can see this in our guide on how to create and edit Expectations with a DataAssistant or a Custom Profiler. Checkpoints utilize a Validator when running an Expectation Suite against a Batch Request.  This process is entirely handled for you by the Checkpoint; you will not need to create or configure the Validator in question.  Access Validators are not typically saved.  Instead, they are instantiated when needed.  If you need a Validator outside a Checkpoint (for example, to create Expectations interactively in a Jupyter Notebook) you will use one that is created for that purpose. Create You can create a Validator through the get_validator(...) command of a Data Context. Configure Creating a Validator with the get_validator(...) method will require you to provide an Expectation Suite and a Batch Request.  Other than these parameters, there is no configuration needed for Validators.", "Code style guide": " title: Code style guide :::info Note This style guide will be enforced for all incoming PRs. However, certain legacy areas within the repo do not yet fully adhere to the style guide. We welcome PRs to bring these areas up to code. ::: Code   Methods are almost always named using snake_case.   Methods that behave as operators (e.g. comparison or equality) are named using camelCase. These methods are rare and should be changed with great caution. Please reach out to us if you see the need for a change of this kind.   Experimental methods should log an experimental warning when called: \u201cWarning: some_method is experimental. Methods, APIs, and core behavior may change in the future.\u201d   Experimental classes should log an experimental warning when initialized: \u201cWarning: great_expectations.some_module.SomeClass is experimental. Methods, APIs, and core behavior may change in the future.\u201d   Docstrings are highly recommended. We use the Sphinx\u2019s Napoleon extension to build documentation from Google-style docstrings.  Docstrings on Expectation classes have additional requirements since their content is converted and included in the Expectation Gallery.    Tasks Common developer tasks such as linting, formatting, type-checking are defined in tasks.py and runnable via the invoke task runner library. To see the available task run invoke --list from the project root. ```console $ invoke --list Available tasks: fmt             Run code formatter.   hooks           Run and manage pre-commit hooks.   lint            Run code linter   sort            Sort module imports.   type-coverage   Check total type-hint coverage compared to develop.   upgrade         Run code syntax upgrades. ``` For detailed usage guide, invoke <TASK-NAME> --help ```console $ invoke fmt --help Usage: inv[oke] [--core-opts] fmt [--options] [other tasks here ...] Docstring:   Run code formatter. Options:   -c, --check                   Only checks for needed changes without writing back. Exit with error code if changes needed.   -e STRING, --exclude=STRING   Exclude files or directories   -p STRING, --path=STRING      Target path. (Default: .)   -s, --[no-]sort               Disable import sorting. Runs by default. ``` Linting Our CI system will check using black, and ruff. If you have already committed files but are seeing errors during the continuous integration tests, you can run tests manually: console black <PATH/TO/YOUR/CHANGES> ruff <PATH/TO/YOUR/CHANGES> --fix Type Checking Our CI system will perform static type-checking using mypy. contrib and other select great_expectations/ packages are excluded from type-checking. See the mypy section in pyproject.toml for more details. To verify your code will pass the CI type-checker, run invoke type-check --install-types. Or run mypy directly against the packages listed above. Expectations   Use unambiguous Expectation names, even if they\u2019re a bit longer, e.g. expect_columns_to_match_ordered_list instead of expect_columns_to_be.   Avoid abbreviations, e.g. column_index instead of column_idx.   Expectation names should be prefixed to reflect their base classes:   | Base class                   |  prefix                         | |------------------------------|---------------------------------| | Expectation                |  expect_...                   |  | BatchExpectation           |  expect_table_...             |  | ColumnMapExpectation       |  expect_column_values_...     |  | ColumnAggregateExpectation |  expect_column_...            |  | ColumnPairMapExpectation   |  expect_column_pair_values... |  | MultiColumnMapExpectation  |  expect_multicolumn_values... | ", "Use Great Expectations with Amazon Web Services using Athena": " title: Use Great Expectations with Amazon Web Services using Athena sidebar_label: \"AWS S3 and Athena\"  import Prerequisites from '@site/docs/components/_prerequisites.jsx' import PrereqPython from '@site/docs/components/prerequisites/_python_version.md' import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; import Congratulations from './components/_congratulations_aws_athena.md' import TechnicalTag from '@site/docs/term_tags/_tag.mdx';    import VerifyAwsInstalled from './components/_aws_cli_verify_installation.md'  import VerifyAwsCredentials from '@site/docs/guides/setup/configuring_metadata_stores/components/_verify_aws_credentials_are_configured_properly.mdx'   import VerifyPythonVersion from '@site/docs/guides/setup/installation/components_local/_check_python_version.mdx' import WhereToGetPython from './components/_python_where_to_get.md'  import CreateVirtualEnvironment from '@site/docs/guides/setup/installation/components_local/_create_an_venv_with_pip.mdx'  import GetLatestPip from '@site/docs/guides/setup/installation/components_local/_ensure_latest_pip.mdx'  import InstallBoto3WithPip from '@site/docs/guides/setup/configuring_metadata_stores/components/_install_boto3_with_pip.mdx'  import InstallGxWithPip from '@site/docs/guides/setup/installation/components_local/_install_ge_with_pip.mdx'  import VerifySuccessfulGxInstallation from '@site/docs/guides/setup/installation/components_local/_verify_ge_install_succeeded.mdx'    import IdentifyDataContextExpectationsStore from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_an_expectation_store_in_amazon_s3/_identify_your_data_context_expectations_store.mdx'  import AddS3ExpectationsStoreConfiguration from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_an_expectation_store_in_amazon_s3/_update_your_configuration_file_to_include_a_new_store_for_expectations_on_s.mdx'  import VerifyS3ExpectationsStoreExists from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_an_expectation_store_in_amazon_s3/_confirm_that_the_new_expectations_store_has_been_added_by_running_great_expectations_store_list.mdx'  import OptionalCopyExistingExpectationsToS3 from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_an_expectation_store_in_amazon_s3/_copy_existing_expectation_json_files_to_the_s_bucket_this_step_is_optional.mdx'  import OptionalVerifyCopiedExpectationsAreAccessible from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_an_expectation_store_in_amazon_s3/_confirm_list.mdx'   import IdentifyDataContextValidationResultsStore from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_a_validation_result_store_in_amazon_s3/_identify_your_data_context_validation_results_store.mdx'  import AddS3ValidationResultsStoreConfiguration from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_a_validation_result_store_in_amazon_s3/_update_your_configuration_file_to_include_a_new_store_for_validation_results_on_s.mdx'  import VerifyS3ValidationResultsStoreExists from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_a_validation_result_store_in_amazon_s3/_update_store_reference.mdx'  import OptionalCopyExistingValidationResultsToS3 from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_a_validation_result_store_in_amazon_s3/_copy_existing_validation_results_to_the_s_bucket_this_step_is_optional.mdx'   import CreateAnS3BucketForDataDocs from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_create_an_s3_bucket.mdx'  import ConfigureYourBucketPolicyToEnableAppropriateAccess from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_configure_your_bucket_policy_to_enable_appropriate_access.mdx'  import ApplyTheDataDocsAccessPolicy from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_apply_the_policy.mdx'  import AddANewS3SiteToTheDataDocsSitesSectionOfYourGreatExpectationsYml from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_add_a_new_s3_site_to_the_data_docs_sites_section_of_your_great_expectationsyml.mdx'  import TestThatYourConfigurationIsCorrectByBuildingTheSite from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_test_that_your_configuration_is_correct_by_building_the_site.mdx'  import AdditionalDataDocsNotes from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_additional_notes.mdx'   import HowToRunDatasourceCode from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_datasource_code_environment.md'  import InstantiateDataContext from '@site/docs/guides/connecting_to_your_data/cloud/s3/components_pandas/_instantiate_your_projects_datacontext.mdx'  import ConnectionStringAthena from '@site/docs/guides/connecting_to_your_data/database/components/_connection_string_athena.md'  import TestAthenaDatasource from '@site/docs/guides/connecting_to_your_data/database/components/_datasource_athena_test.md'   import CreateExpectationsInteractively from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_expectation_suite_add_expectations_with_validator.md'  import SaveTheExpectationSuite from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_expectation_suite_save.md'   import CheckpointCreateAndRun from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_checkpoint_create_and_run.md'  import CreateCheckpoint from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_checkpoint_create_tab_python.md'  import SaveCheckpoint from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_checkpoint_save.md'  import RunCheckpoint from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_checkpoint_run.md'  import BuildAndViewDataDocs from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_data_docs_build_and_view.md' Use the information provided here to learn how to use Great Expectations (GX) with AWS and cloud storage. You'll configure a local GX project to store Expectations, Validation Results, and Data Docs in Amazon S3 buckets. You'll also configure Great Expectations to access data stored in an Athena database. Prerequisites    The AWS CLI. To download and install the AWS CLI, see Installing or updating the latest version of the AWS CLI. AWS credentials. See Configuring the AWS CLI. Permissions to install the Python packages boto3 and great_expectations with pip. An S3 bucket and prefix to store Expectations and Validation Results.   Ensure that the AWS CLI is ready for use         Prepare a local installation of Great Expectations                      Create your Data Context It is assumed that there is an empty folder to initialize the Filesystem Data Context. For example: python title=\"Python code\" path_to_empty_folder = '/my_gx_project/' You provide the path for the empty folder in the GX library FileDataContext.create(...) method as a project_root_dir parameter. When you provide the path to the empty folder, the Filesystem Data Context is initialized in that location. For convenience, the FileDataContext.create(...) method instantiates and returns the initialized Data Context, which you can keep in a Python variable. For example: ```python title=\"Python code\" from great_expectations.data_context import FileDataContext context = FileDataContext.create(project_root_dir=path_to_empty_folder) ``` Configure your Expectations Store on Amazon S3                Configure your Validation Results Store on Amazon S3             Configure Data Docs for hosting and sharing from Amazon S3                  Optional settings  Connect to data           To configure a SQL Data Source, see Connect to SQL database source data.      Create Expectations    Validate data               ", "Use Great Expectations with Amazon Web Services using S3 and Pandas": " title: Use Great Expectations with Amazon Web Services using S3 and Pandas description: AWS S3 sidebar_label: \"AWS S3 and Pandas\" sidebar_custom_props: { icon: 'img/integrations/s3_icon.png' }  import Prerequisites from '@site/docs/components/_prerequisites.jsx' import PrereqPython from '@site/docs/components/prerequisites/_python_version.md' import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; import Congratulations from './components/_congratulations_aws_s3_pandas.md' import TechnicalTag from '@site/docs/term_tags/_tag.mdx';    import VerifyAwsInstalled from './components/_aws_cli_verify_installation.md'  import VerifyAwsCredentials from '@site/docs/guides/setup/configuring_metadata_stores/components/_verify_aws_credentials_are_configured_properly.mdx'   import VerifyPythonVersion from '@site/docs/guides/setup/installation/components_local/_check_python_version.mdx' import WhereToGetPython from './components/_python_where_to_get.md'  import CreateVirtualEnvironment from '@site/docs/guides/setup/installation/components_local/_create_an_venv_with_pip.mdx'  import GetLatestPip from '@site/docs/guides/setup/installation/components_local/_ensure_latest_pip.mdx'  import InstallBoto3WithPip from '@site/docs/guides/setup/configuring_metadata_stores/components/_install_boto3_with_pip.mdx'  import InstallGxWithPip from '@site/docs/guides/setup/installation/components_local/_install_ge_with_pip.mdx'  import VerifySuccessfulGxInstallation from '@site/docs/guides/setup/installation/components_local/_verify_ge_install_succeeded.mdx'  import CreateDataContextWithCreate from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_initialize_data_context_with_create.mdx'   import IdentifyDataContextExpectationsStore from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_an_expectation_store_in_amazon_s3/_identify_your_data_context_expectations_store.mdx'  import AddS3ExpectationsStoreConfiguration from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_an_expectation_store_in_amazon_s3/_update_your_configuration_file_to_include_a_new_store_for_expectations_on_s.mdx'  import OptionalCopyExistingExpectationsToS3 from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_an_expectation_store_in_amazon_s3/_copy_existing_expectation_json_files_to_the_s_bucket_this_step_is_optional.mdx'   import IdentifyDataContextValidationResultsStore from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_a_validation_result_store_in_amazon_s3/_identify_your_data_context_validation_results_store.mdx'  import AddS3ValidationResultsStoreConfiguration from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_a_validation_result_store_in_amazon_s3/_update_your_configuration_file_to_include_a_new_store_for_validation_results_on_s.mdx'  import OptionalCopyExistingValidationResultsToS3 from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_a_validation_result_store_in_amazon_s3/_copy_existing_validation_results_to_the_s_bucket_this_step_is_optional.mdx'   import CreateAnS3BucketForDataDocs from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_create_an_s3_bucket.mdx'  import ConfigureYourBucketPolicyToEnableAppropriateAccess from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_configure_your_bucket_policy_to_enable_appropriate_access.mdx'  import ApplyTheDataDocsAccessPolicy from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_apply_the_policy.mdx'  import AddANewS3SiteToTheDataDocsSitesSectionOfYourGreatExpectationsYml from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_add_a_new_s3_site_to_the_data_docs_sites_section_of_your_great_expectationsyml.mdx'  import TestThatYourConfigurationIsCorrectByBuildingTheSite from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_test_that_your_configuration_is_correct_by_building_the_site.mdx'  import AdditionalDataDocsNotes from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_additional_notes.mdx'   import CreateDataContextWithCreateAgain from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_initialize_data_context_with_create.mdx'  import ConfigureYourDatasource from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_configure_your_datasource.mdx'  import AddCSVAssetToS3Datasource from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_add_csv_asset_to_pandas_s3_datasource.mdx'  import TestS3Datasource from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_test_your_new_datasource.mdx'   import PrepareABatchRequestAndValidatorForCreatingExpectations from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_add_expectation_suite_and_validator_for_fluent_datasource.mdx'  import CreateExpectationsInteractively from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_expectation_suite_add_expectations_with_validator.md'  import SaveTheExpectationSuite from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_expectation_suite_save.md'   import CheckpointCreateAndRun from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_checkpoint_create_and_run.md'  import CreateCheckpoint from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_checkpoint_create.md'  import RunCheckpoint from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_checkpoint_run.md'  import BuildAndViewDataDocs from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_data_docs_build_and_view.md' Great Expectations can work within many frameworks.  In this guide you will be shown a workflow for using Great Expectations with AWS and cloud storage.  You will configure a local Great Expectations project to store Expectations, Validation Results, and Data Docs in Amazon S3 buckets.  You will further configure Great Expectations to use Pandas and access data stored in another Amazon S3 bucket. This guide will demonstrate each of the steps necessary to go from installing a new instance of Great Expectations to Validating your data for the first time and viewing your Validation Results as Data Docs. Prerequisites    The AWS CLI. To download and install the AWS CLI, see Installing or updating the latest version of the AWS CLI. AWS credentials. See Configuring the AWS CLI. Permissions to install the Python packages (boto3 and great_expectations) with pip. An S3 bucket and prefix to store Expectations and Validation Results.   Steps Part 1: Setup 1.1 Ensure that the AWS CLI is ready for use 1.1.1 Verify that the AWS CLI is installed  1.1.2 Verify that your AWS credentials are properly configured  1.2 Prepare a local installation of Great Expectations 1.2.1 Verify that your Python version meets requirements   1.2.2 Create a virtual environment for your Great Expectations project  1.2.3 Ensure you have the latest version of pip  1.2.4 Install boto3  1.2.5 Install Great Expectations  1.2.6 Verify that Great Expectations installed successfully  1.3 Create your Data Context  1.4 Configure your Expectations Store on Amazon S3 1.4.1 Identify your Data Context Expectations Store  1.4.2 Update your configuration file to include a new Store for Expectations on Amazon S3  1.4.3 (Optional) Copy existing Expectation JSON files to the Amazon S3 bucket  1.5 Configure your Validation Results Store on Amazon S3 1.5.1 Identify your Data Context's Validation Results Store  1.5.2 Update your configuration file to include a new Store for Validation Results on Amazon S3  1.5.3 (Optional) Copy existing Validation results to the Amazon S3 bucket  1.6 Configure Data Docs for hosting and sharing from Amazon S3 1.6.1 Create an Amazon S3 bucket for your Data Docs  1.6.2 Configure your bucket policy to enable appropriate access  1.6.3 Apply the access policy to your Data Docs' Amazon S3 bucket  1.6.4 Add a new Amazon S3 site to the data_docs_sites section of your great_expectations.yml  1.6.5 Test that your Data Docs configuration is correct by building the site  Additional notes on hosting Data Docs from an Amazon S3 bucket  Part 2: Connect to data 2.1 Instantiate your project's DataContext  If you have already instantiated your DataContext in a previous step, this step can be skipped. 2.2 Add Data Source to your DataContext  2.2 Add CSV Asset to your Data Source  2.3 Test your new Data Source  Part 3: Create Expectations 3.1: Prepare a Batch Request, empty Expectation Suite, and Validator  3.2: Use a Validator to add Expectations to the Expectation Suite  3.3: Save the Expectation Suite  Part 4: Validate Data 4.1: Create and run a Checkpoint  4.1.1 Create a Checkpoint  4.1.2 Run the Checkpoint  4.2: Build and view Data Docs  Congratulations! ", "Use Great Expectations with Amazon Web Services using Redshift": " title: Use Great Expectations with Amazon Web Services using Redshift sidebar_label: \"AWS S3 and Redshift\"  import Prerequisites from '@site/docs/components/_prerequisites.jsx' import PrereqPython from '@site/docs/components/prerequisites/_python_version.md' import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; import Congratulations from './components/_congratulations_aws_redshift.md' import TechnicalTag from '@site/docs/term_tags/_tag.mdx';    import VerifyAwsInstalled from './components/_aws_cli_verify_installation.md'  import VerifyAwsCredentials from '@site/docs/guides/setup/configuring_metadata_stores/components/_verify_aws_credentials_are_configured_properly.mdx'   import VerifyPythonVersion from '@site/docs/guides/setup/installation/components_local/_check_python_version.mdx' import WhereToGetPython from './components/_python_where_to_get.md'  import CreateVirtualEnvironment from '@site/docs/guides/setup/installation/components_local/_create_an_venv_with_pip.mdx'  import GetLatestPip from '@site/docs/guides/setup/installation/components_local/_ensure_latest_pip.mdx'  import InstallBoto3WithPip from '@site/docs/guides/setup/configuring_metadata_stores/components/_install_boto3_with_pip.mdx'  import InstallGxWithPip from '@site/docs/guides/setup/installation/components_local/_install_ge_with_pip.mdx'  import VerifySuccessfulGxInstallation from '@site/docs/guides/setup/installation/components_local/_verify_ge_install_succeeded.mdx'  import RedshiftDependencies from '@site/docs/guides/connecting_to_your_data/database/components/_redshift_dependencies.md'  import CreateDataContextWithCreate from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_initialize_data_context_with_create.mdx'   import IdentifyDataContextExpectationsStore from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_an_expectation_store_in_amazon_s3/_identify_your_data_context_expectations_store.mdx'  import AddS3ExpectationsStoreConfiguration from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_an_expectation_store_in_amazon_s3/_update_your_configuration_file_to_include_a_new_store_for_expectations_on_s.mdx'  import OptionalCopyExistingExpectationsToS3 from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_an_expectation_store_in_amazon_s3/_copy_existing_expectation_json_files_to_the_s_bucket_this_step_is_optional.mdx'  import OptionalVerifyCopiedExpectationsAreAccessible from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_an_expectation_store_in_amazon_s3/_confirm_list.mdx'   import IdentifyDataContextValidationResultsStore from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_a_validation_result_store_in_amazon_s3/_identify_your_data_context_validation_results_store.mdx'  import AddS3ValidationResultsStoreConfiguration from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_a_validation_result_store_in_amazon_s3/_update_your_configuration_file_to_include_a_new_store_for_validation_results_on_s.mdx'  import OptionalCopyExistingValidationResultsToS3 from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_a_validation_result_store_in_amazon_s3/_copy_existing_validation_results_to_the_s_bucket_this_step_is_optional.mdx'   import CreateAnS3BucketForDataDocs from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_create_an_s3_bucket.mdx'  import ConfigureYourBucketPolicyToEnableAppropriateAccess from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_configure_your_bucket_policy_to_enable_appropriate_access.mdx'  import ApplyTheDataDocsAccessPolicy from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_apply_the_policy.mdx'  import AddANewS3SiteToTheDataDocsSitesSectionOfYourGreatExpectationsYml from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_add_a_new_s3_site_to_the_data_docs_sites_section_of_your_great_expectationsyml.mdx'  import TestThatYourConfigurationIsCorrectByBuildingTheSite from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_test_that_your_configuration_is_correct_by_building_the_site.mdx'  import AdditionalDataDocsNotes from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_additional_notes.mdx'   import CreateDataContextWithCreateAgain from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_initialize_data_context_with_create.mdx'  import ConnectionStringRedshift from '@site/docs/guides/connecting_to_your_data/database/components/_redshift_credentials.md'  import ConfigureYourRedshiftDatasource from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_configure_your_redshift_datasource.mdx'  import ConnectToDataAssets from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_add_table_and_query_assets.mdx'  import TestRedshiftDatasource from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_test_your_new_redshift_datasource.mdx'   import PrepareABatchRequestAndValidatorForCreatingExpectations from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_add_expectation_suite_and_validator_for_fluent_datasource.mdx'  import CreateExpectationsInteractively from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_expectation_suite_add_expectations_with_validator.md'  import SaveTheExpectationSuite from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_expectation_suite_save.md'   import CheckpointCreateAndRun from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_checkpoint_create_and_run.md'  import CreateCheckpoint from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_checkpoint_create.md'  import RunCheckpoint from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_checkpoint_run.md'  import BuildAndViewDataDocs from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_data_docs_build_and_view.md' Great Expectations can work within many frameworks.  In this guide you will be shown a workflow for using Great Expectations with AWS and cloud storage.  You will configure a local Great Expectations project to store Expectations, Validation Results, and Data Docs in Amazon S3 buckets.  You will further configure Great Expectations to access data from a Redshift database. This guide will demonstrate each of the steps necessary to go from installing a new instance of Great Expectations to Validating your data for the first time and viewing your Validation Results as Data Docs. Prerequisites    The AWS CLI. To download and install the AWS CLI, see Installing or updating the latest version of the AWS CLI. AWS credentials. See Configuring the AWS CLI. Permissions to install the Python packages (boto3 and great_expectations) with pip. An S3 bucket and prefix to store Expectations and Validation Results.   Steps Part 1: Setup 1.1 Ensure that the AWS CLI is ready for use 1.1.1 Verify that the AWS CLI is installed  1.1.2 Verify that your AWS credentials are properly configured  1.2 Prepare a local installation of Great Expectations 1.2.1 Verify that your Python version meets requirements   1.2.2 Create a virtual environment for your Great Expectations project  1.2.3 Ensure you have the latest version of pip  1.2.4 Install boto3  1.2.5 Install Great Expectations  1.2.6 Verify that Great Expectations installed successfully  1.2.7 Install additional dependencies for Redshift  1.3 Create your Data Context  1.4 Configure your Expectations Store on Amazon S3 1.4.1 Identify your Data Context Expectations Store  1.4.2 Update your configuration file to include a new Store for Expectations on Amazon S3  1.4.3 (Optional) Copy existing Expectation JSON files to the Amazon S3 bucket  1.4.4 (Optional) Verify that copied Expectations can be accessed from Amazon S3  1.5 Configure your Validation Results Store on Amazon S3 1.5.1 Identify your Data Context's Validation Results Store  1.5.2 Update your configuration file to include a new Store for Validation Results on Amazon S3  1.5.3 (Optional) Copy existing Validation results to the Amazon S3 bucket  1.6 Configure Data Docs for hosting and sharing from Amazon S3 1.6.1 Create an Amazon S3 bucket for your Data Docs  1.6.2 Configure your bucket policy to enable appropriate access  1.6.3 Apply the access policy to your Data Docs' Amazon S3 bucket  1.6.4 Add a new Amazon S3 site to the data_docs_sites section of your great_expectations.yml  1.6.5 Test that your Data Docs configuration is correct by building the site  Additional notes on hosting Data Docs from an Amazon S3 bucket  Part 2: Connect to data 2.1 Instantiate your project's DataContext  If you have already instantiated your DataContext in a previous step, this step can be skipped. 2.1.1 Determine your connection string  :::tip Is there a more secure way to store my credentials than plain text in a connection string? We recommend that database credentials be stored in the config_variables.yml file, which is located in the uncommitted/ folder by default, and is not part of source control. For additional options on configuring the config_variables.yml file or additional environment variables, please see our guide on how to configure credentials. ::: 2.2 Add Data Source to your DataContext  2.3. Connect to a specific set of data with a Data Asset  2.4 Test your new Data Source  Part 3: Create Expectations 3.1: Prepare a Batch Request, empty Expectation Suite, and Validator  3.2: Use a Validator to add Expectations to the Expectation Suite  3.3: Save the Expectation Suite  Part 4: Validate Data 4.1: Create and run a Checkpoint  4.1.1 Create a Checkpoint  4.1.2 Run the Checkpoint  4.2: Build and view Data Docs ", "Use Great Expectations with Amazon Web Services using S3 and Spark": " title: Use Great Expectations with Amazon Web Services using S3 and Spark sidebar_label: \"AWS S3 and Spark\"  import Prerequisites from '@site/docs/components/_prerequisites.jsx' import PrereqPython from '@site/docs/components/prerequisites/_python_version.md' import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; import Congratulations from './components/_congratulations_aws_s3_spark.md' import TechnicalTag from '@site/docs/term_tags/_tag.mdx';    import VerifyAwsInstalled from './components/_aws_cli_verify_installation.md'  import VerifyAwsCredentials from '@site/docs/guides/setup/configuring_metadata_stores/components/_verify_aws_credentials_are_configured_properly.mdx'   import VerifyPythonVersion from '@site/docs/guides/setup/installation/components_local/_check_python_version.mdx' import WhereToGetPython from './components/_python_where_to_get.md'  import CreateVirtualEnvironment from '@site/docs/guides/setup/installation/components_local/_create_an_venv_with_pip.mdx'  import GetLatestPip from '@site/docs/guides/setup/installation/components_local/_ensure_latest_pip.mdx'  import InstallBoto3WithPip from '@site/docs/guides/setup/configuring_metadata_stores/components/_install_boto3_with_pip.mdx'  import InstallSparkS3Dependencies from './components/_spark_s3_dependencies.md'  import InstallGxWithPip from '@site/docs/guides/setup/installation/components_local/_install_ge_with_pip.mdx'  import VerifySuccessfulGxInstallation from '@site/docs/guides/setup/installation/components_local/_verify_ge_install_succeeded.mdx'  import CreateDataContextWithCreate from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_initialize_data_context_with_create.mdx'   import IdentifyDataContextExpectationsStore from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_an_expectation_store_in_amazon_s3/_identify_your_data_context_expectations_store.mdx'  import AddS3ExpectationsStoreConfiguration from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_an_expectation_store_in_amazon_s3/_update_your_configuration_file_to_include_a_new_store_for_expectations_on_s.mdx'  import VerifyS3ExpectationsStoreExists from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_an_expectation_store_in_amazon_s3/_confirm_that_the_new_expectations_store_has_been_added_by_running_great_expectations_store_list.mdx'  import OptionalCopyExistingExpectationsToS3 from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_an_expectation_store_in_amazon_s3/_copy_existing_expectation_json_files_to_the_s_bucket_this_step_is_optional.mdx'  import OptionalVerifyCopiedExpectationsAreAccessible from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_an_expectation_store_in_amazon_s3/_confirm_list.mdx'   import IdentifyDataContextValidationResultsStore from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_a_validation_result_store_in_amazon_s3/_identify_your_data_context_validation_results_store.mdx'  import AddS3ValidationResultsStoreConfiguration from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_a_validation_result_store_in_amazon_s3/_update_your_configuration_file_to_include_a_new_store_for_validation_results_on_s.mdx'  import VerifyS3ValidationResultsStoreExists from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_a_validation_result_store_in_amazon_s3/_update_store_reference.mdx'  import OptionalCopyExistingValidationResultsToS3 from '@site/docs/guides/setup/configuring_metadata_stores/components_how_to_configure_a_validation_result_store_in_amazon_s3/_copy_existing_validation_results_to_the_s_bucket_this_step_is_optional.mdx'   import CreateAnS3BucketForDataDocs from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_create_an_s3_bucket.mdx'  import ConfigureYourBucketPolicyToEnableAppropriateAccess from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_configure_your_bucket_policy_to_enable_appropriate_access.mdx'  import ApplyTheDataDocsAccessPolicy from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_apply_the_policy.mdx'  import AddANewS3SiteToTheDataDocsSitesSectionOfYourGreatExpectationsYml from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_add_a_new_s3_site_to_the_data_docs_sites_section_of_your_great_expectationsyml.mdx'  import TestThatYourConfigurationIsCorrectByBuildingTheSite from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_test_that_your_configuration_is_correct_by_building_the_site.mdx'  import AdditionalDataDocsNotes from '@site/docs/guides/setup/configuring_data_docs/components_how_to_host_and_share_data_docs_on_amazon_s3/_additional_notes.mdx'   import CreateDataContextWithCreateAgain from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_initialize_data_context_with_create.mdx'  import ConfigureYourDatasource from '@site/docs/guides/connecting_to_your_data/cloud/s3/components_spark/_configure_your_datasource.md'  import AddCSVAssetToS3Datasource from '@site/docs/guides/connecting_to_your_data/cloud/s3/components_spark/_add_csv_asset_to_spark_s3_datasource.md'  import TestS3Datasource from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_test_your_new_datasource.mdx'   import PrepareABatchRequestAndValidatorForCreatingExpectations from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_add_expectation_suite_and_validator_for_fluent_datasource.mdx'  import CreateExpectationsInteractively from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_expectation_suite_add_expectations_with_validator.md'  import SaveTheExpectationSuite from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_expectation_suite_save.md'   import CheckpointCreateAndRun from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_checkpoint_create_and_run.md'  import CreateCheckpoint from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_checkpoint_create.md'  import RunCheckpoint from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_checkpoint_run.md'  import BuildAndViewDataDocs from '@site/docs/deployment_patterns/how_to_use_gx_with_aws/components/_data_docs_build_and_view.md' Great Expectations can work within many frameworks.  In this guide you will be shown a workflow for using Great Expectations with AWS and cloud storage.  You will configure a local Great Expectations project to store Expectations, Validation Results, and Data Docs in Amazon S3 buckets.  You will further configure Great Expectations to use Spark and access data stored in another Amazon S3 bucket. This guide will demonstrate each of the steps necessary to go from installing a new instance of Great Expectations to Validating your data for the first time and viewing your Validation Results as Data Docs. Prerequisites    The AWS CLI. To download and install the AWS CLI, see Installing or updating the latest version of the AWS CLI. AWS credentials. See Configuring the AWS CLI. Permissions to install the Python packages (boto3 and great_expectations) with pip. An S3 bucket and prefix to store Expectations and Validation Results.   Steps Part 1: Setup 1.1 Ensure that the AWS CLI is ready for use 1.1.1 Verify that the AWS CLI is installed  1.1.2 Verify that your AWS credentials are properly configured  1.2 Prepare a local installation of Great Expectations and necessary dependencies 1.2.1 Verify that your Python version meets requirements   1.2.2 Create a virtual environment for your Great Expectations project  1.2.3 Ensure you have the latest version of pip  1.2.4 Install boto3  1.2.5 Install Spark dependencies for S3  1.2.6 Install Great Expectations  1.2.7 Verify that Great Expectations installed successfully  1.3 Create your Data Context  1.4 Configure your Expectations Store on Amazon S3 1.4.1 Identify your Data Context Expectations Store  1.4.2 Update your configuration file to include a new Store for Expectations on Amazon S3  1.4.3 Verify that the new Amazon S3 Expectations Store has been added successfully  1.4.4 (Optional) Copy existing Expectation JSON files to the Amazon S3 bucket  1.4.5 (Optional) Verify that copied Expectations can be accessed from Amazon S3  1.5 Configure your Validation Results Store on Amazon S3 1.5.1 Identify your Data Context's Validation Results Store  1.5.2 Update your configuration file to include a new Store for Validation Results on Amazon S3  1.5.3 Verify that the new Amazon S3 Validation Results Store has been added successfully  1.5.4 (Optional) Copy existing Validation results to the Amazon S3 bucket  1.6 Configure Data Docs for hosting and sharing from Amazon S3 1.6.1 Create an Amazon S3 bucket for your Data Docs  1.6.2 Configure your bucket policy to enable appropriate access  1.6.3 Apply the access policy to your Data Docs' Amazon S3 bucket  1.6.4 Add a new Amazon S3 site to the data_docs_sites section of your great_expectations.yml  1.6.5 Test that your Data Docs configuration is correct by building the site  Additional notes on hosting Data Docs from an Amazon S3 bucket  Part 2: Connect to data 2.1 Instantiate your project's DataContext  If you have already instantiated your DataContext in a previous step, this step can be skipped. 2.2 Add Data Source to your DataContext  2.3 Add CSV Asset to your Data Source  2.3 Test your new Data Source  Part 3: Create Expectations 3.1: Prepare a Batch Request, empty Expectation Suite, and Validator  3.2: Use a Validator to add Expectations to the Expectation Suite  3.3: Save the Expectation Suite  Part 4: Validate Data 4.1: Create and run a Checkpoint  4.1.1 Create a Checkpoint  4.1.2 Run the Checkpoint  4.2: Build and view Data Docs  Congratulations! ", "'Connect to source data'": " sidebar_label: 'Connect to source data' title: 'Connect to source data' id: connect_to_data_lp description: Connect to source data stored on Amazon S3, Google Cloud Storage (GCS), Microsoft Azure Blob Storage, or local filesystems.  import LinkCardGrid from '/docs/components/LinkCardGrid'; import LinkCard from '/docs/components/LinkCard'; This is where you'll find information for connecting to source data stored on databases and local filesystems, how to request data from a Data Source, how to organize Batches in a file-based Data Asset, and how to connect Great Expectations (GX) to SQL tables and data returned by SQL database queries.      ", "How to choose between working with a single or multiple Batches of data": " title: How to choose between working with a single or multiple Batches of data import Prerequisites from '../connecting_to_your_data/components/prerequisites.jsx' import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; This guide will help you decide when you should use a single Batch and when it is beneficial to use multiple Batches, instead.  Building on that knowledge, this guide will also explain the principles behind configuring a Data Source's Data Asset to either only allow for a single Batch to be returned when requested, or to potentially include multiple Batches.  Finally, this guide will discuss how to configure Batch Requests to return a specific Batch or subset of Batches when you are requesting data from a Data Asset that has been configured to include multiple Batches, so that you can get the most versatility out of your Data Source configurations. By the end of this guide, you will know when it will be most beneficial to be working with a single Batch of data, when it will be most beneficial to be working with multiple Batches of data, and what you need to do in order to ensure your Data Source and Batch Request configurations permit returning multiple Batches or ensure the return of only one. Prerequisites   An understanding of Data Source basics An understanding of how to request data with a Batch Request   Steps The steps of this guide will allow you to quickly determine if you will want to use a single Batch or multiple Batches when creating Expectations and performing Validations.  This knowledge will then inform how you configure Data Assets in your Datasources. 1. Determine if a single Batch or multiple Batches of data will be most beneficial in your future use cases When to work with a single Batch of data: In some cases, a single Batch of data is all you need (or all that is supported).  In particular: - When you are Validating data, you will need to specify a single Batch of data.  If you provide multiple Batches in a Batch Request, Validations will default to operating on the last Batch in the list. - If you want to quickly create some Expectations as part of an early Expectation Suite or exploratory data analysis, a single Batch is likely to be all that you need. :::note  When Validating data you may include multiple Batch Requests in a single Checkpoint.  But only one Batch out of each Batch Request will be used in the Validation process. ::: When to work with multiple Batches of data: If you are creating Expectations, you will benefit from ensuring your Data Assets and Batch Requests can return multiple Batches when: - You will be using a Data Assistant to generate an Expectation Suite that is populated with Expectations for you. - You will be using auto-initializing Expectations to automatically generate the parameters for Expectations that you individually define and add to an Expectation Suite. This is particularly true if you are working with Expectations that involve statistical analysis on a per-Batch basis. For example, let's say you are working with the NYC taxi data and intended to define an Expectation to validate that the median trip distance of rides taken within a given month falls in a specific range. You would first need to ensure you could divide your data up into months for analysis, which you could do by configuring your Data Source to define an individual Data Asset for each month.  Then, you could analyze each month individually to determine what the appropriate minimum and maximum values for your Expectation would be.  But that would be a lot of work that Great Expectations can automate for you. Instead, you can configure your Data Source to define a single Data Asset that groups each month's worth of data into a discrete Batch.  Then, you can use a single Batch Request to retrieve that list of Batches. By feeding that Batch Request into the auto-initializing Expectation expect_column_median_to_be_between and setting the parameters (column=\"trip_distance\", auto=True) you can have Great Expectations analyze each Batch and create an Expectation that has appropriate minimum and maximum values for you, saving you a lot of repetitive work. Alternatively, if you know you will need additional Expectations, you can feed that Batch Request into a Data Assistant, which will generate an entire Expectation Suite with Expectations that have had their parameters determined from the analysis of the provided Batches... saving you from even more repetitive work. Huzzah for automation! 2. Determine if your Data Source should include Data Asset configurations that permit multiple Batches If you have determined that your future use case will benefit from being able to analyze multiple Batches of data at once, then you will necessarily want to ensure that your Data Source is capable of returning multiple Batches. Otherwise, it is sufficient for your Data Source to define Data Assets that are only capable of returning a single Data Asset. Getting more versatility out of a Data Asset that is configured to return multiple Batches: However, there is a caveat.  When you create a Batch Request, you may configure it to return a specific Batch from a Data Asset even if that Data Asset returns multiple Batches by default.  This feature means that if you have use cases that would benefit from working with multiple Batches of data and use cases that benefit from working with a single Batch of data you don't necessarily have to define two Data Assets in your Data Source configuration. Let's look at the NYC taxi data example from before.  In that example, we determined that working with multiple Batches would benefit us because we wanted to do a per-Batch statistical analysis when creating an Expectation.  However, at the end of the day we are going to want to Validate future data against that Expectation, and the Validation process (as you read above) only supports operating on a single Batch at a time. Does this mean you need to define two Data Assets, one that is configured to return one Batch of data for each month (for creating your Expectation) and one that is configured to only return the most recent month's data (for Validation purposes)? No! As long as the data you want to Validate corresponds to a single Batch in your Data Asset, you can use a Batch Request's ability to limit the returned Batches to specify a single Batch for Validation or any other use. When you must configure your Data Asset to only permit single Batch of data to be returned: There are only two cases where you must configure your Data Asset to correspond to at most a single Batch of data. The first of these is when you are working with a Runtime Data Connector. The Runtime Data Connector puts a wrapper around a single Batch of data, and therefore does not support Data Asset configurations that permit the return of more than one Batch of data. The second is when you are using a Sql Data Source and you want to work with a single Batch of data that would otherwise be split between Batches in a Data Asset that was configured to permit the return of more than one Batch.   The reason that this case only applies to Sql Datasources is due to the difference in how Batches are handled in File System Datasources and Sql Datasources. How do Sql Datasources and File System Datasources differ in how they handle Batches? Data Asset configurations that permit the return of more than one Batch of data function differently between File System Datasources and Sql Datasources.  This is due to differences in how files are handled by Pandas and PySpark versus how tables are handled by SqlAlchemy. In a File System Data Source: - A Data Asset may consist of one or more files. - A Batch of data corresponds to a single file. - You cannot split a single file into multiple Batches. - You cannot combine multiple files into a single Batch. In a Sql Data Source: - A Data Asset always consists of the data contained in a single Table. - A Batch of data corresponds to a discrete portion of the data in the table associated with a given Data Asset (including, potentially, all of the data in the table). - You cannot combine multiple tables as multiple Batches in a single Data Asset. - You cannot combine multiple tables into a single Batch. This means that in a File System Data Source, a Data Asset that permits the return of more than one Batch does so by combining multiple files and treating each file as a separate Batch in the Data Asset.  You can therefore always specify the return of a single file's data in a Batch Request by specifying the return of the Batch that corresponds to that file. However, in a Sql Data Source, a Data Asset that permits the return of more than one Batch does so by splitting the contents of a single table based on a provided criteria and having each Batch correspond to a discrete portion of the split up data.  Therefore, if you have use cases for both splitting a table into Batches and working with the entire table as a single Batch the best practice is to define two Data Assets for the table: one that includes the configuration to split the table into Batches, and one that does not. 3. Next Steps Congratulations!  At this point you should have a solid understanding of when to work with a single Batch of Data and when to work with multiple Batches of Data.  You should also know when you can use a single Data Asset that permits the return of multiple Batches to cover both workflows, and when you need to define and use a Data Asset that only corresponds to a single Batch.  Using this knowledge, your next step is to actually define the Datasources and Data Assets that you will be requesting Batches from in the future. Additional Information For more detailed information on how to use Batch Requests to return a specific Batch of data from a configured Data Source, please see: - How to get one or more Batches of data from a configured Data Source", "'Manage Data Assets'": " sidebar_label: 'Manage Data Assets' title: 'Manage Data Assets' id: manage_data_assets_lp description: Request data from a Data Source and organize Batches in file-based and SQL Data Assets.  import LinkCardGrid from '/docs/components/LinkCardGrid'; import LinkCard from '/docs/components/LinkCard'; This is where you'll find information for managing your Data Assets. A Data Asset is a collection of records within a Data Source that define how Great Expectations (GX) organizes data into Batches      ", "'Add features to Custom Expectations'": " sidebar_label: 'Add features to Custom Expectations' title: 'Add features to Custom Expectations' id: add_features_custom_expectations_lp description: Add additional functionality to your Custom Expectations.  import LinkCardGrid from '/docs/components/LinkCardGrid'; import LinkCard from '/docs/components/LinkCard'; Add additional functionality to your Custom Expectations.        ", "'Expectation creation workflow'": " sidebar_label: 'Expectation creation workflow' title: 'Expectation creation workflow' id: create_expectations_overview description: An overview of the process for creating and managing Expectations and Expectation Suites.  import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; The following image shows the four workflows you can follow to create Expectations:  The methodology for saving and testing Expectations is the same for all workflows. GX recommends using the interactive workflow or the Data Assistant workflow to create Expectations. Create Expectations interactively In this workflow, you work in a Python interpreter or Jupyter Notebook.  You use a Validator and call Expectations as methods on it to define them in an Expectation Suite, and when you have finished you save that Expectation Suite into your Expectation Store. For an overview of this workflow, see How to create and edit Expectations with instant feedback from a sample batch of data. Create Expectations with Data Assistants In this workflow, you use a Data Assistant to generate Expectations based on the input data you provide.  You can preview the Metrics that these Expectations are based on, and you can save the generated Expectations as an Expectation Suite in an Expectation Store.  As with creating Expectations interactively, you start with your Data Context.  However, you work in a Python environment, so you need to load or create your Data Context as an instantiated object.  Next, you create a Batch Request to specify the data you would like to Profile with your Data Assistant.  Once you have a Batch Request configured you will use it as the input for the run method of your Data Assistant, which can be accessed from your Data Context object.  Once the Data Assistant has run, you will be able to review the results and save the generated Expectations to an empty Expectation Suite. GX recommends using the Onboarding Data Assistant to create an Expectation Suite. See How to create an Expectation Suite with the Onboarding Data Assistant. After you've created the Expectation Suite, it's expected that you'll edit and update the Expectation Suite to better suit your specific use case. The Expectation Suite is not intended to be used without changes. Manually define Expectations Advanced users can use a manual process to create Expectations. This workflow does not require source data to work against, but it does require knowledge of the configurations available for Expectations. To create Expectations manually, see how to create and edit expectations based on domain knowledge without inspecting data directly. Create Expectations with custom methods Advanced users can use custom methods to generate Expectations that are based on source data system metadata. If you want to use custom methods to create Expectations, contact a member of the support team on Slack. Test your Expectation Suite After you've created and saved your Expectation Suite, GX recommends that you test it by Validating it. You can Validate an Expectation with SimpleCheckpoint.  An overview of the Validation process is provided here. Edit a saved Expectation Suite See How to Edit an Expectation Suite View Expectation Suite Expectations See View the Expectations in the Expectation Suite. Next steps  Validate Data ", "'Create and manage Expectations and Expectation Suites'": " sidebar_label: 'Create and manage Expectations and Expectation Suites' title: 'Create and manage Expectations and Expectation Suites' id: create_manage_expectations_lp description: Create and manage Expectations and Expectation Suites.  import LinkCardGrid from '/docs/components/LinkCardGrid'; import LinkCard from '/docs/components/LinkCard'; Create, edit, and implement Expectations and Expectation Suites. An Expectation is a verifiable assertion about your data, and an  Expectation Suite is a collection of verifiable assertions about your data.        ", "'Create and manage Custom Expectations'": " sidebar_label: 'Create and manage Custom Expectations' title: 'Create and manage Custom Expectations' id: custom_expectations_lp description: Create and manage Custom Expectations.  import LinkCardGrid from '/docs/components/LinkCardGrid'; import LinkCard from '/docs/components/LinkCard'; Create Custom Expectations to extend the functionality of Great Expectations (GX) and satisfy your unique business requirements. To contribute new Expectations to the open source project, see Contribute Custom Expectations.             ", "'Create Expectations'": " sidebar_label: 'Create Expectations' title: 'Create Expectations' id: expectations_lp description: Create and manage Expectations and Expectation Suites.  import LinkCardGrid from '/docs/components/LinkCardGrid'; import LinkCard from '/docs/components/LinkCard'; Create and manage Expectations and Expectation Suites. An Expectation is a verifiable assertion about your data, and an  Expectation Suite is a collection of verifiable assertions about your data.        ", "Create and edit Expectations based on domain knowledge, without inspecting data directly": " title: Create and edit Expectations based on domain knowledge, without inspecting data directly tag: [how-to, getting started] description: Create ExpectationConfigurations based on domain knowledge. keywords: [Expectations, Domain Knowledge]  import Prerequisites from '/docs/components/_prerequisites.jsx' import PrerequisiteQuickstartGuideComplete from '/docs/components/prerequisites/_quickstart_completed.mdx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; import IfYouStillNeedToSetupGX from '/docs/components/prerequisites/_if_you_still_need_to_setup_gx.md' import DataContextInitializeQuickOrFilesystem from '/docs/components/setup/link_lists/_data_context_initialize_quick_or_filesystem.mdx' import ConnectingToDataFluently from '/docs/components/connect_to_data/link_lists/_connecting_to_data_fluently.md' This guide shows how to create an Expectation Suite without a sample Batch. The following are the reasons why you might want to do this:  You don't have a sample. You don't currently have access to the data to make a sample. You know exactly how you want your Expectations to be configured. You want to create Expectations parametrically (you can also do this in interactive mode). You don't want to spend the time to validate against a sample.  If you have a use case we have not considered, please contact us on Slack. :::info Does this process edit my data? No.  The interactive method used to create and edit Expectations does not edit or alter the Batch data. ::: Prerequisites   Great Expectations installed in a Python environment A Filesystem Data Context for your Expectations Created a Data Source from which to request a Batch of data for introspection      ### If you haven't set up Great Expectations        ### If you haven't initialized your Data Context    See one of the following guides:       ### If you haven't created a Data Source    See one of the following guides:    Steps 1. Import the Great Expectations module and instantiate a Data Context For this guide we will be working with Python code in a Jupyter Notebook. Jupyter is included with GX and lets us easily edit code and immediately see the results of our changes. Run the following code to import Great Expectations and instantiate a Data Context: python name=\"tests/integration/docusaurus/expectations/how_to_create_and_edit_an_expectationsuite_domain_knowledge.py get_data_context\" :::info Data Contexts and persisting data If you're using an Ephemeral Data Context, your configurations will not persist beyond the current Python session.  However, if you're using a Filesystem or Cloud Data Context, they do persist.  The get_context() method returns the first Cloud or Filesystem Data Context it can find.  If a Cloud or Filesystem Data Context has not be configured or cannot be found, it provides an Ephemeral Data Context.  For more information about the get_context() method, see How to quickly instantiate a Data Context. ::: 2. Create an ExpectationSuite We will use the add_expectation_suite() method to create an empty ExpectationSuite. python name=\"tests/integration/docusaurus/expectations/how_to_create_and_edit_an_expectationsuite_domain_knowledge.py create_expectation_suite\" 3. Create Expectation Configurations You are adding Expectation configurations to the suite. Since there is no sample Batch of data, no Validation happens during this process. To illustrate how to do this, consider a hypothetical example. Suppose that you have a table with the columns account_id, user_id, transaction_id, transaction_type, and transaction_amt_usd. Then the following code snipped adds an Expectation that the columns of the actual table will appear in the order specified above: python name=\"tests/integration/docusaurus/expectations/how_to_create_and_edit_an_expectationsuite_domain_knowledge.py create_expectation_1\" Here are a few more example expectations for this dataset: python name=\"tests/integration/docusaurus/expectations/how_to_create_and_edit_an_expectationsuite_domain_knowledge.py create_expectation_2\" python name=\"tests/integration/docusaurus/expectations/how_to_create_and_edit_an_expectationsuite_domain_knowledge.py create_expectation_3\" python name=\"tests/integration/docusaurus/expectations/how_to_create_and_edit_an_expectationsuite_domain_knowledge.py create_expectation_4\" You can see all the available Expectations in the Expectation Gallery. 4. Save your Expectations for future use To keep your Expectations for future use, you save them to your Data Context.  A Filesystem or Cloud Data Context persists outside the current Python session, so saving the Expectation Suite in your Data Context's Expectations Store ensures you can access it in the future: python name=\"tests/integration/docusaurus/expectations/how_to_create_and_edit_an_expectationsuite_domain_knowledge.py save_expectation_suite\" :::caution Ephemeral Data Contexts and persistence Ephemeral Data Contexts don't persist beyond the current Python session.  If you're working with an Ephemeral Data Context, you'll need to convert it to a Filesystem Data Context using the Data Context's convert_to_file_context() method.  Otherwise, your saved configurations won't be available in future Python sessions as the Data Context itself is no longer available. ::: Next steps Now that you have created and saved an Expectation Suite, you can Validate your data.", "Create Expectations interactively with Python": " title: Create Expectations interactively with Python tag: [how-to, getting started] description: Create Expectations with a Python interpreter or a script and then use interactive feedback to validate them with batch data. keywords: [Expectations, Interactive Mode, Interactive]  import Prerequisites from '/docs/components/_prerequisites.jsx' import PrerequisiteQuickstartGuideComplete from '/docs/components/prerequisites/_quickstart_completed.mdx' import IfYouStillNeedToSetupGX from '/docs/components/prerequisites/_if_you_still_need_to_setup_gx.md' import DataContextInitializeQuickOrFilesystem from '/docs/components/setup/link_lists/_data_context_initialize_quick_or_filesystem.mdx' import ConnectingToDataFluently from '/docs/components/connect_to_data/link_lists/_connecting_to_data_fluently.md' To Validate data we must first define a set of Expectations for that data to be Validated against.  In this guide, you'll learn how to create Expectations and interactively edit them with feedback from Validating each against a Batch of data. Validating your Expectations as you define them allows you to quickly determine if the Expectations are suitable for our data, and identify where changes might be necessary. :::info Does this process edit my data? No.  The interactive method used to create and edit Expectations does not edit or alter the Batch data. ::: Prerequisites   Great Expectations installed in a Python environment A Filesystem Data Context for your Expectations Created a Data Source from which to request a Batch of data for introspection       ### If you haven't set up Great Expectations        ### If you haven't initialized your Data Context    See one of the following guides:       ### If you haven't created a Data Source    See one of the following guides:    Steps 1. Import the Great Expectations module and instantiate a Data Context For this guide we will be working with Python code in a Jupyter Notebook. Jupyter is included with GX and lets us easily edit code and immediately see the results of our changes. Run the following code to import Great Expectations and instantiate a Data Context: python name=\"tests/integration/docusaurus/validation/validator/how_to_create_and_edit_expectations_with_instant_feedback_fluent.py imports and data context\" :::info Data Contexts and persisting data If you're using an Ephemeral Data Context, your configurations will not persist beyond the current Python session.  However, if you're using a Filesystem or Cloud Data Context, they do persist.  The get_context() method returns the first Cloud or Filesystem Data Context it can find.  If a Cloud or Filesystem Data Context has not be configured or cannot be found, it provides an Ephemeral Data Context.  For more information about the get_context() method, see How to quickly instantiate a Data Context. ::: 2. Use an existing Data Asset to create a Batch Request Add the following method to retrieve a previously configured Data Asset from the Data Context you initialized and create a Batch Request to identify the Batch of data that you'll use to validate your Expectations: python name=\"tests/integration/docusaurus/validation/validator/how_to_create_and_edit_expectations_with_instant_feedback_fluent.py get_data_asset_and_build_batch_request\" :::info Limit the Batches returned by a Batch Request You can provide a dictionary as the options parameter of build_batch_request() to limit the Batches returned by a Batch Request.  If you leave the options parameter empty, your Batch Request will include all the Batches configured in the corresponding Data Asset.  For more information about Batch Requests, see How to request data from a Data Asset. ::: 3. Create a Validator When you use a Validator to interactively create your Expectations, the Validator needs two parameters. One parameter identifies the Batch that contains the data that is used to Validate the Expectations. The second parameter provides a name for the combined list of Expectations you create. :::info Working outside a Jupyter Notebook If you're using a Jupyter Notebook you'll automatically see the results of the code you run in a new cell when you run the code. If you're using a different interpreter, you might need to explicitly print these results to view them. For example: python name=\"tests/integration/docusaurus/validation/validator/how_to_create_and_edit_expectations_with_instant_feedback_fluent.py inspect_data_no_jupyter\" :::   Optional. Run the following command if you haven't created an Expectation Suite: python name=\"tests/integration/docusaurus/validation/validator/how_to_create_and_edit_expectations_with_instant_feedback_fluent.py create_expectation_suite\"   Run the following command to create a Validator: python name=\"tests/integration/docusaurus/validation/validator/how_to_create_and_edit_expectations_with_instant_feedback_fluent.py get_validator_and_inspect_data\"   4. Use the Validator to create and run an Expectation The Validator provides access to all the available Expectations as methods.  When an expect_*() method is run from the Validator, the Validator adds the specified Expectation to an Expectation Suite (or edits an existing Expectation in the Expectation Suite, if applicable) in its configuration, and then the specified Expectation is run against the data that was provided when the Validator was initialized with a Batch Request. python name=\"tests/integration/docusaurus/validation/validator/how_to_create_and_edit_expectations_with_instant_feedback_fluent.py interactive_validation\" Since we are working in a Jupyter Notebook, the results of the Validation are printed after we run an expect_*() method.  We can examine those results to determine if the Expectation needs to be edited. :::info Working outside a Jupyter Notebook If you are not working in a Jupyter Notebook you may need to explicitly print your results: python name=\"tests/integration/docusaurus/validation/validator/how_to_create_and_edit_expectations_with_instant_feedback_fluent.py interactive_validation_no_jupyter\" ::: 5. (Optional) Repeat step 4 to edit Expectations or create additional Expectations If you choose to edit an Expectation after you've viewed the Validation Results that were returned when it was created, you can do so by running the validator.expect_*() method with different parameters than you supplied previously.  You can also have the Validator run an entirely different expect_*() method and create additional Expectations.  All the Expectations that you create are stored in a list in the Validator's in-memory configuration. :::tip What if I want to use the same Expectation more than once? GX takes into account certain parameters when determining if an Expectation is being added to the list or if an existing Expectation should be edited.  For example, if you are created an Expectation with a method such as expect_column_*() you could later edit it by providing the same column parameter when running the expect_column_*() method a second time, and different values for any other parameters.  However, if you ran the same expect_column_*() method and provided a different column parameter, you will create an additional instance of the Expectation for the new column value, rather than overwrite the Expectation you defined with the first column value. ::: 6. (Optional) Save your Expectations for future use The Expectations you create with the interactive method are saved in an Expectation Suite on the Validator object.  Validators do not persist outside the current Python session and for this reason these Expectations will not be kept unless you save them to your Data Context.  This can be ideal if you are using a Validator for quick data validation and exploration, but in most cases you'll want to reuse your newly created Expectation Suite in future Python sessions. To keep your Expectations for future use, you save them to your Data Context.  A Filesystem or Cloud Data Context persists outside the current Python session, so saving the Expectation Suite in your Data Context's Expectations Store ensures you can access it in the future: python name=\"tests/integration/docusaurus/validation/validator/how_to_create_and_edit_expectations_with_instant_feedback_fluent.py save_expectation_suite\" :::caution Ephemeral Data Contexts and persistence Ephemeral Data Contexts don't persist beyond the current Python session.  If you're working with an Ephemeral Data Context, you'll need to convert it to a Filesystem Data Context using the Data Context's convert_to_file_context() method.  Otherwise, your saved configurations won't be available in future Python sessions as the Data Context itself is no longer available. ::: Next steps Now that you have created and saved an Expectation Suite, you can Validate your data.", "Edit an Expectation Suite": " title: Edit an Expectation Suite  tag: [how-to, getting started] description: Create an Expectation Suite using a Validator. Then, examine and modify specific Expectations in the Suite. keywords: [Expectations, ExpectationsSuite]  import Prerequisites from '/docs/components/_prerequisites.jsx' import IfYouStillNeedToSetupGX from '/docs/components/prerequisites/_if_you_still_need_to_setup_gx.md' In this guide, you'll learn how to create Expectations and interactively edit the resulting Expectation Suite. :::info Does this process edit my data? No.  The interactive method used to create and edit Expectations does not edit or alter the Batch data. ::: Prerequisites   Great Expectations installed in a Python environment A Filesystem Data Context for your Expectations Created a Data Source from which to request a Batch of data for introspection       ### If you haven't set up Great Expectations     Import the Great Expectations module and instantiate a Data Context The simplest way to create a new Data Context is by using the get_context() method. python name=\"tests/integration/docusaurus/expectations/how_to_edit_an_expectation_suite get_context\" Create a Validator from Data Run the following command to connect to .csv data stored in the great_expectations GitHub repository: python name=\"tests/integration/docusaurus/expectations/how_to_edit_an_expectation_suite create_validator\" Create Expectations with Validator Run the following commands to create two Expectations. The first Expectation uses domain knowledge (the pickup_datetime shouldn't be null), and the second Expectation uses auto=True to detect a range of values in the passenger_count column. python name=\"tests/integration/docusaurus/expectations/how_to_edit_an_expectation_suite add_2_expectations\" Under the hood, the Validator will be creating and updating an Expectation Suite, which we can view next.  View the Expectations in the Expectation Suite There are a number of different ways that this can be done, with one way being using the show_expectations_by_expectation_type() function, which will use prettyprint to print the Suite to the console in a way that can be easily visualized.  First load the ExpectationSuite from the Validator:  python name=\"tests/integration/docusaurus/expectations/how_to_edit_an_expectation_suite get_suite\" Now use the show_expectations_by_expectation_type() to print the Suite to console or Jupyter Notebook. python name=\"tests/integration/docusaurus/expectations/how_to_edit_an_expectation_suite show_suite\" Your output will look something similar to this:  bash  [ { 'expect_column_values_to_be_between': { 'auto': True,                                             'column': 'passenger_count',                                             'domain': 'column',                                             'max_value': 6,                                             'min_value': 1,                                             'mostly': 1.0,                                             'strict_max': False,                                             'strict_min': False}},   { 'expect_column_values_to_not_be_null': { 'column': 'pickup_datetime',                                              'domain': 'column'}}] Instantiate ExpectationConfiguration From the Expectation Suite, you will be able to create an ExpectationConfiguration object using the output from show_expectations_by_expectation_type() Here is the example output of the first Expectation in our suite. It runs the expect_column_values_to_be_between Expectation on the passenger_count column and expects the min and max values to be 1 and 6 respectively.  python name=\"tests/integration/docusaurus/expectations/how_to_edit_an_expectation_suite example_dict_1\" Here is the same configuration, but this time as a ExpectationConfiguration object.   python name=\"tests/integration/docusaurus/expectations/how_to_edit_an_expectation_suite import_expectation_configuration\" python name=\"tests/integration/docusaurus/expectations/how_to_edit_an_expectation_suite example_configuration_1\" Update Configuration and ExpectationSuite Let's say that you are interested in adjusting the max_value of the Expectation to be 4 instead of 6. Then you could create a new ExpectationConfiguration with the new value:  python name=\"tests/integration/docusaurus/expectations/how_to_edit_an_expectation_suite updated_configuration\" And update the ExpectationSuite by calling add_expectation(). The add_expectation() function will perform an 'upsert' into the ExpectationSuite, meaning it will update an existing Expectation if it already exists, or add a new one if it doesn't.  python name=\"tests/integration/docusaurus/expectations/how_to_edit_an_expectation_suite add_configuration\" You can check that the ExpectationSuite has been correctly updated by either running the show_expectations_by_expectation_type() function again, or by running find_expectation() and confirming that the expected Expectation exists in the suite.  The search will need to be performed with a new ExpectationConfiguration, but will not need to inclued all of the  kwarg values. python name=\"tests/integration/docusaurus/expectations/how_to_edit_an_expectation_suite find_configuration\" Remove the ExpectationConfiguration (Optional) If you would like to remove an ExpectationConfiguration, you can use the remove_configuration() function.  Similar to find_expectation(), the remove_configuration() function needs to be called with an ExpectationConfiguration. python name=\"tests/integration/docusaurus/expectations/how_to_edit_an_expectation_suite remove_configuration\" The output of show_expectations_by_expectation_type() should now look like this:  bash  [    { 'expect_column_values_to_not_be_null': { 'column': 'pickup_datetime',                                              'domain': 'column'}}] Save ExpectationSuite Finally, when you are done editing the ExpectationSuite, you can save it to your Data Context by using the save_suite() function.  python name=\"tests/integration/docusaurus/expectations/how_to_edit_an_expectation_suite save_suite\" Do not forget to overwrite the validator (or create a new one) from the updated context with context.get_validator(), otherwise your expectation suite changes will not be reflected on the validator. Related Documentation   If you would like to learn more about the functions available at the Expectation Suite-level, see the API Documentation for ExpectationSuite.    To view the full script used for example code on this page, see it on GitHub: how_to_edit_an_expectation_suite.py  ", "Use auto-initializing Expectations": " title: Use auto-initializing Expectations import Prerequisites from '../../guides/connecting_to_your_data/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; Use the information provided here to learn how you can use auto-initializing Expectations to automate parameter estimation when you create Expectations interactively using a Batch or Batches that have been loaded into a Validator. This guide assumes that you are creating and editing expectations in a Jupyter Notebook.  For more information about this process, see How to create and edit expectations with instant feedback from a sample batch of data.   Additionally, this guide assumes that you are using a multi-batch Batch Request to provide your sample data. Auto-initializing Expectations will work when run on a single Batch, but they really shine when run on multiple Batches that would have otherwise needed to be individually processed if a manual approach were taken. The following scripts are used in this topic and are available in GitHub:  is_expectation_auto_initializing.py auto_initializing_expect_column_mean_to_be_between.py  Prerequisites  Completion of the Quickstart guide A configured Data Context A configured Data Source An understanding of how to configure a BatchRequest An understanding of how to create and edit Expectations with instant feedback from a sample batch of data  Determine if your Expectation is auto-initializing Not all Expectations are auto-initializing.  In order to be an auto-initializing Expectation, an Expectation must have parameters that can be estimated.  As an example: ExpectColumnToExist only takes in a Domain (which is the column name) and checks whether the column name is in the list of names in the table's metadata.  This would be an example of an Expectation that would not work under the auto-initializing framework. An example of Expectations that would work under the auto-initializing framework would be the ones that have numeric ranges, like ExpectColumnMeanToBeBetween, ExpectColumnMaxToBeBetween, and ExpectColumnSumToBeBetween. To check whether the Expectation you are interested in works under the auto-initializing framework, run the is_expectation_auto_initializing() method of the Expectation class. For example: python name=\"tests/integration/docusaurus/expectations/auto_initializing_expectations/is_expectation_auto_initializing.py is_expectation_self_initializing False\" will return False and print the message: markdown title=\"Console output\" The Expectation expect_column_to_exist is not able to be auto-initialized. However, the command: python name=\"tests/integration/docusaurus/expectations/auto_initializing_expectations/is_expectation_auto_initializing.py is_expectation_self_initializing True\" will return True and print the message: markdown title=\"Console output\" The Expectation expect_column_mean_to_be_between is able to be auto-initialized. Please run by using the auto=True parameter. For the purposes of this guide, we will be using expect_column_mean_to_be_between as our example Expectation. Run the expectation with auto=True Say you are interested in constructing an Expectation that captures the average distance of taxi trips across all of 2018.  You have a Data Source that provides 12 Batches (one for each month of the year) and you know that expect_colum_mean_to_be_between is the Expectation you want to implement. Run the expectation manually The Expectation expect_column_mean_to_be_between() has the following parameters:  column (str): The column name. min_value (float or None): The minimum value for the column mean. max_value (float or None): The maximum value for the column mean. strict_min (boolean): If True, the column mean must be strictly larger than min_value, default=False strict_max (boolean): If True, the column mean must be strictly smaller than max_value, default=False  Without the auto-initialization framework you would have to get the values for min_value and max_value for your series of 12 Batches by calculating the mean value for each Batch and using calculated mean values to determine the min_value and max_value parameters to pass your Expectation.  This, although not difficult, would be a monotonous and time-consuming task. Using auto=True Auto-initializing Expectations automate this sort of calculation across batches.  To perform the same calculation described above (the mean ranges across the 12 Batches in the 2018 taxi data) the only thing you need to do is run the Expectation with auto=True python name=\"tests/integration/docusaurus/expectations/auto_initializing_expectations/auto_initializing_expect_column_mean_to_be_between.py run expectation\" Now the Expectation will calculate the min_value (2.83) and max_value (3.06) using all the Batches that are loaded into the Validator.  In our case, that means all 12 Batches associated with the 2018 taxi data. Save your Expectation with the calculated values Now that the Expectation's upper and lower bounds have come from the Batches, you can save your Expectation Suite and move on. python name=\"tests/integration/docusaurus/expectations/auto_initializing_expectations/auto_initializing_expect_column_mean_to_be_between.py save suite\"", "'Profilers and Data Assistants'": " sidebar_label: 'Profilers and Data Assistants' title: 'Profilers and Data Assistants' id: profilers_data_assistants_lp description: Use the Onboarding Data Assistant and Profiler to Profile your data and create Expectation Suites.  import LinkCardGrid from '/docs/components/LinkCardGrid'; import LinkCard from '/docs/components/LinkCard'; Use the Onboarding Data Assistant and Profiler to Profile your data and create Expectation Suites.      ", "Migration guide": " title: Migration guide The 0.13.x major release of Great Expectations deprecated Validation Operators and added new features such as \"new style\" Datasources, modular Expectations, and an improved Checkpoints feature (introduced as part of the 0.13.7 release). These updates led to the deprecation of much of the API surface utilized prior to the GX 0.13.x major release.  While we maintained that API surface through two major revisions in accordance with our deprecation policy, it has been fully deprecated as of the 0.16.x major release. Since the deprecated features and API surface are no longer present in the current GX code base, we will no longer be maintaining the migration guide from pre-GX 0.13.x processes to post GX 0.13.x processes in our current documentation.  To see the document as of the last version to of GX to support the pre-GX 0.13.x processes and API surface, reference the GX 0.15.50 documentation's migration guide.", "'Configure Data Contexts'": " sidebar_label: 'Configure Data Contexts' title: 'Configure Data Contexts' id: configure_data_contexts_lp description: Instantiate and convert a Data Context.  import LinkCardGrid from '/docs/components/LinkCardGrid'; import LinkCard from '/docs/components/LinkCard'; This is where you'll find information for instantiating and converting a Data Context.     ", "'Get started with Great Expectations'": " sidebar_label: 'Get started with GX' title: 'Get started with Great Expectations' id: get_started_lp description: Install Great Expectations and initialize your deployment.  import LinkCardGrid from '/docs/components/LinkCardGrid'; import LinkCard from '/docs/components/LinkCard'; Start here if you're unfamiliar with Great Expectations (GX), or you want to use GX with Databricks or a SQL Data Source in a production environment. To install and configure GX in your specific production environment, see Set up your Great Expectations environment.       ", "\"Great Expectations installation and configuration workflow\"": " sidebar_label: 'GX installation and configuration workflow' title: \"Great Expectations installation and configuration workflow\"  import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; import UniversalMap from '@site/docs/images/universal_map/_universal_map.mdx'; import GxData from '/docs/components/_data.jsx';  Setting up Great Expectations (GX) includes installing GX and initializing your deployment. Optionally, you can customize the configuration of some components, such as Stores, Data Docs, and Plugins. After you've completed the setup for your production deployment, you can access all GX features from your Data Context. Also, your Stores and Data Docs will be optimized for your business requirements. To set up Datasources, Expectation Suites, and Checkpoints see the specific topics for these components.  If you don't want to manage your own configurations and infrastructure, then Great Expectations Cloud might be the solution. If you're interested in participating in the Great Expectations Cloud Beta program, or you want to receive progress updates, sign up for the Beta program. :::info Windows Support Windows support for the open source Python version of GX is currently unavailable. If you\u2019re using GX in a Windows environment, you might experience errors or performance issues. ::: Before you start Before you start installing and configuring GX, you should complete the Quickstart guide and have the following items installed:  A supported version of Python. GX supports Python versions {GxData.min_python} to {GxData.max_python}. pip (the package installer for Python). An internet connection. A web browser (for Jupyter Notebooks). A virtual environment. Recommended for your project workspace.  Install Great Expectations See Install Great Expectations. Initialize a Data Context Your Data Context contains your Great Expectations project, and it is the entry point for configuring and interacting with Great Expectations. The Data Context manages various classes and helps limit the number of objects you need to manage to get Great Expectations working.  See Configure Data Contexts. Optional configurations After you've initialized your Data Context, you can start using Great Expectations. However, a few components such as Stores, Data Docs, and Plugins that are configured by default to operate locally can be changed to hosted if it better suits your use case. Stores Stores are the locations where your Data Context stores information about your Expectations, your Validation Results, and your Metrics.  By default, these are stored locally. To reconfigure a Store to work with a specific backend, see Configure Expectation Stores, Configure Validation Result Stores, and Configure a MetricStore. Data Docs Data Docs provide human-readable renderings of your Expectation Suites and Validation Results, and they are built locally by default. To host and share Data Docs differently, see Host and share Data Docs. Plugins Python files are treated as Plugins when they are in the plugins directory of your project (which is created automatically when you initialize your Data Context) and they can be used to extend Great Expectations.  If you have Custom Expectations or other extensions that you want to use as Plugins with Great Expectations, add them to the plugins directory. Next steps  Install Great Expectations ", "'Configure your Great Expectations environment'": " sidebar_label: 'Configure your Great Expectations environment' title: 'Configure your Great Expectations environment' id: setup_overview_lp description: Configure GX in your specific environment.  import LinkCardGrid from '/docs/components/LinkCardGrid'; import LinkCard from '/docs/components/LinkCard'; This is where you'll find information for setting up Great Expectations (GX) in your specific environment. Install and configure         Host and share   ", "Run a Checkpoint to validate data": " title: Run a Checkpoint to validate data import Prerequisites from '../../guides/connecting_to_your_data/components/prerequisites.jsx'; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; This guide will help you Validate your data by running a Checkpoint. The best way to Validate data with Great Expectations is using a Checkpoint. Checkpoints identify what Expectation Suites to run against which Data Asset and Batch (described by a Batch Requests), and what Actions to take based on the results of those tests. Succinctly: Checkpoints are used to test your data and take action based on the results. Prerequisites   Configured a Data Context Configured an Expectations Suite Configured a Checkpoint   Run the Checkpoint You can run the Checkpoint from the CLI in a Terminal shell or using Python.   If you already have created and saved a Checkpoint, then the following code snippet will retrieve it from your context and run it: python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_validate_data_by_running_a_checkpoint.py checkpoint script\" If you do not have a Checkpoint, the pre-requisite guides mentioned above will take you through the necessary steps. Alternatively, this concise example below shows how to connect to data, create an expectation suite using a validator, and create a checkpoint (saving everything to the Data Context along the way). python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_validate_data_by_running_a_checkpoint.py setup\"   If you have already created and saved a Checkpoint, then you can run the Checkpoint using the CLI. bash great_expectations checkpoint run my_checkpoint Additional notes This command will return posix status codes and print messages as follows: +-------------------------------+-----------------+-----------------------+ | **Situation**                 | **Return code** | **Message**           | +-------------------------------+-----------------+-----------------------+ | all validations passed        | 0               | Validation succeeded! | +-------------------------------+-----------------+-----------------------+ | one or more validation failed | 1               | Validation failed!    | +-------------------------------+-----------------+-----------------------+   ", "'Validate Data'": " sidebar_label: 'Validate Data' title: 'Validate Data' id: validate_data_lp description: Validate Data, save Validation Results, run Actions, and create Data Docs.  import LinkCardGrid from '/docs/components/LinkCardGrid'; import LinkCard from '/docs/components/LinkCard'; Validate Data, save Validation Results, run Actions, and create Data Docs.     ", "\"Data Validation workflow\"": " title: \"Data Validation workflow\" import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; Great Expectations recommends using Checkpoints to validate data.  Checkpoints validate data, save Validation Results, run any Actions you have specified, and finally, create Data Docs with their results.  A Checkpoint can be reused to Validate data in the future, and you can create and configure additional Checkpoints for different business requirements.  After you've created your Checkpoint, configured it, and specified the Actions you want it to take based on the Validation Results, all you'll need to do in the future is run the Checkpoint. Prerequisites  Completion of the Quickstart guide  Create a Checkpoint See How to create a new Checkpoint. Configure your Checkpoint When you configure your Checkpoint you can add additional validation data, or specify that validation data must be specified at run time.  You can add additional Expectation Suites, and you can add Actions which the Checkpoint executes when it finishes Validating data.  To learn more about Checkpoint configuration, see Checkpoints and Actions. Checkpoints, Batch Requests, and Expectation Suites Batch Requests are used to specify the data that a Checkpoint Validates.  You can add additional validation data to your Checkpoint by assigning it Batch Requests, or specifying that a Batch Request is required at run time. Expectation Suites contain the Expectations that the Checkpoint runs against the validation data specified in its Batch Requests.  Checkpoints are assigned Expectation Suites and Batch Requests in pairs, and when the Checkpoint is run it will Validate each of its Expectation Suites against the data provided by its paired Batch Request. For more information on adding Batch Requests and Expectation Suites to a Checkpoint, see How to add validations data or suites to a Checkpoint. Checkpoints and Actions Actions are optional and are executed after a Checkpoint validates data. Some of the more common Actions include updating Data Docs, sending emails, posting Slack notifications, or sending custom notifications. You can create custom Actions to complete business specific actions after a Checkpoint Validates. For more information about Actions, see Configure Actions. Run your Checkpoint See How to validate data by running a Checkpoint. Validation Results and Data Docs When a Checkpoint finishes Validation, its Validation Results are automatically compiled as Data Docs.  You can find these results in the Validation Results tab of your Data Docs, and clicking an individual Validation Result in the Data Docs displays a detailed list of all the Expectations that ran, as well as which Expectations passed or failed. For more information, see the Data Docs documentation.  Checkpoint reuse After your Checkpoint is created, and you have used it to validate data, you can reuse it in a Python script. If you want your Checkpoint to run on a schedule, see How to deploy a scheduled Checkpoint with cron. If your pipeline architecture supports it, you can run your Checkpoints with Python scripts.  Regardless of the method you use to run your Checkpoint, Actions let you customize what is done with the generated Validation Results. ", "Conditional Expectations": " title: Conditional Expectations :::note Conditional Expectations are experimental, and they are available for Pandas, Spark, and SQLAlchemy backends. ::: You can create an Expectation for an entire dataset, or for a subset of the dataset. Some variables are dependent on the values of other variables. For example, a column that specifies that the country of origin must not be null for people of foreign descent. Great Expectations lets you express Conditional Expectations with a row_condition argument that can be passed to all Dataset Expectations. The row_condition argument should be a boolean expression string. In addition, you must provide the condition_parser argument which defines the syntax of conditions. When implementing conditional Expectations with Pandas, this argument must be set to \"pandas\". When implementing conditional Expectations with Spark or SQLAlchemy, this argument must be set to \"great_expectations__experimental__\".  :::note In Pandas the row_condition value is passed to pandas.DataFrame.query() before Expectation Validation. See pandas.DataFrame.query. In Spark and SQLAlchemy, the row_condition value is parsed as a data filter or a query before Expectation Validation. ::: Examples To test if different encodings of identical pieces of information are consistent with each other, run a command similar to this example:  python validator.expect_column_values_to_be_in_set(     column='Sex',     value_set=['male'],     condition_parser='pandas',     row_condition='SexCode==0' ) This returns: python {     \"success\": true,     \"result\": {         \"element_count\": 851,         \"missing_count\": 0,         \"missing_percent\": 0.0,         \"unexpected_count\": 0,         \"unexpected_percent\": 0.0,         \"unexpected_percent_nonmissing\": 0.0,         \"partial_unexpected_list\": []     } } :::note To get a Validator object, see How to create Expectations interactively in Python. ::: It is possible to add multiple Expectations of the same type to the Expectation Suite for a single column. One Expectation can be unconditional while an arbitrary number of Expectations (each with a different condition) can be conditional. For example: ```python validator.expect_column_values_to_be_in_set(         column='Survived',         value_set=[0, 1]     ) validator.expect_column_values_to_be_in_set(         column='Survived',         value_set=[1],         condition_parser='pandas',         row_condition='PClass==\"1st\"'     ) The second Expectation fails, but we want to include it in the output: validator.get_expectation_suite(   discard_failed_expectations=False ) ``` This results in the following Expectation Suite: python {     \"expectation_suite_name\": \"default\",     \"expectations\": [         {             \"meta\": {},             \"kwargs\": {                 \"column\": \"Survived\",                 \"value_set\": [0, 1]             },             \"expectation_type\": \"expect_column_values_to_be_in_set\"         },         {             \"meta\": {},             \"kwargs\": {                 \"column\": \"Survived\",                 \"value_set\": [1],                 \"row_condition\": \"PClass==\\\"1st\\\"\",                 \"condition_parser\": \"pandas\"             },             \"expectation_type\": \"expect_column_values_to_be_in_set\"         }     ],     \"data_asset_type\": \"Dataset\" } Format of row_conditions values Do not use single quotes or \\n inside the specified row_condition as shown in the following examples: python  row_condition=\"PClass=='1st'\"  # never use simple quotes inside !!! python  row_condition=\"\"\" PClass==\"1st\" \"\"\"  # never use \\n inside !!! Data Docs and Conditional Expectations Conditional Expectations are displayed differently from standard Expectations in the Data Docs. Each Conditional Expectation is qualified with if 'row_condition_string', then values must be... as shown in the following image:  If 'row_condition_string' is a complex expression, it is split into several components to improve readability. Scope and limitations While conditions can be attached to most Expectations, the following Expectations cannot be conditioned and do not take the row_condition argument:  expect_column_to_exist expect_table_columns_to_match_ordered_list expect_table_column_count_to_be_between expect_table_column_count_to_equal  For more information, see the Data Docs.", "Distributional Expectations": " title: Distributional Expectations Distributional Expectations help identify when new datasets or samples may be different than expected, and can help ensure that assumptions developed during exploratory analysis still hold as new data becomes available. You should use Distributional Expectations in the same way as other Expectations: to help accelerate identification of risks and changes to a modeled system or disruptions to a complex upstream data feed. Great Expectations provides a direct, expressive framework for describing Distributional Expectations. Most Distributional Expectations apply nonparametric tests, although it is possible to build Expectations from parameterized distributions. Partition objects Distributional Expectations rely on expected distributions or \"partition objects\", which are built from intervals for continuous data or categorical classes and their associated weights. For continuous data  A partition is defined by an ordered list of points that define intervals on the real number line. Note that partition   intervals do not need to be uniform. Each bin in a partition is partially open: a data element x is in bin i if lower_bound_i <= x < upper_bound_i. However, following the behavior of numpy.histogram, a data element x is in the largest bin k if x == upper_bound_k. A partition object can also include tail_weights which extend from -Infinity to the lowest bound, and from the   highest bound to +Infinity. Partition weights define the probability of the associated bin or interval. Note that this applies a \"piecewise   uniform\" distribution to the data for the purpose of statistical tests. The weights must define a valid probability   distribution, i.e. they must be non-negative numbers that sum to 1.  Example continuous partition object: python partition = {     \"bins\": [0, 1, 2, 10],     \"weights\": [0.2, 0.3, 0.3],     \"tail_weights\": [0.1, 0.1] } json.dumps(partition, indent=2) results in: python {     \"bins\": [ 0, 1, 2, 10],     \"weights\": [0.2, 0.3, 0.3],     \"tail_weights\": [0.1, 0.1] } For discrete/categorical data  A partition defines the categorical values present in the data. Partition weights define the probability of the associated categorical value.  Example discrete partition object: python {     \"values\": [\"cat\", \"dog\", \"fish\"],     \"weights\": [0.3, 0.3, 0.4] } Constructing partition objects Convenience functions are available to easily construct partition objects from existing data:  build_continuous_partition_object build_categorical_partition_object  Convenience functions are also provided to validate that an object is valid:  is_valid_continuous_partition_object is_valid_categorical_partition_object  Tests interpret partition objects literally, so care should be taken when a partition includes a segment with zero weight. The convenience methods consequently allow you to include small amounts of residual weight on the \"tails\" of a dataset used to construct a partition. Available Distributional Expectations Kullback-Leibler (KL) divergence/) (also known as relative entropy) is available as an expectation for both categorical and continuous data (continuous data will be discretized according to the provided partition prior to computing divergence). Unlike KS and Chi-Squared tests which can use a p-value, you must provide a threshold for the relative entropy to use KL divergence. Further, KL divergence is not symmetric.  expect_column_kl_divergence_to_be_less_than  For continuous data, the expect_column_bootstrapped_ks_test_p_value_to_be_greater_than expectation uses the Kolmogorov-Smirnov (KS) test/), which compares the actual and expected cumulative densities of the data. Because of the partition_object's piecewise uniform approximation of the expected distribution, the test would be overly sensitive to differences when used with a sample of data of much larger than the size of the partition interval. The expectation consequently uses a bootstrapping method to sample the provided data with tunable specificity.  expect_column_bootstrapped_ks_test_p_value_to_be_greater_than  For categorical data, the expect_column_chisquare_test_p_value_to_be_greater_than expectation uses the Chi-Squared test. The Chi-Squared test works with expected and observed counts, but that is handled internally in this function -- both the input and output to this function are valid partition objects (ie with weights that are probabilities and sum to 1).  expect_column_chisquare_test_p_value_to_be_greater_than ", "Result format": " title: Result format The result_format parameter may be either a string or a dictionary which specifies the fields to return in result.   * The following string values are supported:     * \"BOOLEAN_ONLY\", \"BASIC\", \"SUMMARY\", or \"COMPLETE\". The default is \"SUMMARY\". The behavior of each setting is described in the examples below.   * For dictionary usage, result_format may include the following keys:     * result_format: Sets the fields to return in result.     * unexpected_index_column_names: Defines columns that can be used to identify unexpected results, for example primary key (PK) column(s) or other columns with unique identifiers. Supports multiple column names as a list.     * return_unexpected_index_query: When running validations, a query (or a set of indices) will be returned that will       allow you to retrieve the full set of unexpected results including any columns identified in unexpected_index_column_names.  Setting this value to False will        suppress the output (default is True).     * partial_unexpected_count: Sets the number of results to include in partial_unexpected_count, if applicable. If        set to 0, this will suppress the unexpected counts.     * include_unexpected_rows: When running validations, this will return the entire row for each unexpected value in       dictionary form. When using include_unexpected_rows, you must explicitly specify result_format as well, and       result_format must be more verbose than BOOLEAN_ONLY. *WARNING: * :::warning   include_unexpected_rows returns EVERY row for each unexpected value; for large tables, this could return an    unwieldy amount of data.   ::: Configure result format result_format can be specified for either a single Expectation or an entire Checkpoint. When configured at the Expectation-level,  the configuration will not be persisted, and you will receive a UserWarning. We therefore recommend that the Expectation-level configuration be used for exploratory analysis, with the final configuration added at the Checkpoint-level. Expectation-level configuration To apply result_format to an Expectation, pass it into the Expectation. We will first need to obtain a Validator object instance (e.g. by running the $ great_expectations suite new command). python name=\"tests/integration/docusaurus/reference/core_concepts/result_format/result_format_complete_example_set\" Checkpoint-level configuration To apply result_format to every Expectation in a Suite, define it in your Checkpoint configuration under the runtime_configuration key. python name=\"tests/integration/docusaurus/reference/core_concepts/result_format/result_format_checkpoint_example\" The results will then be stored in the Validation Result after running the Checkpoint. :::note The unexpected_index_list, as represented by primary key (PK) columns, is  rendered in DataDocs when COMPLETE is selected.  The unexpected_index_query, which for SQL and Spark is a query that allows you to retrieve the full set of  unexpected values from the dataset, is also rendered by default when COMPLETE is selected. For Pandas, this parameter  returns the full set of unexpected indices, which can also be used to retrieve the full set of unexpected values. This is returned whether or not the unexpected_index_column_names are defined.  To suppress this output, the return_unexpected_index_query parameter can be set to False.  Regardless of how Result Format is configured, unexpected_list is never rendered in Data Docs. ::: Result format values and fields All Expectations | Fields within result                | BOOLEAN_ONLY                       |BASIC           |SUMMARY         |COMPLETE        | ----------------------------------------|------------------------------------|----------------|----------------|----------------- |    details (dictionary)               | Defined on a per-Expectation basis | Column map Expectations (e.g. ColumnMapExpectation, ColumnPairMapExpectation, MulticolumnMapExpectation) | Fields within result                |BOOLEAN_ONLY    |BASIC           |SUMMARY         |COMPLETE        | ----------------------------------------|----------------|----------------|----------------|----------------- |    element_count                      |no              |yes             |yes             |yes             | |    missing_count                      |no              |yes             |yes             |yes             | |    missing_percent                    |no              |yes             |yes             |yes             | |    unexpected_count                   |no              |yes             |yes             |yes             | |    unexpected_percent                 |no              |yes             |yes             |yes             | |    unexpected_percent_nonmissing      |no              |yes             |yes             |yes             | |    partial_unexpected_list            |no              |yes             |yes             |yes             | |    partial_unexpected_index_list      |no              |no              |yes             |yes             | |    partial_unexpected_counts          |no              |no              |yes             |yes             | |    unexpected_index_list              |no              |no              |no              |yes             | |    unexpected_index_query             |no              |no              |no              |yes             | |    unexpected_list                    |no              |no              |no              |yes             | Column aggregate Expectations | Fields within result                |BOOLEAN_ONLY    |BASIC           |SUMMARY         |COMPLETE        | ----------------------------------------|----------------|----------------|----------------|----------------- |    observed_value                     |no              |yes             |yes             |yes             | Example use cases for different result_format values | result_format Setting               | Example use case                                             | ----------------------------------------|--------------------------------------------------------------- |    BOOLEAN_ONLY                       | Automatic validation. No result is returned.                 | |    BASIC                              | Exploratory analysis in a notebook.                          | |    SUMMARY                            | Detailed exploratory work with follow-on investigation.      | |    COMPLETE                           | Debugging pipelines or developing detailed regression tests. | Examples The following examples will use the data defined in the following Pandas DataFrame: python name=\"tests/integration/docusaurus/reference/core_concepts/result_format/pandas_df_for_result_format\" Behavior for BOOLEAN_ONLY When the result_format is BOOLEAN_ONLY, no result is returned. The result of evaluating the Expectation is exclusively returned via the value of the success parameter. For example: python name=\"tests/integration/docusaurus/reference/core_concepts/result_format/result_format_boolean_example\" Will return the following output: python name=\"tests/integration/docusaurus/reference/core_concepts/result_format/result_format_boolean_example_output\" Behavior for BASIC For BASIC format, a result is generated with a basic justification for why an Expectation was met or not. The format is intended  for quick, at-a-glance feedback. For example, it tends to work well in Jupyter Notebooks. Great Expectations has standard behavior for support for describing the results of column_map_expectation and ColumnAggregateExpectation Expectations. column_map_expectation applies a boolean test function to each element within a column, and so returns a list of unexpected values to justify the Expectation result. The basic result includes: python {     \"success\" : Boolean,     \"result\" : {         \"partial_unexpected_list\" : [A list of up to 20 values that violate the Expectation]         \"unexpected_count\" : The total count of unexpected values in the column         \"unexpected_percent\" : The overall percent of unexpected values         \"unexpected_percent_nonmissing\" : The percent of unexpected values, excluding missing values from the denominator     } } Note: When unexpected values are duplicated, unexpected_list will contain multiple copies of the value. For example: python name=\"tests/integration/docusaurus/reference/core_concepts/result_format/result_format_basic_example_set\" Will return the following output: python name=\"tests/integration/docusaurus/reference/core_concepts/result_format/result_format_basic_example_set_output\" ColumnAggregateExpectation computes a single aggregate value for the column, and so returns a single  observed_value to justify the Expectation result. The basic result includes: python {     \"success\" : Boolean,     \"result\" : {         \"observed_value\" : The aggregate statistic computed for the column     } } For example: python name=\"tests/integration/docusaurus/reference/core_concepts/result_format/result_format_basic_example_agg\" Will return the following output: python name=\"tests/integration/docusaurus/reference/core_concepts/result_format/result_format_basic_example_agg_output\" Behavior for SUMMARY A result is generated with a summary justification for why an Expectation was met or not. The format is intended for more detailed exploratory work and includes additional information beyond what is included by BASIC. For example, it can support generating dashboard results of whether a set of Expectations are being met. Great Expectations has standard behavior for support for describing the results of column_map_expectation and ColumnAggregateExpectation Expectations. column_map_expectation applies a boolean test function to each element within a column, and so returns a list of unexpected values to justify the Expectation result. The summary result includes: python {     'success': False,     'result': {         'element_count': The total number of values in the column         'unexpected_count': The total count of unexpected values in the column (also in `BASIC`)         'unexpected_percent': The overall percent of unexpected values (also in `BASIC`)         'unexpected_percent_nonmissing': The percent of unexpected values, excluding missing values from the denominator (also in `BASIC`)         \"partial_unexpected_list\" : [A list of up to 20 values that violate the Expectation] (also in `BASIC`)         'missing_count': The number of missing values in the column         'missing_percent': The total percent of missing values in the column         'partial_unexpected_counts': [{A list of objects with value and counts, showing the number of times each of the unexpected values occurs}         'partial_unexpected_index_list': [A list of up to 20 of the indices of the unexpected values in the column, as defined by the columns in `unexpected_index_column_names`]     } } For example: python name=\"tests/integration/docusaurus/reference/core_concepts/result_format/result_format_summary_example_set\" Will return the following output: python name=\"tests/integration/docusaurus/reference/core_concepts/result_format/result_format_summary_example_set_output\" ColumnAggregateExpectation computes a single aggregate value for the column, and so returns a observed_value  to justify the Expectation result. It also includes additional information regarding observed values and counts,  depending on the specific Expectation. The summary result includes: python {     'success': False,     'result': {         'observed_value': The aggregate statistic computed for the column (also in `BASIC`)     } } For example: python name=\"tests/integration/docusaurus/reference/core_concepts/result_format/result_format_summary_example_agg\" Will return the following output: python name=\"tests/integration/docusaurus/reference/core_concepts/result_format/result_format_summary_example_agg_output\" Behavior for COMPLETE A result is generated with all available justification for why an Expectation was met or not. The format is intended for debugging pipelines or developing detailed regression tests. Great Expectations has standard behavior for support for describing the results of column_map_expectation and ColumnAggregateExpectation Expectations. column_map_expectation applies a boolean test function to each element within a column, and so returns a list of  unexpected values to justify the Expectation result. The complete result includes: python {     'success': False,     'result': {         \"unexpected_list\" : [A list of all values that violate the Expectation]         'unexpected_index_list': [A list of the indices of the unexpected values in the column, as defined by the columns in `unexpected_index_column_names`]         'unexpected_index_query': [A query that can be used to retrieve all unexpected values (SQL and Spark), or the full list of unexpected indices (Pandas)]         'element_count': The total number of values in the column (also in `SUMMARY`)         'unexpected_count': The total count of unexpected values in the column (also in `SUMMARY`)         'unexpected_percent': The overall percent of unexpected values (also in `SUMMARY`)         'unexpected_percent_nonmissing': The percent of unexpected values, excluding missing values from the denominator (also in `SUMMARY`)         'missing_count': The number of missing values in the column  (also in `SUMMARY`)         'missing_percent': The total percent of missing values in the column  (also in `SUMMARY`)     } } For example: python name=\"tests/integration/docusaurus/reference/core_concepts/result_format/result_format_complete_example_set\" Will return the following output: python name=\"tests/integration/docusaurus/reference/core_concepts/result_format/result_format_complete_example_set_output\" ColumnAggregateExpectation computes a single aggregate value for the column, and so returns a observed_value  to justify the Expectation result. It also includes additional information regarding observed values and counts, depending on the specific Expectation. The complete result includes: python {     'success': False,     'result': {         'observed_value': The aggregate statistic computed for the column (also in `SUMMARY`)         'details': {<Expectation-specific result justification fields, which may be more detailed than in `SUMMARY`>}     } } For example: python name=\"tests/integration/docusaurus/reference/core_concepts/result_format/result_format_complete_example_agg\" Will return the following output: python name=\"tests/integration/docusaurus/reference/core_concepts/result_format/result_format_complete_example_agg_output\"", "Standard arguments for Expectations": " title: Standard arguments for Expectations All Expectations return a JSON-serializable dictionary when evaluated, and share four standard (optional) arguments:  result_format: Controls what information is returned from the evaluation of the Expectation. catch_exceptions: If true, execution will not fail if the Expectation encounters an error.   Instead, it will return success = False and provide an informative error message. meta: Allows user-supplied meta-data to be stored with an Expectation.  All ColumnMapExpectations and MultiColumnMapExpectation also have the following argument:  mostly: A special argument that allows for fuzzy validation based on some percentage  (available for all column_map_expectations and multicolumn_map_expectations)  result_format See Result format for more information. catch_exceptions All Expectations accept a boolean catch_exceptions parameter. If this parameter is set to True, then Great Expectations will intercept any exceptions so that execution will not fail if the Expectation encounters an error. Instead, if Great Excpectations catches an exception while evaluating an Expectation, the Expectation result will ( in BASIC and SUMMARY modes) return the following informative error message: python {     \"result\": False,     \"catch_exceptions\": True,     \"exception_traceback\": \"...\" } catch_exceptions is on by default in command-line validation mode, and off by default in exploration mode. meta All Expectations accept an optional meta parameter. If meta is a valid JSON-serializable dictionary, it will be \\ passed through to the expectation_result object without modification. The meta parameter can be used to add \\ helpful markdown annotations to Expectations (shown below). These Expectation \"notes\" are rendered within \\ Expectation Suite pages in Data Docs. ```python validator.expect_column_values_to_be_in_set(     \"my_column\",     [\"a\", \"b\", \"c\"],     meta={       \"notes\": {         \"format\": \"markdown\",         \"content\": [           \"#### These are expectation notes \\n - you can use markdown \\n - or just strings\"         ]       }     } ) This returns: {     \"success\": False,     \"meta\": {       \"notes\": {         \"format\": \"markdown\",         \"content\": [           \"#### These are expectation notes \\n - you can use markdown \\n - or just strings\"         ]       }     } } ``` mostly mostly is a special argument that is automatically available in all column_map_expectations and multicolumn_map_expectations. mostly must be a float between 0 and 1. Great Expectations evaluates it as a percentage, allowing some wiggle room when evaluating Expectations: as long as mostly percent of rows evaluate to True, the Expectation returns \"success\": True. ```python [0,1,2,3,4,5,6,7,8,9] validator.expect_column_values_to_be_between(     \"my_column\",     min_value=0,     max_value=7 ) This returns: {     \"success\": False,     ... } validator.expect_column_values_to_be_between(     \"my_column\",     min_value=0,     max_value=7,     mostly=0.7 ) This returns: {     \"success\": True,     ... } ``` Expectations with mostly return exception lists even if they succeed: ```python validator.expect_column_values_to_be_between(     \"my_column\",     min_value=0,     max_value=7,     mostly=0.7 ) This returns: {   \"success\": true   \"result\": {     \"unexpected_percent\": 0.2,     \"partial_unexpected_index_list\": [       8,       9     ],     \"partial_unexpected_list\": [       8,       9     ],     \"unexpected_percent_nonmissing\": 0.2,     \"unexpected_count\": 2   } } ``` Checkpoints and result_format While result_format and catch_expectations are both standard arguments for Expectations, the result_format argument is also a valid parameter when included in calls to the run(...) command of a Checkpoint. :::note Reminder: For more detailed information on how to define result_format values, please see our reference guide on result_format. :::", "Get started with Great Expectations and Databricks": " sidebar_label: 'Get started with GX and Databricks' title: Get started with Great Expectations and Databricks  import Prerequisites from '../../deployment_patterns/components/deployment_pattern_prerequisites.jsx' import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; Use the information provided here to learn how you can use Great Expectations (GX) with Databricks. To use GX with Databricks, you'll complete the following tasks:  Load data Instantiate a Data Context Create a Data Source and a Data Asset Create an Expectation Suite Validate data using a Checkpoint  The information provided here is intended to get you up and running quickly. To validate files stored in the DBFS, select the File tab. If you have an existing Spark DataFrame loaded, select one of the DataFrame tabs. See the specific integration guides if you're using a different file store such as Amazon S3, Google Cloud Storage (GCS), or Microsoft Azure Blob Storage (ABS). The full code used in the following examples is available on GitHub:   databricks_deployment_patterns_file_python_configs.py   databricks_deployment_patterns_dataframe_python_configs.py   Prerequisites   A complete Databricks setup including a running Databricks cluster with an attached notebook Access to DBFS   Install GX   Run the following command in your notebook to install GX as a notebook-scoped library: bash %pip install great-expectations   A notebook-scoped library is a custom Python environment that is specific to a notebook. You can also install a library at the cluster or workspace level. See Databricks Libraries.  Run the following command to import the Python configurations you'll use in the following steps:  python name=\"tests/integration/docusaurus/deployment_patterns/databricks_deployment_patterns_file_python_configs.py imports\" Set up GX To avoid configuring external resources, you'll use the Databricks File System (DBFS) for your Metadata Stores and Data Docs store. DBFS is a distributed file system mounted in a Databricks workspace and available on Databricks clusters. Files on DBFS can be written and read as if they were on a local filesystem, just by adding the /dbfs/ prefix to the path. It is also persisted to object storage, so you won\u2019t lose data after you terminate a cluster. See the Databricks documentation for best practices including mounting object stores.  Run the following code to set up a Data Context with the default settings:  python name=\"tests/integration/docusaurus/deployment_patterns/databricks_deployment_patterns_file_python_configs.py choose context_root_dir\" 2. Run the following code to instantiate your Data Context: python name=\"tests/integration/docusaurus/deployment_patterns/databricks_deployment_patterns_file_python_configs.py set up context\" Prepare your data   Run the following command with dbutils to copy existing example csv taxi data to your DBFS folder: ```python Copy 3 months of data for month in range(1, 4):     dbutils.fs.cp(       f\"/databricks-datasets/nyctaxi/tripdata/yellow/yellow_tripdata_2019-0{month}.csv.gz\",       f\"/example_data/nyctaxi/tripdata/yellow/yellow_tripdata_2019-0{month}.csv.gz\"     ) ```   Run the following code in your notebook to load a month of existing example taxi data as a DataFrame: python df = spark.read.format(\"csv\")\\     .option(\"header\", \"true\")\\     .option(\"inferSchema\", \"true\")\\     .load(\"/databricks-datasets/nyctaxi/tripdata/yellow/yellow_tripdata_2019-01.csv.gz\")   Connect to your data    Run the following command to set the base directory that contains the data:  python name=\"tests/integration/docusaurus/deployment_patterns/databricks_deployment_patterns_file_python_configs.py choose base directory\"  Run the following command to create our Data Source:  python name=\"tests/integration/docusaurus/deployment_patterns/databricks_deployment_patterns_file_python_configs.py add datasource\"  Run the following command to set the batching regex:  python name=\"tests/integration/docusaurus/deployment_patterns/databricks_deployment_patterns_file_python_configs.py choose batching regex\"  Run the following command to create a Data Asset with the Data Source:  python name=\"tests/integration/docusaurus/deployment_patterns/databricks_deployment_patterns_file_python_configs.py add data asset\"  Run the following command to build a Batch Request with the Data Asset you configured earlier:  python name=\"tests/integration/docusaurus/deployment_patterns/databricks_deployment_patterns_file_python_configs.py build batch request\"    Run the following command to create the Data Source:  python name=\"tests/integration/docusaurus/deployment_patterns/databricks_deployment_patterns_dataframe_python_configs.py add datasource\"  Run the following command to create a Data Asset with the Data Source:  python name=\"tests/integration/docusaurus/deployment_patterns/databricks_deployment_patterns_dataframe_python_configs.py add data asset\"  Run the following command to build a Batch Request with the Data Asset you configured earlier:  python name=\"tests/integration/docusaurus/deployment_patterns/databricks_deployment_patterns_dataframe_python_configs.py build batch request\"   Create Expectations You'll use a Validator to interact with your batch of data and generate an Expectation Suite. Every time you evaluate an Expectation with validator.expect_*, it is immediately Validated against your data. This instant feedback helps you identify unexpected data and removes the guesswork from data exploration. The Expectation configuration is stored in the Validator. When you are finished running the Expectations on the dataset, you can use validator.save_expectation_suite() to save all of your Expectation configurations into an Expectation Suite for later use in a checkpoint.  Run the following command to create the suite and get a Validator:  python name=\"tests/integration/docusaurus/deployment_patterns/databricks_deployment_patterns_dataframe_python_configs.py get validator\"  Run the following command to use the Validator to add a few Expectations:  python name=\"tests/integration/docusaurus/deployment_patterns/databricks_deployment_patterns_dataframe_python_configs.py add expectations\"  Run the following command to save your Expectation Suite (all the unique Expectation Configurations from each run of validator.expect_*) to your Expectation Store:  python name=\"tests/integration/docusaurus/deployment_patterns/databricks_deployment_patterns_dataframe_python_configs.py save suite\" Validate your data You'll create and store a Checkpoint for your batch, which you can use to validate and run post-validation actions.  Run the following command to create the Checkpoint configuration that uses your Data Context, passes in your Batch Request (your data) and your Expectation Suite (your tests):  python name=\"tests/integration/docusaurus/deployment_patterns/databricks_deployment_patterns_file_python_configs.py checkpoint config\"  Run the following command to save the Checkpoint:  python name=\"tests/integration/docusaurus/deployment_patterns/databricks_deployment_patterns_file_python_configs.py add checkpoint config\"  Run the following command to run the Checkpoint:  python name=\"tests/integration/docusaurus/deployment_patterns/databricks_deployment_patterns_file_python_configs.py run checkpoint\" Your Checkpoint configuration includes the store_validation_result and update_data_docs actions. The store_validation_result action saves your validation results from the Checkpoint run and allows the results to be persisted for future use. The  update_data_docs action builds Data Docs files for the validations run in the Checkpoint. To learn more about Data validation and customizing Checkpoints, see Validate Data:Overview . To view the full Checkpoint configuration, run: print(checkpoint.get_config().to_yaml_str()). Build and view Data Docs Your Checkpoint contained an UpdateDataDocsAction, so your Data Docs have already been built from the validation you ran and your Data Docs store contains a new rendered validation result. Because you used the DBFS for your Data Docs store, you need to download your Data Docs locally to view them. If you use a different store, you can host your data docs in a place where they can be accessed directly by your organization.  Run the following Databricks CLI command to download your data docs and open the local copy of index.html to view your updated Data Docs: bash databricks fs cp -r dbfs:/great_expectations/uncommitted/data_docs/local_site/ great_expectations/uncommitted/data_docs/local_site/ The displayHTML command is another option for displaying Data Docs in a Databricks notebook. There is a restriction, though, in that clicking a link in the displayed data documents returns an empty page. To view some validation results, use this method. For example: python  html = '/dbfs/great_expectations/uncommitted/data_docs/local_site/index.html' with open(html, \"r\") as f:     data = \"\".join([l for l in f]) displayHTML(data) Next steps Now that you've created and saved a Data Context, Data Source, Data Asset, Expectation Suite, and Checkpoint, see Validate data by running a Checkpoint  to create a script to run the Checkpoint without the need to recreate your Data Assets and Expectations. To move Databricks notebooks to production, see Software Engineering Best Practices With Databricks Notebooks from Databricks.", "Get started with Great Expectations and SQL": " sidebar_label: 'Get started with GX and SQL' title: Get started with Great Expectations and SQL  import Prerequisites from '../../deployment_patterns/components/deployment_pattern_prerequisites.jsx' import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; Use the information provided here to learn how you can use Great Expectations (GX) with a SQL Data Source. The following examples use a PostgreSQL Database. To use GX with PostgreSQL Database, you'll complete the following tasks:  Load data Instantiate a Data Context Create a Data Source and a Data Asset Create an Expectation Suite Validate data using a Checkpoint  The full code used in the following examples is available on GitHub:  postgres_deployment_patterns.py  Prerequisites   A working PostgreSQL Database A working Python environment   Install GX  Run the following command to install GX in your Python environment:  bash   pip install great-expectations  Run the following command to import configuration information that you'll use in the following steps:  python name=\"tests/integration/docusaurus/deployment_patterns/postgres_deployment_patterns.py imports\" Set up GX To avoid configuring external resources, you'll use your local filesystem for your Metadata Stores and Data Docs store. Run the following code to create a Data Context with the default settings: python name=\"tests/integration/docusaurus/deployment_patterns/postgres_deployment_patterns.py set up context\" Connect to your data  Use a connection_string to securely connect to your PostgreSQL instance. For example  python   PG_CONNECTION_STRING = \"postgresql+psycopg2://postgres:@localhost/taxi_db\" Replace the connection string with the connection string for your database. For additional information about other connection methods, see How to configure credentials. In this example, existing New York City taxi cab data is being used.  Run the following command to create a Data Source to represent the data available in your PostgreSQL database:  python name=\"tests/integration/docusaurus/deployment_patterns/postgres_deployment_patterns.py add_datasource\"  Run the following command to create a Data Asset to represent a discrete set of data:   python name=\"tests/integration/docusaurus/deployment_patterns/postgres_deployment_patterns.py add_asset\" In this example, the name of a specific table within your database is used.  Run the following command to build a Batch Request using the Data Asset you configured previously:  python name=\"tests/integration/docusaurus/deployment_patterns/postgres_deployment_patterns.py pg_batch_request\" Create Expectations You'll use a Validator to interact with your batch of data and generate an Expectation Suite. Every time you evaluate an Expectation with validator.expect_*, it is immediately Validated against your data. This instant feedback helps you identify unexpected data and removes the guesswork from data exploration. The Expectation configuration is stored in the Validator. When you are finished running the Expectations on the dataset, you can use validator.save_expectation_suite() to save all of your Expectation configurations into an Expectation Suite for later use in a checkpoint.  Run the following command to create the suite and get a Validator:  python name=\"tests/integration/docusaurus/deployment_patterns/postgres_deployment_patterns.py get validator\"  Run the following command to use the Validator to add a few Expectations:  python name=\"tests/integration/docusaurus/deployment_patterns/postgres_deployment_patterns.py add expectations\"  Run the following command to save your Expectation Suite (all the unique Expectation Configurations from each run of validator.expect_*) to your Expectation Store:  python name=\"tests/integration/docusaurus/deployment_patterns/postgres_deployment_patterns.py save suite\" Validate your data You'll create and store a Checkpoint for your batch, which you can use to validate and run post-validation actions.  Run the following command to create the Checkpoint configuration that uses your Data Context:  python name=\"tests/integration/docusaurus/deployment_patterns/postgres_deployment_patterns.py checkpoint config\"  Run the following command to save the Checkpoint:  python name=\"tests/integration/docusaurus/deployment_patterns/postgres_deployment_patterns.py add checkpoint config\"  Run the following command to run the Checkpoint and pass in your Batch Request (your data) and your Expectation Suite (your tests):  python name=\"tests/integration/docusaurus/deployment_patterns/postgres_deployment_patterns.py run checkpoint\" Your Checkpoint configuration includes the store_validation_result and update_data_docs actions. The store_validation_result action saves your validation results from the Checkpoint run and allows the results to be persisted for future use. The  update_data_docs action builds Data Docs files for the validations run in the Checkpoint. To learn more about Data validation and customizing Checkpoints, see Validate Data: Overview . To view the full Checkpoint configuration, run print(checkpoint.get_config().to_yaml_str()). Build and view Data Docs Your Checkpoint contained an UpdateDataDocsAction, so your Data Docs have already been built from the validation you ran and your Data Docs store contains a new rendered validation result. Run the following command to open your Data Docs and review the results of your Checkpoint run: python context.open_data_docs() Next steps Now that you've created and saved a Data Context, Data Source, Data Asset, Expectation Suite, and Checkpoint, see Validate data by running a Checkpoint  to create a script to run the Checkpoint without the need to recreate your Data Assets and Expectations.", "Quickstart": " sidebar_label: 'Quickstart' title: Quickstart tag: [tutorial, getting started]  import Prerequisites from '/docs/components/_prerequisites.jsx' import PrereqPython from '/docs/components/prerequisites/_python_version.md' import SetupAndInstallGx from '/docs/components/setup/link_lists/_setup_and_install_gx.md' import DataContextInitializeInstantiateSave from '/docs/components/setup/link_lists/_data_context_initialize_instatiate_save.md' Use this quickstart to install GX, connect to sample data, build your first Expectation, validate data, and review the validation results. This is a great place to start if you're new to GX and aren't sure if it's the right solution for you or your organization. If you're using Databricks or SQL to store data, see Get Started with GX and Databricks or Get Started with GX and SQL. :::note Great Expectations Cloud You can use this quickstart with the open source Python version of GX or with Great Expectations Cloud. If you're interested in participating in the Great Expectations Cloud Beta program, or you want to receive progress updates, sign up for the Beta program. ::: :::info Windows Support Windows support for the open source Python version of GX is currently unavailable. If you\u2019re using GX in a Windows environment, you might experience errors or performance issues. ::: Prerequisites   pip An internet browser  Install GX   Run the following command in an empty base directory inside a Python virtual environment: bash title=\"Terminal input\" pip install great_expectations It can take several minutes for the installation to complete.   Run the following Python code to import the great_expectations module: python name=\"tutorials/quickstart/quickstart.py import_gx\" Create a DataContext   Run the following command to import the existing DataContext object: python name=\"tutorials/quickstart/quickstart.py get_context\" Connect to Data   Run the following command to connect to existing .csv data stored in the great_expectations GitHub repository: python name=\"tutorials/quickstart/quickstart.py connect_to_data\" The example code uses the default Data Context Data Source for Pandas to access the .csv data in the file at the specified path.   Create Expectations   Run the following command to create two Expectations: python name=\"tutorials/quickstart/quickstart.py create_expectation\"   The first Expectation uses domain knowledge (the pickup_datetime shouldn't be null), and the second Expectation uses auto=True to detect a range of values in the passenger_count column. Validate data   Run the following command to define a Checkpoint and examine the data to determine if it matches the defined Expectations: python name=\"tutorials/quickstart/quickstart.py create_checkpoint\"   Run the following command to return the Validation results: python name=\"tutorials/quickstart/quickstart.py run_checkpoint\"   Run the following command to view an HTML representation of the Validation results: python name=\"tutorials/quickstart/quickstart.py view_results\"   Related documentation If you're ready to continue your Great Expectations journey, the following topics can help you implement a tailored solution for your specific environment and business requirements:  Install GX in a specific environment with support for a specific source data system. Initialize, instantiate, and save a Data Contex. ", "Add comments to Expectations and display them in Data Docs": " title: Add comments to Expectations and display them in Data Docs import Prerequisites from '../../connecting_to_your_data/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; This guide will help you add descriptive comments (or notes, here used interchangeably) to Expectations and display those comments in Data Docs. In these comments you can add some clarification or motivation to the Expectation definition to help you communicate more clearly with your team about specific Expectations. Markdown is supported in these comments. Prerequisites   A working Great Expectations deployment A Data Context An Expectations Suite   Edit your Expectation Suite bash great_expectations suite edit <your_suite_name> Add comments to specific Expectations For each Expectation you wish to add notes to, add a dictionary to the meta field with the key notes and your comment as the value. Here is an example. python validator.expect_table_row_count_to_be_between(   max_value=1000000, min_value=1,   meta={\"notes\": \"Example notes about this expectation.\"} ) Leads to the following representation in the Data Docs (For Expectation Suite pages, click on the speech bubble to view the comment).  Add styling to your comments (Optional) To add styling to your comments, you can add a format tag. Here are a few examples. A single line of markdown is rendered in red, with any Markdown formatting applied. python validator.expect_column_values_to_not_be_null(   column=\"column_name\",   meta={       \"notes\": {           \"format\": \"markdown\",           \"content\": \"Example notes about this expectation. **Markdown** `Supported`.\"       }   } )  Multiple lines can be rendered by using a list for content; these lines are rendered in black text with any Markdown formatting applied. python validator.expect_column_values_to_not_be_null(   column=\"column_name\",   meta={       \"notes\": {           \"format\": \"markdown\",           \"content\": [               \"Example notes about this expectation. **Markdown** `Supported`.\",               \"Second example note **with** *Markdown*\",           ]       }   } )  You can also change the format to string and single or multiple lines will be formatted similar to the above, but the Markdown formatting will not be applied. python validator.expect_column_values_to_not_be_null(   column=\"column_name\",   meta={       \"notes\": {           \"format\": \"string\",           \"content\": [               \"Example notes about this expectation. **Markdown** `Not Supported`.\",               \"Second example note **without** *Markdown*\",           ]       }   } )  Review your comments in the Expectation Suite overview of your Data Docs You can open your Data Docs by using the .open_data_docs() method of your Data Context, which should be present in the last cell of the Jupyter Notebook you did your editing in.", "Compare two tables with the Onboarding Data Assistant": " title: Compare two tables with the Onboarding Data Assistant import Prerequisites from '../../../guides/connecting_to_your_data/components/prerequisites.jsx'; import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; In this guide, you will utilize a Data Assistant to create an Expectation Suite that can be used to gauge whether two tables are identical. This workflow can be used, for example, to validate migrated data. Prerequisites   A minimum of two configured Datasources and Assets A basic understanding of how to configure Expectation in Great Expectations Completion of the Data Assistants overview   Set-Up In this workflow, we will be making use of the OnboardingDataAssistant to profile against a BatchRequest representing our source data, and validate the resulting suite against a BatchRequest representing our second set of data. To begin, we'll need to import Great Expectations and instantiate our Data Context: python name=\"tests/integration/docusaurus/expectations/advanced/data_assistant_cross_table_comparison.py imports\" :::note Depending on your use-case, workflow, and directory structures, you may need to update you context root directory as follows: python context = gx.get_context(     context_root_dir='/my/context/root/directory/great_expectations' ) ::: Create Batch Requests In order to profile our first table and validate our second table, we need to set up our Batch Requests pointing to each set of data. In this guide, we will use a MySQL Data Source as our source data -- the data we trust to be correct. python name=\"tests/integration/docusaurus/expectations/advanced/data_assistant_cross_table_comparison.py mysql_batch_request\" From this data, we will create an Expectation Suite and use that suite to validate our second table, pulled from a PostgreSQL Data Source. python name=\"tests/integration/docusaurus/expectations/advanced/data_assistant_cross_table_comparison.py pg_batch_request\" Profile source data We can now use the OnboardingDataAssistant to profile our MySQL data defined in the mysql_batch_request above. python name=\"tests/integration/docusaurus/expectations/advanced/data_assistant_cross_table_comparison.py run_assistant\" And use the results from the Data Assistant to build and save an Expectation Suite: python name=\"tests/integration/docusaurus/expectations/advanced/data_assistant_cross_table_comparison.py build_suite\"  exclude_column_names? In the previous example, specific columns were excluded to prevent Expectations from being set against them.  Some SQL dialects handle data types in different ways, and this can cause precision mismatches on some numbers.  In our hypothetical use case these inconsistencies are tolerated, and therefore Expectations are not set against the columns likely to generate the errors.  This is an example of how an Expectation Suite created by the Data Assistant can be customized. For more on these configurations, see our [guide on the `OnboardingDataAssistant](../../../guides/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_onboarding_data_assistant.md).  Checkpoint Set-Up Before we can validate our second table, we need to define a Checkpoint. We will pass both the pg_batch_request and the Expectation Suite defined above to this checkpoint. python name=\"tests/integration/docusaurus/expectations/advanced/data_assistant_cross_table_comparison.py checkpoint_config\" Validation Finally, we can use our Checkpoint to validate that our two tables are identical: python name=\"tests/integration/docusaurus/expectations/advanced/data_assistant_cross_table_comparison.py run_checkpoint\" If we now inspect the results of this Checkpoint (results[\"success\"]), we can see that our Validation was successful! By default, the Checkpoint above also updates your Data Docs, allowing you to further inspect the results of this workflow.   Congratulations!\ud83c\udf89 You've just compared two tables across Datasources! \ud83c\udf89  ", "Create an Expectation Suite with a Custom Profiler": " title: Create an Expectation Suite with a Custom Profiler import Prerequisites from '../../../guides/connecting_to_your_data/components/prerequisites.jsx'; import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; In this tutorial, you will develop hands-on experience with configuring a Custom Profiler Profiler to create an Expectation Suite. You will Profile several Batches of NYC yellow taxi trip data to come up with reasonable estimates for the ranges of Expectations for several numeric columns. Prerequisites   A basic understanding of Metrics in Great Expectations. A basic understanding of Expectation Configurations in Great Expectations. Completion of the overview of Profilers and the  Custom Profilers section.   Create a new Great Expectations project  Create a new directory, called taxi_profiling_tutorial Within this directory, create another directory called data  Download the data  Download this directory of yellow taxi trip csv files from the Great Expectations GitHub repo. You can use a tool like DownGit to do so Move the unzipped directory of csv files into the data directory that you created in Step 1  Create a context and add your Data Source  See How to connect to data on a filesystem using Pandas. Run the following command to add a Pandas Filesystem asset for the taxi data.  python name=\"tests/integration/docusaurus/expectations/advanced/multi_batch_rule_based_profiler_example.py init\" Configure the Custom Profiler  Now, we'll create a new script in the same top-level taxi_profiling_tutorial directory called profiler_script.py. If you prefer, you could open up a Jupyter Notebook and run this there instead. At the top of this file, we will create a new YAML docstring assigned to a variable called profiler_config. This will look similar to the YAML docstring we used above when creating our Data Source. Over the next several steps, we will slowly add lines to this docstring by typing or pasting in the lines below:  ```python  profiler_config = \"\"\" \"\"\" ``` First, we'll add some relevant top level keys (name and config_version) to label our Profiler and associate it with a specific version of the feature. yaml name=\"tests/integration/docusaurus/expectations/advanced/multi_batch_rule_based_profiler_example.py name and config_version\" :::info Config Versioning Note that at the time of writing this document, 1.0 is the only supported config version. ::: Then, we'll add in a Variables key and some variables that we'll use. Next, we'll add a top level rules key, and then the name of your rule: yaml name=\"tests/integration/docusaurus/expectations/advanced/multi_batch_rule_based_profiler_example.py variables and rule name\" After that, we'll add our Domain Builder. In this case, we'll use a TableDomainBuilder, which will indicate that any expectations we build for this Domain will be at the Table level. Each Rule in our Profiler config can only use one Domain Builder. yaml name=\"tests/integration/docusaurus/expectations/advanced/multi_batch_rule_based_profiler_example.py row_count_rule domain_builder\" Next, we'll use a NumericMetricRangeMultiBatchParameterBuilder to get an estimate to use for the min_value and max_value of our expect_table_row_count_to_be_between Expectation. This Parameter Builder will take in a Batch Request consisting of the five Batches prior to our current Batch, and use the row counts of each of those months to get a probable range of row counts that you could use in your ExpectationConfiguration. yaml name=\"tests/integration/docusaurus/expectations/advanced/multi_batch_rule_based_profiler_example.py row_count_rule parameter_builders\" A Rule can have multiple ParameterBuilders if needed, but in our case, we'll only use the one for now. Finally, you would use an ExpectationConfigurationBuilder to actually build your expect_table_row_count_to_be_between Expectation, where the Domain is the Domain returned by your TableDomainBuilder (your entire table), and the min_value and max_value are Parameters returned by your NumericMetricRangeMultiBatchParameterBuilder. yaml name=\"tests/integration/docusaurus/expectations/advanced/multi_batch_rule_based_profiler_example.py row_count_rule expectation_configuration_builders\" You can see here that we use a special $ syntax to reference variables and parameters that have been previously defined in our config. You can see a more thorough description of this syntax in the  docstring for ParameterContainer here.  When we put it all together, here is what our config with our single row_count_rule looks like:  yaml name=\"tests/integration/docusaurus/expectations/advanced/multi_batch_rule_based_profiler_example.py full row_count_rule\" Run the Custom Profiler Now let's use our config to Profile our data and create an Expectation Suite! First, we load the profiler config and instantiate our Profiler, passing in our config and our Data Context python name=\"tests/integration/docusaurus/expectations/advanced/multi_batch_rule_based_profiler_example.py instantiate\" Then we run the profiler and save the result to a variable.  python name=\"tests/integration/docusaurus/expectations/advanced/multi_batch_rule_based_profiler_example.py run\" Now we can print our Expectation Suite so we can see how it looks! python name=\"tests/integration/docusaurus/expectations/advanced/multi_batch_rule_based_profiler_example.py row_count_rule_suite\" Add a Rule for Columns Let's add one more rule to our Profiler config. This Rule will use the DomainBuilder to populate a list of all of the numeric columns in one Batch of taxi data (in this case, the most recent Batch). It will then use our NumericMetricRangeMultiBatchParameterBuilder looking at the five Batches prior to our most recent Batch to get probable ranges for the min and max values for each of those columns. Finally, it will use those ranges to add two ExpectationConfigurations for each of those columns: expect_column_min_to_be_between and expect_column_max_to_be_between. This rule will go directly below our previous rule. As before, we will first add the name of our rule, and then specify the DomainBuilder. yaml name=\"tests/integration/docusaurus/expectations/advanced/multi_batch_rule_based_profiler_example.py column_ranges_rule domain_builder\" In this case, our DomainBuilder configuration is a bit more complex. First, we are using a SimpleSemanticTypeColumnDomainBuilder. This will take a table, and return a list of all columns that match the semantic_type specified - numeric in our case. Then, we need to specify a Batch Request that returns exactly one Batch of data (this is our data_connector_query with index equal to -1). This tells us which Batch to use to get the columns from which we will select our numeric columns. Though we might hope that all our Batches of data have the same columns, in actuality, there might be differences between the Batches, and so we explicitly specify the Batch we want to use here. After this, we specify our ParameterBuilders. This is very similar to the specification in our previous rule, except we will be specifying two NumericMetricRangeMultiBatchParameterBuilders to get a probable range for the min_value and max_value of each of our numeric columns. Thus one ParameterBuilder will take the column.min metric_name, and the other will take the column.max metric_name. yaml name=\"tests/integration/docusaurus/expectations/advanced/multi_batch_rule_based_profiler_example.py column_ranges_rule parameter_builders\" Finally, we'll put together our Domains and Parameters in our ExpectationConfigurationBuilders: yaml name=\"tests/integration/docusaurus/expectations/advanced/multi_batch_rule_based_profiler_example.py column_ranges_rule expectation_configuration_builders\" Putting together our entire config, with both of our Rules, we get: yaml name=\"tests/integration/docusaurus/expectations/advanced/multi_batch_rule_based_profiler_example.py full profiler_config\" And if we re-instantiate our Profiler with our config which now has two rules, and then we re-run the Profiler, we'll have an updated Expectation Suite with a table row count Expectation for our table, and column min and column max Expectations for each of our numeric columns! \ud83d\ude80Congratulations! You have successfully Profiled multi-batch data using a Custom Profiler. Now you can try adding some new Rules, or running your Profiler on some other data (remember to change the BatchRequest in your config)!\ud83d\ude80 Additional Notes To view the full script used in this page, see it on GitHub:  multi_batch_rule_based_profiler_example.py ", "Create Expectations that span multiple Batches using Evaluation Parameters": " title: Create Expectations that span multiple Batches using Evaluation Parameters import Prerequisites from '../../../guides/connecting_to_your_data/components/prerequisites.jsx'; import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; This guide will help you create Expectations that span multiple Batches of data using Evaluation Parameters (see also Evaluation Parameter Stores). This pattern is useful for things like verifying that row counts between tables stay consistent. Prerequisites   A configured Data Context. A configured Data Source (or several Datasources) with a minimum of two Data Assets and an understanding of the basics of Batch Requests. A Expectations Suites for the Data Assets. A working Evaluation Parameter store. The default in-memory Store from great_expectations init can work for this. A working Checkpoint   Import great_expectations and instantiate your Data Context Run the following Python code in a notebook: python name=\"tests/integration/docusaurus/expectations/advanced/how_to_create_expectations_that_span_multiple_batches_using_evaluation_parameters.py get_context\" Instantiate two Validators, one for each Data Asset We'll call one of these Validators the upstream Validator and the other the downstream Validator. Evaluation Parameters will allow us to use Validation Results from the upstream Validator as parameters passed into Expectations on the downstream. python name=\"tests/integration/docusaurus/expectations/advanced/how_to_create_expectations_that_span_multiple_batches_using_evaluation_parameters.py get validators\" Create the Expectation Suite for the upstream Validator python name=\"tests/integration/docusaurus/expectations/advanced/how_to_create_expectations_that_span_multiple_batches_using_evaluation_parameters.py create upstream_expectation_suite\" This suite will be used on the upstream batch. The observed value for number of rows will be used as a parameter in the Expectation Suite for the downstream batch. Disable interactive evaluation for the downstream Validator python name=\"tests/integration/docusaurus/expectations/advanced/how_to_create_expectations_that_span_multiple_batches_using_evaluation_parameters.py disable interactive_evaluation\" Disabling interactive evaluation allows you to declare an Expectation even when it cannot be evaluated immediately. Define an Expectation using an Evaluation Parameter on the downstream Validator python name=\"tests/integration/docusaurus/expectations/advanced/how_to_create_expectations_that_span_multiple_batches_using_evaluation_parameters.py add expectation with evaluation parameter\" The core of this is a $PARAMETER : URN pair. When Great Expectations encounters a $PARAMETER flag during Validation, it will replace the URN with a value retrieved from an Evaluation Parameter Store or Metrics Store (see also How to configure a MetricsStore). When executed in the notebook, this Expectation will generate a Validation Result. Most values will be missing, since interactive evaluation was disabled. python name=\"tests/integration/docusaurus/expectations/advanced/how_to_create_expectations_that_span_multiple_batches_using_evaluation_parameters.py expected_validation_result\" :::warning Your URN must be exactly correct in order to work in production. Unfortunately, successful execution at this stage does not guarantee that the URN is specified correctly and that the intended parameters will be available when executed later. ::: Save your Expectation Suite python name=\"tests/integration/docusaurus/expectations/advanced/how_to_create_expectations_that_span_multiple_batches_using_evaluation_parameters.py save downstream_expectation_suite\" This step is necessary because your $PARAMETER will only function properly when invoked within a Validation operation with multiple Validators. The simplest way to execute such an operation is through a :ref:Validation Operator <reference__core_concepts__validation__validation_operator>, and Validation Operators are configured to load Expectation Suites from Expectation Stores, not memory. Execute a Checkpoint This will execute both validations and pass the evaluation parameter from the upstream validation to the downstream. python name=\"tests/integration/docusaurus/expectations/advanced/how_to_create_expectations_that_span_multiple_batches_using_evaluation_parameters.py run checkpoint\" Rebuild Data Docs and review results in docs You can do this within your notebook by running: python name=\"tests/integration/docusaurus/expectations/advanced/how_to_create_expectations_that_span_multiple_batches_using_evaluation_parameters.py build data docs\" Once your Data Docs rebuild, open them in a browser and navigate to the page for the new Validation Result. If your Evaluation Parameter was executed successfully, you'll see something like this:  If it encountered an error, you'll see something like this. The most common problem is a mis-specified URN name. ", "Dynamically load evaluation parameters from a database": " title: Dynamically load evaluation parameters from a database import Prerequisites from '../../connecting_to_your_data/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; This guide will help you create an Expectation that loads part of its Expectation configuration from a database at runtime. Using a dynamic Evaluation Parameter makes it possible to maintain part of an Expectation Suite in a shared database. Prerequisites   A working deployment of Great Expectations. Credentials for a database to query for dynamic values. A SQL query to return values for your expectation configuration.   Add a new SqlAlchemy Query Store to your Data Context A SqlAlchemy Query Store acts as a bridge that can query a SqlAlchemy-connected database and return the result of the query to be available for an evaluation parameter. Find the stores section in your great_expectations.yml file, and add the following configuration for a new store called \"my_query_store\". You can add and reference multiple Query Stores with different names. By default, query results will be returned as a list. If instead you need a scalar for your expectation, you can specify the return_type yaml name=\"tests/integration/fixtures/query_store/great_expectations/great_expectations.yml my_query_store\" Ensure you have added valid credentials to the config-variables.yml file (replacing the values with your database credentials): yaml name=\"tests/integration/fixtures/query_store/great_expectations/uncommitted/config_variables.yml my_query_store_creds\" In a notebook, get a test Batch of data to use for Validation python name=\"tests/integration/docusaurus/expectations/advanced/how_to_dynamically_load_evaluation_parameters_from_a_database.py get_validator\" Define an Expectation that relies on a dynamic query Great Expectations recognizes several types of Evaluation Parameters that can use advanced features provided by the Data Context. To dynamically load data, we will be using a store-style URN, which starts with urn:great_expectations:stores. The next component of the URN is the name of the store we configured above (my_query_store), and the final component is the name of the query we defined above (unique_passenger_counts): python name=\"tests/integration/docusaurus/expectations/advanced/how_to_dynamically_load_evaluation_parameters_from_a_database.py define expectation\" The SqlAlchemyQueryStore that you configured above will execute the defined query and return the results as the value of the value_set parameter to evaluate your Expectation: python name=\"tests/integration/docusaurus/expectations/advanced/how_to_dynamically_load_evaluation_parameters_from_a_database.py expected_validator_results\"", "'Add custom parameters to Custom Expectations'": " sidebar_label: 'Add custom parameters to Custom Expectations' title: 'Add custom parameters to Custom Expectations' id: add_custom_parameters description: Add custom parameters to Custom Expectations.  Using custom parameters in your Custom Expectations can help you create powerful, business-specific validations and help you optimize your Great Expectations (GX) workflows. Custom parameters should be implemented when you have data dependencies that can\u2019t or shouldn\u2019t be hardcoded. The method of implementing custom parameters is specific to the Custom Expectation class being extended. Implementation All Custom Expectations include a success_keys attribute that is a tuple of strings. The tuple items are the names of the parameters which are available during the evaluation of your Custom Expectation. For example, the success_keys attribute for the ColumnPairMapExpectation appears similar to the following example: python success_keys = (    \"column_A\",    \"column_B\",    \"mostly\", ) The tuple in the example includes the two columns being evaluated and the mostly parameter. These parameters are passed as a part of your ExpectationConfiguration and are available to the Expectation _validate method. Use case In this use case, a condition_value_keys tuple and a condition_domain_keys tuple are added to ColumnPairMapMetrics, MulticolumnMapMetrics, and ColumnMapMetrics. The condition_value_keys tuple supplies the arguments needed to compute your Metric, and the condition_domain_keys tuple defines the domain on which the Metric operates. All parameters being passed to a Metric for use in value or domain keys must also be passed as success_keys in the Expectation class. This is how the tuples appear in the MulticolumnMapMetrics Metric: python condition_domain_keys = (    \"batch_id\",           \"table\",           \"column_list\",           \"row_condition\",           \"condition_parser\",           \"ignore_row_if\",    )    condition_value_keys = (\"sum_total\",) ColumnAggregateMetrics, TableMetrics, and QueryMetrics can similarly define a value_keys tuple or a domain_keys tuple: python value_keys = (\"column\",) domain_keys = (\"query\",) After the attributes are added to the Expectation and the Metric, the custom parameters can be passed into and used within the individual Metric functions. For example, this is how it appears in the ColumnValuesBetween Metric: python classColumnValuesBetween(ColumnMapMetricProvider):    condition_metric_name = \"column_values.between\"    condition_value_keys = (           \"min_value\",           \"max_value\",           \"strict_min\",           \"strict_max\",           \"parse_strings_as_datetimes\",    \"allow_cross_type_comparisons\", ) @column_condition_partial(engine=PandasExecutionEngine)    def_pandas(           cls,           column,           min_value=None,           max_value=None,           strict_min=None,           strict_max=None,           parse_strings_as_datetimes: bool = False,    allow_cross_type_comparisons=None,           **kwargs    ): To view the full code that you would use to pass custom parameters in a Custom Expectation, see expect_column_values_to_be_lat_lon_coordinates_in_range_of_given_point.py. kwarg-based access GX recommends using custom parameters to create validations and optimize workflows. However, you can also use kwarg syntax to define Metrics for your Expectations. An example kwarg implementation is provided in expect_column_values_to_be_lat_lon_coordinates_in_range_of_given_point.py. To use kwarg syntax to define Metrics for your Expectations, you need to set the kwargs with a value_keys tuple first. Default parameters The following table lists the default parameters that are available for each Expectation class. | Expectation class                 | Default parameters           | | ----------------------------------| -------------------------------| | BatchExpectation                  | See BatchExpectation             | | ColumnAggregateExpectation        | See ColumnAggregateExpectation              | | ColumnMapExpectation              | See ColumnMapExpectation              | | RegexBasedColumnMapExpectation    | See RegexBasedColumnMapExpectation              | | SetBasedColumnMapExpectation      | See SetBasedColumnMapExpectation              | | ColumnPairMapExpectation          | See ColumnPairMapExpectation              | | MulticolumnMapExpectation         | See MulticolumnMapExpectation              | | QueryExpectation                  |  See QueryExpectation              |", "Add support for the auto-initializing framework to a custom Expectation": " title: Add support for the auto-initializing framework to a custom Expectation import Prerequisites from '../creating_custom_expectations/components/prerequisites.jsx' Prerequisites   A custom Expectation.   Determine if the auto-initializing framework is appropriate to include in your Expectation Auto-initializing Expectations automate parameter estimation for Expectations, but not all parameters require this kind of estimation.  If your expectation only takes in a Domain (such as the name of a column) then it will not benefit from being configured to work in the auto-initializing framework.  In general, the auto-initializing Expectation framework benefits those Expectations that have numeric ranges which are intended to be descriptive of the data found in a Batch or Batches.  Existing examples of these would be Expectations such as ExpectColumnMeanToBeBetween, ExpectColumnMaxToBeBetween, or ExpectColumnSumToBeBetween. Build a Custom Profiler for your Expectation In order to automate the estimation of parameters, auto-initializing Expectations utilize a Custom Profiler.  You will need to create an appropriate configuration for the Profiler that your Expectation will use.  The easiest way to do this is to modify an existing Profiler configuration. You can find existing Profiler configurations in the source code for any Expectation that works within the auto-initializing framework.  For this example, we will look at the existing configuration for the ExpectColumnMeanToBeBetween Expectation. You can view the source code for this Expectation on our GitHub. Modify variables Key-value pairs defined in the variables portion of a Profiler Configuration will be shared across all of its Rules and Rule components.  This helps you define and keep track of values without having to input them multiple times.  In our example, the variables are:  strict_min: Used by expect_column_mean_to_be_between Expectation. Recognized values are True or False. strict_max: Used by expect_column_mean_to_be_between Expectation. Recognized values are True or False.  false_positive_rate: Used by NumericMetricRangeMultiBatchParameterBuilder. Typically, this will be a float 0 <= 1.0. quantile_statistic_interpolation_method: Used by NumericMetricRangeMultiBatchParameterBuilder, which is used when estimating quantile values (not relevant in our case). Recognized values include auto, nearest, and linear. estimator: Used by NumericMetricRangeMultiBatchParameterBuilder. Recognized values include oneshot, bootstrap, and kde.  n_resamples:  Used by NumericMetricRangeMultiBatchParameterBuilder. Integer values are expected.  include_estimator_samples_histogram_in_details: Used by NumericMetricRangeMultiBatchParameterBuilder. Recognized values are True or False. truncate_values: A value used by the NumericMetricRangeMultiBatchParameterBuilder to specify the [lower_bound, upper_bound] interval, where either boundary is numeric or None. In our case the value is an empty dictionary, and an equivalent configuration would have been truncate_values : { lower_bound: None, upper_bound: None }.  round_decimals : Used by NumericMetricRangeMultiBatchParameterBuilder, and determines how many digits after the decimal point to output (in our case 2).   Modify the domain_builder The DomainBuilder configuration requries a class_name and module_name.  In this example, we will be using the ColumnDomainBuilder which outputs the column of interest (for example: trip_distance in the NYC taxi data) which is then accessed by the ExpectationConfigurationBuilder using the variable $domain.domain_kwargs.column.  class_name: is the name of the DomainBuilder class that is to be used.  Additional Domain Builders are: ColumnDomainBuilder: This DomainBuilder outputs column Domains, which are required by ColumnAggregateExpectations like (expect_column_median_to_be_between). MultiColumnDomainBuilder: This DomainBuilder outputs multicolumn Domains by taking in a column list in the include_column_names parameter. ColumnPairDomainBuilder: This DomainBuilder outputs columnpair domains by taking in a column pair list in the include_column_names parameter. TableDomainBuilder: This DomainBuilder outputs table Domains, which is required by Expectations that act on tables, like (expect_table_row_count_to_equal, or expect_table_columns_to_match_set). MapMetricColumnDomainBuilder: This DomainBuilder allows you to choose columns based on Map Metrics, which give a yes/no answer for individual values or rows.  CategoricalColumnDomainBuilder: This DomainBuilder allows you to choose columns based on their cardinality (number of unique values).   :::note   CategoricalColumnDomainBuilder will take in various cardinality_limit_mode values for cardinality. For a full listing of valid modes, along with the associated values, please refer to the CardinalityLimitMode enum in the source code on our GitHub.   :::   module_name: is great_expectations.rule_based_profiler.domain_builder, which is common for all DomainBuilders.   Modify the ParameterBuilder Our example contains a configuration for one ParamterBuilder, a NumericMetricRangeMultiBatchParameterBuilder.  You can find the other types of ParameterBuilder by browsing the source code in our GitHub.  For the NumericMetricRangeMultiBatchParameterBuilder the configuration key-value pairs consist of:  name: an arbitrary name assigned to this ParameterBuilder configuration. class_name: the name of the class that corresponds to the ParameterBuilder defined by this configuration. module_name: great_expectations.rule_based_profiler.parameter_builder which is the same for all ParameterBuilders. json_serialize: Boolean value that determines whether to convert computed value to JSON prior to saving result.  estimator: choice of the estimation algorithm: \"oneshot\" (one observation), \"bootstrap\" (default), or \"kde\" (kernel density estimation). Value is pulled from $variables.estimator, which is set to \"bootstrap\" in our configuration.   quantile_statistic_interpolation_method:  Applicable for the \"bootstrap\" sampling method. Determines the value of interpolation \"method\" to np.quantile() statistic, which is used for confidence intervals. Value is pulled from $variables.quantile_statistic_interpolation_method, which is set to \"auto\" in our configuration. enforce_numeric_metric: used in MetricConfiguration to ensure that metric computations return numeric values. Set to True.  n_resamples: Applicable for the \"bootstrap\" and \"kde\" sampling methods -- if omitted (default), then 9999 is used.  Value is pulled from $variables.n_resamples, which is set to 9999 in our configuration. round_decimals: User-configured non-negative integer indicating the number of decimals of the rounding precision of the computed parameter values (i.e., min_value, max_value) prior to packaging them on output.  If omitted, then no rounding is performed, unless the computed value is already an integer. Value is pulled from $variables.round_decimals which is 2 in our configuration. reduce_scalar_metric: If True (default), then reduces computation of 1-dimensional metric to scalar value. This value is set to True. include_estimator_samples_histogram_in_details: For the \"bootstrap\" sampling method -- if True, then add 10-bin histogram of bootstraps to \"details\"; otherwise, omit this information (default). Value pulled from $variables.include_estimator_samples_histogram_in_details, which is False in our configuration. truncate_values: User-configured directive for whether or not to allow the computed parameter values (i.e.,lower_bound, upper_bound) to take on values outside the specified bounds when packaged on output. Value pulled from $variables.truncate_values, which is None in our configuration. false_positive_rate: User-configured fraction between 0 and 1 expressing desired false positive rate for identifying unexpected values as judged by the upper- and lower- quantiles of the observed metric data. Value pulled from $variables.false_positive_rate and is 0.05 in our configuration. replace_nan_with_zero: If False, then if the computed metric gives NaN, then exception is raised; otherwise, if True (default), then if the computed metric gives NaN, then it is converted to the 0.0 (float) value. Set to True in our configuration. metric_domain_kwargs: Domain values for ParameteBuilder. Pulled from $domain.domain_kwargs, and is empty in our configuration.  Modify the expectation_configuration_builders Our Configuration contains 1 ExpectationConfigurationBuilder, for the expect_column_mean_to_be_between Expectation type.  The ExpectationConfigurationBuilder configuration requires a expectation_type, class_name and module_name:  expectation_type: expect_column_mean_to_be_between class_name: DefaultExpectationConfigurationBuilder module_name: great_expectations.rule_based_profiler.expectation_configuration_builder which is common for all ExpectationConfigurationBuilders  Also included are:  validation_parameter_builder_configs: Which are a list of ValidationParameterBuilder configurations, and our configuration case contains the ParameterBuilder described in the previous section.   Next are the parameters that are specific to the expect_column_mean_to_be_between Expectation.  column: Pulled from DomainBuilder using the parameter$domain.domain_kwargs.column min_value:  Pulled from the ParameterBuilder using $parameter.mean_range_estimator.value[0] max_value: Pulled from the ParameterBuilder using $parameter.mean_range_estimator.value[1] strict_min: Pulled from `$variables.strict_min, which is False.  strict_max: Pulled from `$variables.strict_max, which is False.   Last is meta which contains details from our parameter_builder.  Assign your configuration to the default_profiler_config class attribute of your Expectation Once you have modified the necessary parts of the Profiler configuration to suit your purposes you will need to assign it to the default_profiler_config class attribute of your Expectation.  If you initially copied the Profiler configuration that you modified from another Expectation that was already set up to work with the auto-initializing framework then you can refer to that Expectation for an example of this. Test your Expectation with auto=True After assigning your Profiler configuration to the default_profiler_config attribute of your Expectation, your Expectation should be able to work in the auto-initializing framework.  Test your expectation with the parameter auto=True.", "Create a Custom Batch Expectation": " title: Create a Custom Batch Expectation import Prerequisites from '../creating_custom_expectations/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; BatchExpectations are one of the most common types of Expectation.  They are evaluated for an entire Batch, and answer a semantic question about the Batch itself. For example, expect_table_column_count_to_equal and expect_table_row_count_to_equal answer how many columns and rows are in your Batch. This guide will walk you through the process of creating your own custom BatchExpectation. Prerequisites   Choose a name for your Expectation First, decide on a name for your own Expectation. By convention, BatchExpectations always start with expect_table_.  For more on Expectation naming conventions, see the Expectations section of the Code Style Guide. Your Expectation will have two versions of the same name: a CamelCaseName and a snake_case_name. For example, this tutorial will use:  ExpectBatchColumnsToBeUnique expect_batch_columns_to_be_unique  Copy and rename the template file By convention, each Expectation is kept in its own python file, named with the snake_case version of the Expectation's name. You can find the template file for a custom BatchExpectation here. Download the file, place it in the appropriate directory, and rename it to the appropriate name. bash  cp batch_expectation_template.py /SOME_DIRECTORY/expect_batch_columns_to_be_unique.py  Where should I put my Expectation file?           During development, you don't actually need to put the file anywhere in particular. It's self-contained, and can be executed anywhere as long as great_expectations is installed.               But to use your new Expectation alongside the other components of Great Expectations, you'll need to make sure the file is in the right place. The right place depends on what you intend to use it for.        If you're building a Custom Expectation for personal use, you'll need to put it in the great_expectations/plugins/expectations folder of your Great Expectations deployment, and import your Custom Expectation from that directory whenever it will be used. When you instantiate the corresponding DataContext, it will automatically make all plugins in the directory available for use. If you're building a Custom Expectation to contribute to the open source project, you'll need to put it in the repo for the Great Expectations library itself. Most likely, this will be within a package within contrib/: great_expectations/contrib/SOME_PACKAGE/SOME_PACKAGE/expectations/. To use these Expectations, you'll need to install the package.            See our  guide on how to use a Custom Expectation for more!        Generate a diagnostic checklist for your Expectation Once you've copied and renamed the template file, you can execute it as follows. bash  python expect_batch_columns_to_be_unique.py The template file is set up so that this will run the Expectation's print_diagnostic_checklist() method. This will run a diagnostic script on your new Expectation, and return a checklist of steps to get it to full production readiness. This guide will walk you through the first five steps, the minimum for a functioning Custom Expectation and all that is required for contribution back to open source at an Experimental level. Completeness checklist for ExpectColumnAggregateToMatchSomeCriteria:   \u2714 Has a valid library_metadata object     Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period     Has at least one positive and negative example case, and all test cases pass     Has core logic and passes tests on at least one Execution Engine     Passes all linting checks ... When in doubt, the next step to implement is the first one that doesn't have a \u2714 next to it. This guide covers the first five steps on the checklist. Change the Expectation class name and add a docstring By convention, your Metric class is defined first in a Custom Expectation. For now, we're going to skip to the Expectation class and begin laying the groundwork for the functionality of your Custom Expectation. Let's start by updating your Expectation's name and docstring. Replace the Expectation class name python name=\"tests/integration/docusaurus/expectations/examples/batch_expectation_template.py ExpectBatchToMeetSomeCriteria class_def\" with your real Expectation class name, in upper camel case: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_batch_columns_to_be_unique.py ExpectBatchColumnsToBeUnique class_def\" You can also go ahead and write a new one-line docstring, replacing python name=\"tests/integration/docusaurus/expectations/examples/batch_expectation_template.py docstring\" with something like: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_batch_columns_to_be_unique.py docstring\" Make sure your one-line docstring begins with \"Expect \" and ends with a period. You'll also need to change the class name at the bottom of the file, by replacing this line: python name=\"tests/integration/docusaurus/expectations/examples/batch_expectation_template.py diagnostics\" with this one: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_batch_columns_to_be_unique.py diagnostics\" Later, you can go back and write a more thorough docstring. See Expectation Docstring Formatting. At this point you can re-run your diagnostic checklist. You should see something like this: ``` $ python expect_batch_columns_to_be_unique.py Completeness checklist for ExpectBatchColumnsToBeUnique:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period     Has at least one positive and negative example case, and all test cases pass     Has core logic and passes tests on at least one Execution Engine     Passes all linting checks ... ``` Congratulations! You're one step closer to implementing a Custom Expectation. Add example cases Next, we're going to search for examples = [] in your file, and replace it with at least two test examples. These examples serve a dual purpose:  They provide test fixtures that Great Expectations can execute automatically via pytest. They help users understand the logic of your Expectation by providing tidy examples of paired input and output. If you contribute your Expectation to open source, these examples will appear in the Gallery.  Your examples will look something like this: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_batch_columns_to_be_unique.py examples\" Here's a quick overview of how to create test cases to populate examples. The overall structure is a list of dictionaries. Each dictionary has two keys:  data: defines the input data of the example as a Batch. In these examples the Batch has three columns (col1, col2 and col3). These columns have 5 rows. (Note: if you define multiple columns, make sure that they have the same number of rows.) tests: a list of test cases to validate against the data frame defined in the corresponding data. title should be a descriptive name for the test case. Make sure to have no spaces. include_in_gallery: This must be set to True if you want this test case to be visible in the Gallery as an example. in contains exactly the parameters that you want to pass in to the Expectation. \"in\": {\"strict\": True} in the example above is equivalent to expect_batch_columns_to_be_unique(strict=True) out is based on the Validation Result returned when executing the Expectation. exact_match_out: if you set exact_match_out=False, then you don\u2019t need to include all the elements of the Validation Result object - only the ones that are important to test.   only_for (optional): the list of backends that the Expectation should use for testing suppress_test_for (optional): the list of backends that the Expectation should not use for testing only_for and suppres_test_for can be specified at the top-level (next to data and tests) or within specific tests (next to title, and so on)  If you run your Expectation file again, you won't see any new checkmarks, as the logic for your Custom Expectation hasn't been implemented yet.  However, you should see that the tests you've written are now being caught and reported in your checklist: ``` $ python expect_batch_columns_to_be_unique.py Completeness checklist for ExpectBatchColumnsToBeUnique:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period ...     Has core logic that passes tests for all applicable Execution Engines and SQL dialects           Only 0 / 2 tests for pandas are passing           Failing: basic_positive_test, basic_negative_test ... ``` :::note For more information on tests and example cases,  see our guide on creating example cases for a Custom Expectation. ::: Implement your Metric and connect it to your Expectation This is the stage where you implement the actual business logic for your Expectation.  To do so, you'll need to implement a function within a Metric class, and link it to your Expectation. By the time your Expectation is complete, your Metric will have functions for all three Execution Engines (Pandas, Spark, and SQLAlchemy) supported by Great Expectations. For now, we're only going to define one. :::note Metrics answer questions about your data posed by your Expectation,  and allow your Expectation to judge whether your data meets your expectations. ::: Your Metric function will have the @metric_value decorator, with the appropriate engine. Metric functions can be as complex as you like, but they're often very short. For example, here's the definition for a Metric function to find the unique columns of a Batch with the PandasExecutionEngine. python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_batch_columns_to_be_unique.py pandas\" :::note The @metric_value decorator allows us to explicitly structure queries and directly access our compute domain.  While this can result in extra roundtrips to your database in some situations, it allows for advanced functionality and customization of your Custom Expectations. ::: This is all that you need to define for now. In the next step, we will implement the method to validate the result of this Metric.  Other parameters   Expectation Success Keys - A tuple consisting of values that must / could be provided by the user and defines how the Expectation evaluates success.       Expectation Default Kwarg Values (Optional) - Default values for success keys and the defined domain, among other values.       Metric Condition Value Keys (Optional) - Contains any additional arguments passed as parameters to compute the Metric.        Next, choose a Metric Identifier for your Metric. By convention, Metric Identifiers for Column Map Expectations start with column..  The remainder of the Metric Identifier simply describes what the Metric computes, in snake case. For this example, we'll use column.custom_max. You'll need to substitute this metric into two places in the code. First, in the Metric class, replace python name=\"tests/integration/docusaurus/expectations/examples/batch_expectation_template.py metric_name\" with python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_batch_columns_to_be_unique.py metric_name\" Second, in the Expectation class, replace python name=\"tests/integration/docusaurus/expectations/examples/batch_expectation_template.py metric_dependencies\" with python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_batch_columns_to_be_unique.py metric_dependencies\" It's essential to make sure to use matching Metric Identifier strings across your Metric class and Expectation class. This is how the Expectation knows which Metric to use for its internal logic. Finally, rename the Metric class name itself, using the camel case version of the Metric Identifier, minus any periods. For example, replace: python name=\"tests/integration/docusaurus/expectations/examples/batch_expectation_template.py BatchMeetsSomeCriteria class_def\" with  python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_batch_columns_to_be_unique.py BatchColumnsUnique class_def\" Validate In this step, we simply need to validate that the results of our Metrics meet our Expectation. The validate method is implemented as _validate(...): python name=\"tests/integration/docusaurus/expectations/examples/batch_expectation_template.py validate\" This method takes a dictionary named metrics, which contains all Metrics requested by your Metric dependencies,  and performs a simple validation against your success keys (i.e. important thresholds) in order to return a dictionary indicating whether the Expectation has evaluated successfully or not. To do so, we'll be accessing our success keys, as well as the result of our previously-calculated Metrics. For example, here is the definition of a _validate(...) method to validate the results of our table.columns.unique Metric against our success keys: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_batch_columns_to_be_unique.py validate\" Running your diagnostic checklist at this point should return something like this: ``` $ python expect_batch_columns_to_be_unique.py Completeness checklist for ExpectBatchColumnsToBeUnique:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period   \u2714 Has at least one positive and negative example case, and all test cases pass   \u2714 Has core logic and passes tests on at least one Execution Engine     Passes all linting checks ... ``` Linting Finally, we need to lint our now-functioning Custom Expectation. Our CI system will test your code using black, and ruff. If you've set up your dev environment, these libraries will already be available to you, and can be invoked from your command line to automatically lint your code: console black <PATH/TO/YOUR/EXPECTATION.py> ruff <PATH/TO/YOUR/EXPECTATION.py> --fix :::info If desired, you can automate this to happen at commit time. See our guidance on linting for more on this process. ::: Once this is done, running your diagnostic checklist should now reflect your Custom Expectation as meeting our linting requirements: ``` $ python expect_batch_columns_to_be_unique.py Completeness checklist for ExpectBatchColumnsToBeUnique:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period   \u2714 Has at least one positive and negative example case, and all test cases pass   \u2714 Has core logic and passes tests on at least one Execution Engine   \u2714 Passes all linting checks ... ```   Congratulations!\ud83c\udf89 You've just built your first Custom Expectation! \ud83c\udf89   9. Contribute (Optional) This guide will leave you with a Custom Expectation sufficient for contribution to Great Expectations at an Experimental level. If you plan to contribute your Expectation to the public open source project, you should update the library_metadata object before submitting your Pull Request. For example: python name=\"tests/integration/docusaurus/expectations/examples/batch_expectation_template.py library_metadata\" would become python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_batch_columns_to_be_unique.py library_metadata\" This is particularly important because we want to make sure that you get credit for all your hard work! :::note For more information on our code standards and contribution, see our guide on Levels of Maturity for Expectations. To view the full script used in this page, see it on GitHub: - expect_batch_columns_to_be_unique.py :::", "Create a Custom Column Aggregate Expectation": " title: Create a Custom Column Aggregate Expectation import Prerequisites from '../creating_custom_expectations/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; ColumnAggregateExpectations are one of the most common types of Expectation.  They are evaluated for a single column, and produce an aggregate Metric, such as a mean, standard deviation, number of unique values, column type, etc. If that Metric meets the conditions you set, the Expectation considers that data valid. This guide will walk you through the process of creating your own custom ColumnAggregateExpectation. Prerequisites   Choose a name for your Expectation First, decide on a name for your own Expectation. By convention, ColumnAggregateExpectations always start with expect_column_.  For more on Expectation naming conventions, see the Expectations section of the Code Style Guide. Your Expectation will have two versions of the same name: a CamelCaseName and a snake_case_name. For example, this tutorial will use:  ExpectColumnMaxToBeBetweenCustom expect_column_max_to_be_between_custom  Copy and rename the template file By convention, each Expectation is kept in its own python file, named with the snake_case version of the Expectation's name. You can find the template file for a custom ColumnAggregateExpectation here. Download the file, place it in the appropriate directory, and rename it to the appropriate name. bash  cp column_aggregate_expectation_template.py /SOME_DIRECTORY/expect_column_max_to_be_between_custom.py  Where should I put my Expectation file?           During development, you don't actually need to put the file anywhere in particular. It's self-contained, and can be executed anywhere as long as great_expectations is installed.               But to use your new Expectation alongside the other components of Great Expectations, you'll need to make sure the file is in the right place. The right place depends on what you intend to use it for.        If you're building a Custom Expectation for personal use, you'll need to put it in the great_expectations/plugins/expectations folder of your Great Expectations deployment, and import your Custom Expectation from that directory whenever it will be used. When you instantiate the corresponding DataContext, it will automatically make all Plugins in the directory available for use. If you're building a Custom Expectation to contribute to the open source project, you'll need to put it in the repo for the Great Expectations library itself. Most likely, this will be within a package within contrib/: great_expectations/contrib/SOME_PACKAGE/SOME_PACKAGE/expectations/. To use these Expectations, you'll need to install the package.            See our  guide on how to use a Custom Expectation for more!        Generate a diagnostic checklist for your Expectation Once you've copied and renamed the template file, you can execute it as follows. bash  python expect_column_max_to_be_between_custom.py The template file is set up so that this will run the Expectation's print_diagnostic_checklist() method. This will run a diagnostic script on your new Expectation, and return a checklist of steps to get it to full production readiness. This guide will walk you through the first five steps, the minimum for a functioning Custom Expectation and all that is required for contribution back to open source at an Experimental level. Completeness checklist for ExpectColumnAggregateToMatchSomeCriteria:   \u2714 Has a valid library_metadata object     Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period     Has at least one positive and negative example case, and all test cases pass     Has core logic and passes tests on at least one Execution Engine     Passes all linting checks ... When in doubt, the next step to implement is the first one that doesn't have a \u2714 next to it. This guide covers the first five steps on the checklist. Change the Expectation class name and add a docstring By convention, your Metric class is defined first in a Custom Expectation. For now, we're going to skip to the Expectation class and begin laying the groundwork for the functionality of your Custom Expectation. Let's start by updating your Expectation's name and docstring. Replace the Expectation class name python name=\"tests/integration/docusaurus/expectations/examples/column_aggregate_expectation_template.py ExpectColumnAggregateToMatchSomeCriteria class_def\" with your real Expectation class name, in upper camel case: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py ExpectColumnMaxToBeBetween class_def\" You can also go ahead and write a new one-line docstring, replacing python name=\"tests/integration/docusaurus/expectations/examples/column_aggregate_expectation_template.py docstring\" with something like: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py docstring\" Make sure your one-line docstring begins with \"Expect \" and ends with a period. You'll also need to change the class name at the bottom of the file, by replacing this line: python name=\"tests/integration/docusaurus/expectations/examples/column_aggregate_expectation_template.py diagnostics\" with this one: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py diagnostics\" Later, you can go back and write a more thorough docstring. See Expectation Docstring Formatting. At this point you can re-run your diagnostic checklist. You should see something like this: ``` $ python expect_column_max_to_be_between_custom.py Completeness checklist for ExpectColumnValuesToBeBetweenCustom:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period     Has at least one positive and negative example case, and all test cases pass     Has core logic and passes tests on at least one Execution Engine     Passes all linting checks ... ``` Congratulations! You're one step closer to implementing a Custom Expectation. Add example cases Next, we're going to search for examples = [] in your file, and replace it with at least two test examples. These examples serve a dual purpose:  They provide test fixtures that Great Expectations can execute automatically via pytest. They help users understand the logic of your Expectation by providing tidy examples of paired input and output. If you contribute your Expectation to open source, these examples will appear in the Gallery.  Your examples will look something like this: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py examples\" Here's a quick overview of how to create test cases to populate examples. The overall structure is a list of dictionaries. Each dictionary has two keys:  data: defines the input data of the example as a table/data frame. In this example the table has one column named x and a second column named y. Both columns have 5 rows. (Note: if you define multiple columns, make sure that they have the same number of rows.) tests: a list of test cases to Validate against the data frame defined in the corresponding data. title should be a descriptive name for the test case. Make sure to have no spaces. include_in_gallery: This must be set to True if you want this test case to be visible in the Gallery as an example. in contains exactly the parameters that you want to pass in to the Expectation. \"in\": {\"column\": \"x\", \"min_value\": 4, \"strict_min\": True} in the example above is equivalent to expect_column_max_to_be_between_custom(column=\"x\", min_value=4, strict_min=True) out is based on the Validation Result returned when executing the Expectation. exact_match_out: if you set exact_match_out=False, then you don\u2019t need to include all the elements of the Validation Result object - only the ones that are important to test.   only_for (optional): the backends that the Expectation should use for testing suppress_test_for (optional): the list of backends that the Expectation should not use for testing only_for and suppres_test_for can be specified at the top-level (next to data and tests) or within specific tests (next to title, and so on) allowed backends include: \"bigquery\", \"mssql\", \"mysql\", \"pandas\", \"postgresql\", \"redshift\", \"snowflake\", \"spark\", \"sqlite\", \"trino\"    If you run your Expectation file again, you won't see any new checkmarks, as the logic for your Custom Expectation hasn't been implemented yet.  However, you should see that the tests you've written are now being caught and reported in your checklist: ``` $ python expect_column_column_max_to_be_between_custom.py Completeness checklist for ExpectColumnValuesToBeBetweenCustom:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period ...     Has core logic that passes tests for all applicable Execution Engines and SQL dialects           Only 0 / 2 tests for pandas are passing           Failing: basic_positive_test, basic_negative_test ...     Passes all linting checks ``` :::note For more information on tests and example cases,  see our guide on creating example cases for a Custom Expectation. ::: Implement your Metric and connect it to your Expectation This is the stage where you implement the actual business logic for your Expectation.  To do so, you'll need to implement a function within a Metric class, and link it to your Expectation. By the time your Expectation is complete, your Metric will have functions for all three Execution Engines (Pandas, Spark, and SQLAlchemy) supported by Great Expectations. For now, we're only going to define one. :::note Metrics answer questions about your data posed by your Expectation,  and allow your Expectation to judge whether your data meets your expectations. ::: Your Metric function will have the @column_aggregate_value decorator, with the appropriate engine. Metric functions can be as complex as you like, but they're often very short. For example, here's the definition for a Metric function to calculate the max of a column using the PandasExecutionEngine. python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py _pandas\" This is all that you need to define for now. In the next step, we will implement the method to validate the results of this Metric.  Other parameters   Expectation Success Keys - A tuple consisting of values that must / could be provided by the user and defines how the Expectation evaluates success.       Expectation Default Kwarg Values (Optional) - Default values for success keys and the defined domain, among other values.       Metric Condition Value Keys (Optional) - Contains any additional arguments passed as parameters to compute the Metric.        Next, choose a Metric Identifier for your Metric. By convention, Metric Identifiers for Column Map Expectations start with column..  The remainder of the Metric Identifier simply describes what the Metric computes, in snake case. For this example, we'll use column.custom_max. You'll need to substitute this metric into two places in the code. First, in the Metric class, replace python name=\"tests/integration/docusaurus/expectations/examples/column_aggregate_expectation_template.py metric_name\" with python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py metric_name\" Second, in the Expectation class, replace python name=\"tests/integration/docusaurus/expectations/examples/column_aggregate_expectation_template.py metric_dependencies\" with python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py metric_dependencies\" It's essential to make sure to use matching Metric Identifier strings across your Metric class and Expectation class. This is how the Expectation knows which Metric to use for its internal logic. Finally, rename the Metric class name itself, using the camel case version of the Metric Identifier, minus any periods. For example, replace: python name=\"tests/integration/docusaurus/expectations/examples/column_aggregate_expectation_template.py ColumnAggregateMatchesSomeCriteria class_def\" with  python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py ColumnCustomMax class_def\" Validate In this step, we simply need to validate that the results of our Metrics meet our Expectation. The validate method is implemented as _validate(...): python name=\"tests/integration/docusaurus/expectations/examples/column_aggregate_expectation_template.py validate\" This method takes a dictionary named metrics, which contains all Metrics requested by your Metric dependencies,  and performs a simple validation against your success keys (i.e. important thresholds) in order to return a dictionary indicating whether the Expectation has evaluated successfully or not. To do so, we'll be accessing our success keys, as well as the result of our previously-calculated Metrics. For example, here is the definition of a _validate(...) method to validate the results of our column.custom_max Metric against our success keys: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py _validate\" Running your diagnostic checklist at this point should return something like this: ``` $ python expect_column_max_to_be_between_custom.py Completeness checklist for ExpectColumnMaxToBeBetweenCustom:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period   \u2714 Has at least one positive and negative example case, and all test cases pass   \u2714 Has core logic and passes tests on at least one Execution Engine     Passes all linting checks ... ``` Linting Finally, we need to lint our now-functioning Custom Expectation. Our CI system will test your code using black, and ruff. If you've set up your dev environment, these libraries will already be available to you, and can be invoked from your command line to automatically lint your code: console black <PATH/TO/YOUR/EXPECTATION.py> ruff <PATH/TO/YOUR/EXPECTATION.py> --fix :::info If desired, you can automate this to happen at commit time. See our guidance on linting for more on this process. ::: Once this is done, running your diagnostic checklist should now reflect your Custom Expectation as meeting our linting requirements: ``` $ python expect_column_max_to_be_between_custom.py Completeness checklist for ExpectColumnMaxToBeBetweenCustom:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period   \u2714 Has at least one positive and negative example case, and all test cases pass   \u2714 Has core logic and passes tests on at least one Execution Engine   \u2714 Passes all linting checks ... ```   Congratulations!\ud83c\udf89 You've just built your first Custom Expectation! \ud83c\udf89   Contribute (Optional) This guide will leave you with a Custom Expectation sufficient for contribution to Great Expectations at an Experimental level. If you plan to contribute your Expectation to the public open source project, you should update the library_metadata object before submitting your Pull Request. For example: python name=\"tests/integration/docusaurus/expectations/examples/column_aggregate_expectation_template.py library_metadata\" would become python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py library_metadata\" This is particularly important because we want to make sure that you get credit for all your hard work! :::note For more information on our code standards and contribution, see our guide on Levels of Maturity for Expectations. To view the full script used in this page, see it on GitHub: - expect_column_max_to_be_between_custom.py :::", "Create a Custom Column Map Expectation": " title: Create a Custom Column Map Expectation import Prerequisites from '../creating_custom_expectations/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; ColumnMapExpectations are one of the most common types of Expectation. They are evaluated for a single column and ask a yes/no question for every row in that column. Based on the result, they then calculate the percentage of rows that gave a positive answer. If the percentage is high enough, the Expectation considers that data valid. This guide will walk you through the process of creating a custom ColumnMapExpectation. Prerequisites   Choose a name for your Expectation First, decide on a name for your own Expectation. By convention, ColumnMapExpectations always start with expect_column_values_. You can see other naming conventions in the Expectations section  of the code Style Guide. Your Expectation will have two versions of the same name: a CamelCaseName and a snake_case_name. For example, this tutorial will use:  ExpectColumnValuesToEqualThree expect_column_values_to_equal_three  Copy and rename the template file By convention, each Expectation is kept in its own python file, named with the snake_case version of the Expectation's name. You can find the template file for a custom ColumnMapExpectation here. Download the file, place it in the appropriate directory, and rename it to the appropriate name. bash cp column_map_expectation_template.py /SOME_DIRECTORY/expect_column_values_to_equal_three.py  Where should I put my Expectation file?           During development, you don't actually need to put the file anywhere in particular. It's self-contained, and can be executed anywhere as long as great_expectations is installed.               But to use your new Expectation alongside the other components of Great Expectations, you'll need to make sure the file is in the right place. The right place depends on what you intend to use it for.        If you're building a Custom Expectation for personal use, you'll need to put it in the great_expectations/plugins/expectations folder of your Great Expectations deployment, and import your Custom Expectation from that directory whenever it will be used. When you instantiate the corresponding DataContext, it will automatically make all Plugins in the directory available for use. If you're building a Custom Expectation to contribute to the open source project, you'll need to put it in the repo for the Great Expectations library itself. Most likely, this will be within a package within contrib/: great_expectations/contrib/SOME_PACKAGE/SOME_PACKAGE/expectations/. To use these Expectations, you'll need to install the package.            See our  guide on how to use a Custom Expectation for more!        Generate a diagnostic checklist for your Expectation Once you've copied and renamed the template file, you can execute it as follows. bash python expect_column_values_to_equal_three.py The template file is set up so that this will run the Expectation's print_diagnostic_checklist() method. This will run a diagnostic script on your new Expectation, and return a checklist of steps to get it to full production readiness. Completeness checklist for ExpectColumnValuesToMatchSomeCriteria:   \u2714 Has a valid library_metadata object     Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period     Has at least one positive and negative example case, and all test cases pass     Has core logic and passes tests on at least one Execution Engine     Passes all linting checks     Has basic input validation and type checking     Has both Statement Renderers: prescriptive and diagnostic     Has core logic that passes tests for all applicable Execution Engines and SQL dialects     Has a robust suite of tests, as determined by a code owner     Has passed a manual review by a code owner for code standards and style guides When in doubt, the next step to implement is the first one that doesn't have a \u2714 next to it. This guide covers the first five steps on the checklist. Change the Expectation class name and add a docstring By convention, your Metric class is defined first in a Custom Expectation. For now, we're going to skip to the Expectation class and begin laying the groundwork for the functionality of your Custom Expectation. Let's start by updating your Expectation's name and docstring. Replace the Expectation class name python name=\"tests/integration/docusaurus/expectations/examples/column_map_expectation_template.py ExpectColumnValuesToMatchSomeCriteria class_def\" with your real Expectation class name, in upper camel case: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_equal_three.py ExpectColumnValuesToEqualThree class_def\" You can also go ahead and write a new one-line docstring, replacing python name=\"tests/integration/docusaurus/expectations/examples/column_map_expectation_template.py docstring\" with something like: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_equal_three.py docstring\" Make sure your one-line docstring begins with \"Expect \" and ends with a period. You'll also need to change the class name at the bottom of the file, by replacing this line: python name=\"tests/integration/docusaurus/expectations/examples/column_map_expectation_template.py diagnostics\" with this one: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_equal_three.py diagnostics\" Later, you can go back and write a more thorough docstring. See Expectation Docstring Formatting. At this point you can re-run your diagnostic checklist. You should see something like this: ``` $ python expect_column_values_to_equal_three.py Completeness checklist for ExpectColumnValuesToEqualThree:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period     Has at least one positive and negative example case, and all test cases pass     Has core logic and passes tests on at least one Execution Engine     Passes all linting checks ... ``` Congratulations! You're one step closer to implementing a Custom Expectation. Add example cases Next, we're going to search for examples = [] in your file, and replace it with at least two test examples. These examples serve a dual purpose:   They provide test fixtures that Great Expectations can execute automatically via pytest.   They help users understand the logic of your Expectation by providing tidy examples of paired input and output. If you contribute your Expectation to open source, these examples will appear in the Gallery.   Your examples will look something like this: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_equal_three.py examples\" Here's a quick overview of how to create test cases to populate examples. The overall structure is a list of dictionaries. Each dictionary has two keys:  data: defines the input data of the example as a table/data frame. In this example the table has one column named all_threes and a second column named some_zeroes. Both columns have 5 rows. (Note: if you define multiple columns, make sure that they have the same number of rows.) tests: a list of test cases to Validate against the data frame defined in the corresponding data. title should be a descriptive name for the test case. Make sure to have no spaces. include_in_gallery: This must be set to True if you want this test case to be visible in the Gallery as an example. in contains exactly the parameters that you want to pass in to the Expectation. \"in\": {\"column\": \"mostly_threes\", \"mostly\": 0.6} in the example above is equivalent to expect_column_values_to_equal_three(column=mostly_threes, mostly=0.6) out is based on the Validation Result returned when executing the Expectation. exact_match_out: if you set exact_match_out=False, then you don\u2019t need to include all the elements of the Validation Result object - only the ones that are important to test.    If you run your Expectation file again, you won't see any new checkmarks, as the logic for your Custom Expectation hasn't been implemented yet.  However, you should see that the tests you've written are now being caught and reported in your checklist: ``` $ python expect_column_values_to_equal_three.py Completeness checklist for ExpectColumnValuesToEqualThree:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period ...     Has core logic that passes tests for all applicable Execution Engines and SQL dialects           Only 0 / 2 tests for pandas are passing           Failing: basic_positive_test, basic_negative_test ...     Passes all linting checks ``` :::note For more information on tests and example cases,  see our guide on how to create example cases for a Custom Expectation. ::: Implement your Metric and connect it to your Expectation This is the stage where you implement the actual business logic for your Expectation.    To do so, you'll need to implement a function within a Metric, and link it to your Expectation. By the time your Expectation is complete, your Metric will have functions for all three Execution Engines (Pandas, Spark, & SQLAlchemy) supported by Great Expectations. For now, we're only going to define one.   :::note Metrics answer questions about your data posed by your Expectation,  and allow your Expectation to judge whether your data meets your expectations. ::: Your Metric function will have the @column_condition_partial decorator, with the appropriate engine. Metric functions can be as complex as you like, but they're often very short. For example, here's the definition for a Metric function to calculate whether values equal 3 using the PandasExecutionEngine. python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_equal_three.py pandas\" This is all that you need to define for now. The ColumnMapMetricProvider and ColumnMapExpectation classes have built-in logic to handle all the machinery of data validation, including standard parameters like mostly, generation of Validation Results, etc.  Other parameters   Expectation Success Keys - A tuple consisting of values that must / could be provided by the user and defines how the Expectation evaluates success.       Expectation Default Kwarg Values (Optional) - Default values for success keys and the defined domain, among other values.       Metric Condition Value Keys (Optional) - Contains any additional arguments passed as parameters to compute the Metric.        Next, choose a Metric Identifier for your Metric. By convention, Metric Identifiers for Column Map Expectations start with column_values.. The remainder of the Metric Identifier simply describes what the Metric computes, in snake case. For this example, we'll use column_values.equal_three. You'll need to substitute this metric into two places in the code. First, in the Metric class, replace python name=\"tests/integration/docusaurus/expectations/examples/column_map_expectation_template.py metric_name\" with python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_equal_three.py metric_name\" Second, in the Expectation class, replace python name=\"tests/integration/docusaurus/expectations/examples/column_map_expectation_template.py map_metric\" with python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_equal_three.py map_metric\" It's essential to make sure to use matching Metric Identifier strings across your Metric class and Expectation class. This is how the Expectation knows which Metric to use for its internal logic. Finally, rename the Metric class name itself, using the camel case version of the Metric Identifier, minus any periods. For example, replace: python name=\"tests/integration/docusaurus/expectations/examples/column_map_expectation_template.py ColumnValuesMatchSomeCriteria class_def\" with  python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_equal_three.py ColumnValuesEqualThree class_def\" Running your diagnostic checklist at this point should return something like this: ``` $ python expect_column_values_to_equal_three.py Completeness checklist for ExpectColumnValuesToEqualThree:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period   \u2714 Has at least one positive and negative example case, and all test cases pass   \u2714 Has core logic and passes tests on at least one Execution Engine     Passes all linting checks ... ``` Linting Finally, we need to lint our now-functioning Custom Expectation. Our CI system will test your code using black, and ruff. If you've set up your dev environment as recommended in the Prerequisites, these libraries will already be available to you, and can be invoked from your command line to automatically lint your code: console black <PATH/TO/YOUR/EXPECTATION.py> ruff <PATH/TO/YOUR/EXPECTATION.py> --fix :::info If desired, you can automate this to happen at commit time. See our guidance on linting for more on this process. ::: Once this is done, running your diagnostic checklist should now reflect your Custom Expectation as meeting our linting requirements: ``` $ python expect_column_values_to_equal_three.py Completeness checklist for ExpectColumnValuesToEqualThree:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period   \u2714 Has at least one positive and negative example case, and all test cases pass   \u2714 Has core logic and passes tests on at least one Execution Engine   \u2714 Passes all linting checks ... ```     Congratulations!\ud83c\udf89 You've just built your first Custom Expectation! \ud83c\udf89     :::note If you've already built a Custom Column Aggregate Expectation, you may notice that we didn't implement a _validate method here. While we have to explicitly create this functionality for Column Aggregate Expectations, Column Map Expectations come with that functionality built in; no extra _validate needed! ::: Contribute (Optional) This guide will leave you with a Custom Expectation sufficient for contribution to Great Expectations at an Experimental level. If you plan to contribute your Expectation to the public open source project, you should update the library_metadata object before submitting your Pull Request. For example: python name=\"tests/integration/docusaurus/expectations/examples/column_map_expectation_template.py library_metadata\" would become python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_equal_three.py library_metadata\" This is particularly important because we want to make sure that you get credit for all your hard work! :::note For more information on our code standards and contribution, see our guide on Levels of Maturity for Expectations. To view the full script used in this page, see it on GitHub: - expect_column_values_to_equal_three.py :::", "Create a Custom Column Pair Map Expectation": " title: Create a Custom Column Pair Map Expectation import Prerequisites from '../creating_custom_expectations/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; ColumnPairMapExpectations are a sub-type of Expectation. They are evaluated for a pair of columns and ask a yes/no question about the row-wise relationship between those two columns. Based on the result, they then calculate the percentage of rows that gave a positive answer. If the percentage is high enough, the Expectation considers that data valid. This guide will walk you through the process of creating a custom ColumnPairMapExpectation. Prerequisites   Choose a name for your Expectation First, decide on a name for your own Expectation. By convention, ColumnPairMapExpectations always start with expect_column_pair_values_. You can see other naming conventions in the Expectations section of the code Style Guide. Your Expectation will have two versions of the same name: a CamelCaseName and a snake_case_name. For example, this tutorial will use:  ExpectColumnPairValuesToHaveADifferenceOfThree expect_column_pair_values_to_have_a_difference_of_three  Copy and rename the template file By convention, each Expectation is kept in its own python file, named with the snake_case version of the Expectation's name. You can find the template file for a custom ColumnPairMapExpectation here. Download the file, place it in the appropriate directory, and rename it to the appropriate name. bash cp column_pair_map_expectation_template.py /SOME_DIRECTORY/expect_column_pair_values_to_have_a_difference_of_three.py  Where should I put my Expectation file?            Within a development environment, you don't need to put the file in a specific folder. The Expectation itself should be self-contained, and can be executed anywhere as long as great_expectations is installed, which is sufficient for development and testing. However, to use your new Expectation alongside the other components of Great Expectations (as one would for production purposes), you'll need to make sure the file is in the right place. Where the right place is will depend on what you intend to use it for.        If you're building a Custom Expectation for personal use, you'll need to put it in the great_expectations/plugins/expectations folder of your Great Expectations deployment, and import your Custom Expectation from that directory whenever it will be used. When you instantiate the corresponding DataContext, it will automatically make all Plugins in the directory available for use. If you're building a Custom Expectation to contribute to the open source project, you'll need to put it in the repo for the Great Expectations library itself. Most likely, this will be within a package within contrib/: great_expectations/contrib/SOME_PACKAGE/SOME_PACKAGE/expectations/. To use these Expectations, you'll need to install the package.            See our  guide on how to use a Custom Expectation for more!        Generate a diagnostic checklist for your Expectation Once you've copied and renamed the template file, you can execute it as follows. bash python expect_column_pair_values_to_have_a_difference_of_three.py The template file is set up so that this will run the Expectation's print_diagnostic_checklist() method. This will run a diagnostic script on your new Expectation, and return a checklist of steps to get it to full production readiness. Completeness checklist for ExpectColumnPairValuesToMatchSomeCriteria:   \u2714 Has a valid library_metadata object     Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period     Has at least one positive and negative example case, and all test cases pass     Has core logic and passes tests on at least one Execution Engine     Passes all linting checks     Has basic input validation and type checking     Has both Statement Renderers: prescriptive and diagnostic     Has core logic that passes tests for all applicable Execution Engines and SQL dialects     Has a robust suite of tests, as determined by a code owner     Has passed a manual review by a code owner for code standards and style guides When in doubt, the next step to implement is the first one that doesn't have a \u2714 next to it. This guide covers the first five steps on the checklist. Change the Expectation class name and add a docstring By convention, your Metric class is defined first in a Custom Expectation. For now, we're going to skip to the Expectation class and begin laying the groundwork for the functionality of your Custom Expectation. Let's start by updating your Expectation's name and docstring. Replace the Expectation class name python name=\"tests/integration/docusaurus/expectations/examples/column_pair_map_expectation_template.py ExpectColumnPairValuesToMatchSomeCriteria class_def\" with your real Expectation class name, in upper camel case: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_pair_values_to_have_a_difference_of_three.py ExpectColumnPairValuesToHaveADifferenceOfThree class_def\" You can also go ahead and write a new one-line docstring, replacing python name=\"tests/integration/docusaurus/expectations/examples/column_pair_map_expectation_template.py docstring\" with something like: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_pair_values_to_have_a_difference_of_three.py completed_docstring\" Make sure your one-line docstring begins with \"Expect \" and ends with a period. You'll also need to change the class name at the bottom of the file, by replacing this line: python name=\"tests/integration/docusaurus/expectations/examples/column_pair_map_expectation_template.py diagnostics\" with this one: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_pair_values_to_have_a_difference_of_three.py completed_print_diagnostic_checklist\" Later, you can go back and write a more thorough docstring. See Expectation Docstring Formatting. At this point you can re-run your diagnostic checklist. You should see something like this: ``` $ python expect_column_pair_values_to_have_a_difference_of_three.py Completeness checklist for ExpectColumnPairValuesToHaveADifferenceOfThree:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period     Has at least one positive and negative example case, and all test cases pass     Has core logic and passes tests on at least one Execution Engine     Passes all linting checks ... ``` Congratulations! You're one step closer to implementing a Custom Expectation. Add example cases Next, we're going to search for examples = [] in your file, and replace it with at least two test examples. These examples serve a dual purpose:   They provide test fixtures that Great Expectations can execute automatically via pytest.   They help users understand the logic of your Expectation by providing tidy examples of paired input and output. If you contribute your Expectation to open source, these examples will appear in the Gallery.   Your examples will look something like this: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_pair_values_to_have_a_difference_of_three.py completed_examples\" Here's a quick overview of how to create test cases to populate examples. The overall structure is a list of dictionaries. Each dictionary has two keys:  data: defines the input data of the example as a table/data frame. In this example the table has one column named col_a and a second column named col_b. Both columns have 6 rows. (Note: when you define multiple columns, make sure that they have the same number of rows.) tests: a list of test cases to Validate against the data frame defined in the corresponding data. title should be a descriptive name for the test case. Make sure to have no spaces. include_in_gallery: This must be set to True if you want this test case to be visible in the Gallery as an example. in contains exactly the parameters that you want to pass in to the Expectation. \"in\": {\"column_A\": \"col_a\", \"column_B\": \"col_b\", \"mostly\": 0.8} in the example above is equivalent to expect_column_pair_values_to_have_a_difference_of_three(column_A=\"col_a\", column_B=\"col_b\", mostly=0.8) out is based on the Validation Result returned when executing the Expectation. exact_match_out: if you set exact_match_out=False, then you don\u2019t need to include all the elements of the Validation Result object - only the ones that are important to test.    If you run your Expectation file again, you won't see any new checkmarks, as the logic for your Custom Expectation hasn't been implemented yet. However, you should see that the tests you've written are now being caught and reported in your checklist: ``` $ python expect_column_pair_values_to_have_a_difference_of_three.py Completeness checklist for ExpectColumnPairValuesToHaveADifferenceOfThree:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period ...     Has core logic that passes tests for all applicable Execution Engines and SQL dialects           Only 0 / 2 tests for pandas are passing           Failing: basic_positive_test, basic_negative_test ... ``` :::note For more information on tests and example cases,  see our guide on how to create example cases for a Custom Expectation. ::: Implement your Metric and connect it to your Expectation This is the stage where you implement the actual business logic for your Expectation. To do so, you'll need to implement a function within a Metric, and link it to your Expectation. By the time your Expectation is complete, your Metric will have functions for all three Execution Engines (Pandas, Spark, & SQLAlchemy) supported by Great Expectations. For now, we're only going to define one. :::note Metrics answer questions about your data posed by your Expectation,  and allow your Expectation to judge whether your data meets your expectations. ::: Your Metric function will have the @column_pair_condition_partial decorator, with the appropriate engine. Metric functions can be as complex as you like, but they're often very short. For example, here's the definition for a Metric function to calculate whether the difference between the values in two columns equals 3 using the PandasExecutionEngine. python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_pair_values_to_have_a_difference_of_three.py _pandas\" This is all that you need to define for now. The ColumnMapMetricProvider and ColumnMapExpectation classes have built-in logic to handle all the machinery of data validation, including standard parameters like mostly, generation of Validation Results, etc.  Other parameters   Expectation Success Keys - A tuple consisting of values that must / could be provided by the user and defines how the Expectation evaluates success.       Expectation Default Kwarg Values (Optional) - Default values for success keys and the defined domain, among other values.       Metric Condition Value Keys (Optional) - Contains any additional arguments passed as parameters to compute the Metric.        Next, choose a Metric Identifier for your Metric. By convention, Metric Identifiers for Column Pair Map Expectations start with column_pair_values.. The remainder of the Metric Identifier simply describes what the Metric computes, in snake case. For this example, we'll use column_pair_values.diff_three. You'll need to substitute this metric into two places in the code. First, in the Metric class, replace python name=\"tests/integration/docusaurus/expectations/examples/column_pair_map_expectation_template.py metric_name\" with python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_pair_values_to_have_a_difference_of_three.py condition_metric_name\" Second, in the Expectation class, replace python name=\"tests/integration/docusaurus/expectations/examples/column_pair_map_expectation_template.py map_metric\" with python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_pair_values_to_have_a_difference_of_three.py complete_map_metric\" It's essential to make sure to use matching Metric Identifier strings across your Metric class and Expectation class. This is how the Expectation knows which Metric to use for its internal logic. Finally, rename the Metric class name itself, using the camel case version of the Metric Identifier, minus any periods. For example, replace: python name=\"tests/integration/docusaurus/expectations/examples/column_pair_map_expectation_template.py ColumnPairValuesMatchSomeCriteria class_def\" with python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_pair_values_to_have_a_difference_of_three.py ColumnPairValuesDiffThree class_def\" Running your diagnostic checklist at this point should return something like this: ``` $ python expect_column_pair_values_to_have_a_difference_of_three.py Completeness checklist for ExpectColumnPairValuesToHaveADifferenceOfThree:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period   \u2714 Has at least one positive and negative example case, and all test cases pass   \u2714 Has core logic and passes tests on at least one Execution Engine     Passes all linting checks ... ``` Linting Finally, we need to lint our now-functioning Custom Expectation. Our CI system will test your code using black, and ruff. If you've set up your dev environment as recommended in the Prerequisites, these libraries will already be available to you, and can be invoked from your command line to automatically lint your code: console black <PATH/TO/YOUR/EXPECTATION.py> ruff <PATH/TO/YOUR/EXPECTATION.py> --fix :::info If desired, you can automate this to happen at commit time. See our guidance on linting for more on this process. ::: Once this is done, running your diagnostic checklist should now reflect your Custom Expectation as meeting our linting requirements: ``` $ python expect_column_pair_values_to_have_a_difference_of_three.py Completeness checklist for ExpectColumnPairValuesToHaveADifferenceOfThree:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period   \u2714 Has at least one positive and negative example case, and all test cases pass   \u2714 Has core logic and passes tests on at least one Execution Engine   \u2714 Passes all linting checks ... ```   Congratulations!\ud83c\udf89 You've just built your first Custom ColumnPairMapExpectation! \ud83c\udf89   :::note If you've already built a Custom Column Aggregate Expectation, you may notice that we didn't implement a _validate method here. While we have to explicitly create this functionality for Column Aggregate Expectations, Column Map Expectations come with that functionality built in; no extra _validate needed! ::: Contribute (Optional) This guide will leave you with a Custom Expectation sufficient for contribution to Great Expectations at an Experimental level. If you plan to contribute your Expectation to the public open source project, you should update the library_metadata object before submitting your Pull Request. For example: python name=\"tests/integration/docusaurus/expectations/examples/column_pair_map_expectation_template.py library_metadata\" would become python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_pair_values_to_have_a_difference_of_three.py completed_library_metadata\" This is particularly important because we want to make sure that you get credit for all your hard work! :::note For more information on our code standards and contribution, see our guide on Levels of Maturity for Expectations. To view the full script used in this page, see it on GitHub: - expect_column_pair_values_to_have_a_difference_of_three.py :::", "Create a Custom Multicolumn Map Expectation": " title: Create a Custom Multicolumn Map Expectation import Prerequisites from '../creating_custom_expectations/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; MulticolumnMapExpectations are a sub-type of Expectation. They are evaluated for a set of columns and ask a yes/no question about the row-wise relationship between those columns. Based on the result, they then calculate the percentage of rows that gave a positive answer. If the percentage is high enough, the Expectation considers that data valid. This guide will walk you through the process of creating a custom MulticolumnMapExpectation. Prerequisites   Choose a name for your Expectation First, decide on a name for your own Expectation. By convention, MulticolumnMapExpectations always start with expect_multicolumn_values_. You can see other naming conventions in the Expectations section of the code Style Guide. Your Expectation will have two versions of the same name: a CamelCaseName and a snake_case_name. For example, this tutorial will use:  ExpectMulticolumnValuesToBeMultiplesOfThree expect_multicolumn_values_to_be_multiples_of_three  Copy and rename the template file By convention, each Expectation is kept in its own python file, named with the snake_case version of the Expectation's name. You can find the template file for a custom MulticolumnMapExpectation here. Download the file, place it in the appropriate directory, and rename it to the appropriate name. bash cp multicolumn_map_expectation_template.py /SOME_DIRECTORY/expect_multicolumn_values_to_be_multiples_of_three.py  Where should I put my Expectation file?            Within a development environment, you don't need to put the file in a specific folder. The Expectation itself should be self-contained, and can be executed anywhere as long as great_expectations is installed, which is sufficient for development and testing. However, to use your new Expectation alongside the other components of Great Expectations (as one would for production purposes), you'll need to make sure the file is in the right place. Where the right place is will depend on what you intend to use it for.        If you're building a Custom Expectation for personal use, you'll need to put it in the great_expectations/plugins/expectations folder of your Great Expectations deployment, and import your Custom Expectation from that directory whenever it will be used. When you instantiate the corresponding DataContext, it will automatically make all Plugins in the directory available for use. If you're building a Custom Expectation to contribute to the open source project, you'll need to put it in the repo for the Great Expectations library itself. Most likely, this will be within a package within contrib/: great_expectations/contrib/SOME_PACKAGE/SOME_PACKAGE/expectations/. To use these Expectations, you'll need to install the package.            See our  guide on how to use a Custom Expectation for more!        Generate a diagnostic checklist for your Expectation Once you've copied and renamed the template file, you can execute it as follows. bash python expect_multicolumn_values_to_be_multiples_of_three.py The template file is set up so that this will run the Expectation's print_diagnostic_checklist() method. This will run a diagnostic script on your new Expectation, and return a checklist of steps to get it to full production readiness. Completeness checklist for ExpectMulticolumnValuesToMatchSomeCriteria:   \u2714 Has a valid library_metadata object     Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period     Has at least one positive and negative example case, and all test cases pass     Has core logic and passes tests on at least one Execution Engine     Passes all linting checks     Has basic input validation and type checking     Has both Statement Renderers: prescriptive and diagnostic     Has core logic that passes tests for all applicable Execution Engines and SQL dialects     Has a robust suite of tests, as determined by a code owner     Has passed a manual review by a code owner for code standards and style guides When in doubt, the next step to implement is the first one that doesn't have a \u2714 next to it. This guide covers the first five steps on the checklist. Change the Expectation class name and add a docstring By convention, your Metric class is defined first in a Custom Expectation. For now, we're going to skip to the Expectation class and begin laying the groundwork for the functionality of your Custom Expectation. Let's start by updating your Expectation's name and docstring. Replace the Expectation class name python name=\"tests/integration/docusaurus/expectations/examples/multicolumn_map_expectation_template.py ExpectMulticolumnValuesToMatchSomeCriteria class_def\" with your real Expectation class name, in upper camel case: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_multicolumn_values_to_be_multiples_of_three.py ExpectMulticolumnValuesToBeMultiplesOfThree class_def\" You can also go ahead and write a new one-line docstring, replacing python name=\"tests/integration/docusaurus/expectations/examples/multicolumn_map_expectation_template.py docstring\" with something like: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_multicolumn_values_to_be_multiples_of_three.py docstring\" Make sure your one-line docstring begins with \"Expect \" and ends with a period. You'll also need to change the class name at the bottom of the file, by replacing this line: python name=\"tests/integration/docusaurus/expectations/examples/multicolumn_map_expectation_template.py diagnostics\" with this one: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_multicolumn_values_to_be_multiples_of_three.py print_diagnostic_checklist\" Later, you can go back and write a more thorough docstring. See Expectation Docstring Formatting. At this point you can re-run your diagnostic checklist. You should see something like this: ``` $ python expect_multicolumn_values_to_be_multiples_of_three.py Completeness checklist for ExpectMulticolumnValuesToBeMultiplesOfThree:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period     Has at least one positive and negative example case, and all test cases pass     Has core logic and passes tests on at least one Execution Engine     Passes all linting checks ... ``` Add example cases Next, we're going to search for examples = [] in your file, and replace it with at least two test examples. These examples serve a dual purpose:   They provide test fixtures that Great Expectations can execute automatically via pytest.   They help users understand the logic of your Expectation by providing tidy examples of paired input and output. If you contribute your Expectation to open source, these examples will appear in the Gallery.   Your examples will look something like this: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_multicolumn_values_to_be_multiples_of_three.py examples\" Here's a quick overview of how to create test cases to populate examples. The overall structure is a list of dictionaries. Each dictionary has two keys:  data: defines the input data of the example as a table/data frame. In this example the table has one column named col_a and a second column named col_b. Both columns have 6 rows. (Note: when you define multiple columns, make sure that they have the same number of rows.) tests: a list of test cases to Validate against the data frame defined in the corresponding data. title should be a descriptive name for the test case. Make sure to have no spaces. include_in_gallery: This must be set to True if you want this test case to be visible in the Gallery as an example. in contains exactly the parameters that you want to pass in to the Expectation. \"in\": {\"column_list\": [\"col_a\", \"col_b\", \"col_c\"], \"mostly\": 0.8} in the example above is equivalent to expect_multicolumn_values_to_be_multiples_of_three(column_list=[\"col_a\", \"col_b\", \"col_c\"], mostly=0.8) out is based on the Validation Result returned when executing the Expectation. exact_match_out: if you set exact_match_out=False, then you don\u2019t need to include all the elements of the Validation Result object - only the ones that are important to test.    If you run your Expectation file again, you won't see any new checkmarks, as the logic for your Custom Expectation hasn't been implemented yet.  However, you should see that the tests you've written are now being caught and reported in your checklist: ``` $ python expect_multicolumn_values_to_be_multiples_of_three.py Completeness checklist for ExpectMulticolumnValuesToBeMultiplesOfThree:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period ...     Has core logic that passes tests for all applicable Execution Engines and SQL dialects           Only 0 / 2 tests for pandas are passing           Failing: basic_positive_test, basic_negative_test ... ``` :::note For more information on tests and example cases,  see our guide on how to create example cases for a Custom Expectation. ::: Implement your Metric and connect it to your Expectation This is the stage where you implement the actual business logic for your Expectation.    To do so, you'll need to implement a function within a Metric, and link it to your Expectation. By the time your Expectation is complete, your Metric will have functions for all three Execution Engines (Pandas, Spark, & SQLAlchemy) supported by Great Expectations. For now, we're only going to define one.   :::note Metrics answer questions about your data posed by your Expectation,  and allow your Expectation to judge whether your data meets your expectations. ::: Your Metric function will have the @multicolumn_condition_partial decorator, with the appropriate engine. Metric functions can be as complex as you like, but they're often very short. For example, here's the definition for a Metric function to calculate whether values across a set of columns are multiples of 3 using the PandasExecutionEngine. python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_multicolumn_values_to_be_multiples_of_three.py _pandas\" This is all that you need to define for now. The MulticolumnMapMetricProvider and MulticolumnMapExpectation classes have built-in logic to handle all the machinery of data validation, including standard parameters like mostly, generation of Validation Results, etc.  Other parameters   Expectation Success Keys - A tuple consisting of values that must / could be provided by the user and defines how the Expectation evaluates success.       Expectation Default Kwarg Values (Optional) - Default values for success keys and the defined domain, among other values.       Metric Condition Value Keys (Optional) - Contains any additional arguments passed as parameters to compute the Metric.        Next, choose a Metric Identifier for your Metric. By convention, Metric Identifiers for Column Pair Map Expectations start with multicolumn_values.. The remainder of the Metric Identifier simply describes what the Metric computes, in snake case. For this example, we'll use multicolumn_values.multiple_three. You'll need to substitute this metric into two places in the code. First, in the Metric class, replace python name=\"tests/integration/docusaurus/expectations/examples/multicolumn_map_expectation_template.py metric_name\" with python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_multicolumn_values_to_be_multiples_of_three.py condition_metric_name\" Second, in the Expectation class, replace python name=\"tests/integration/docusaurus/expectations/examples/multicolumn_map_expectation_template.py map_metric\" with python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_multicolumn_values_to_be_multiples_of_three.py map_metric\" It's essential to make sure to use matching Metric Identifier strings across your Metric class and Expectation class. This is how the Expectation knows which Metric to use for its internal logic. Finally, rename the Metric class name itself, using the camel case version of the Metric Identifier, minus any periods. For example, replace: python name=\"tests/integration/docusaurus/expectations/examples/multicolumn_map_expectation_template.py MulticolumnValuesMatchSomeCriteria class_def\" with  python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_multicolumn_values_to_be_multiples_of_three.py MulticolumnValuesMultipleThree class_def\" Running your diagnostic checklist at this point should return something like this: ``` $ python expect_multicolumn_values_to_be_multiples_of_three.py Completeness checklist for ExpectMulticolumnValuesToBeMultiplesOfThree:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period   \u2714 Has at least one positive and negative example case, and all test cases pass   \u2714 Has core logic and passes tests on at least one Execution Engine     Passes all linting checks ... ``` Linting Finally, we need to lint our now-functioning Custom Expectation. Our CI system will test your code using black, and ruff. If you've set up your dev environment, these libraries will already be available to you, and can be invoked from your command line to automatically lint your code: console black <PATH/TO/YOUR/EXPECTATION.py> ruff <PATH/TO/YOUR/EXPECTATION.py> --fix :::info If desired, you can automate this to happen at commit time. See our guidance on linting for more on this process. ::: Once this is done, running your diagnostic checklist should now reflect your Custom Expectation as meeting our linting requirements: ``` $ python expect_multicolumn_values_to_be_multiples_of_three.py Completeness checklist for ExpectMulticolumnValuesToBeMultiplesOfThree:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period   \u2714 Has at least one positive and negative example case, and all test cases pass   \u2714 Has core logic and passes tests on at least one Execution Engine   \u2714 Passes all linting checks ... ```     Congratulations!\ud83c\udf89 You've just built your first Custom MulticolumnMapExpectation! \ud83c\udf89     :::note If you've already built a Custom Column Aggregate Expectation, you may notice that we didn't implement a _validate method here. While we have to explicitly create this functionality for Column Aggregate Expectations, Multicolumn Map Expectations come with that functionality built in; no extra _validate needed! ::: Contribute (Optional) This guide will leave you with a Custom Expectation sufficient for contribution to Great Expectations at an Experimental level. If you plan to contribute your Expectation to the public open source project, you should update the library_metadata object before submitting your Pull Request. For example: python name=\"tests/integration/docusaurus/expectations/examples/multicolumn_map_expectation_template.py library_metadata\" would become python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_multicolumn_values_to_be_multiples_of_three.py library_metadata\" This is particularly important because we want to make sure that you get credit for all your hard work! :::note For more information on our code standards and contribution, see our guide on Levels of Maturity for Expectations. To view the full script used in this page, see it on GitHub: - expect_multicolumn_values_to_be_multiples_of_three.py :::", "Create Custom Parameterized Expectations": " title: Create Custom Parameterized Expectations import Prerequisites from '../creating_custom_expectations/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; This guide will walk you through the process of creating Parameterized Expectations - very quickly. This method is only available using the new Modular Expectations API in 0.13. Prerequisites   A Parameterized Expectation is a capability unlocked by Modular Expectations. Now that Expectations are structured in class form, it is easy to inherit from these classes and build similar Expectations that are adapted to your own needs. Select an Expectation to inherit from For the purpose of this exercise, we will implement the Expectations expect_column_mean_to_be_positive and expect_column_values_to_be_two_letter_country_code - realistic Expectations of the data that can easily inherit from expect_column_mean_to_be_between and expect_column_values_to_be_in_set respectively. Select default values for your class Our first implementation will be expect_column_mean_to_be_positive. As can be seen in the implementation below, we have chosen to keep our default minimum value at 0, given that we are validating that all our values are positive. Setting the upper bound to None means that no upper bound will be checked \u2013 effectively setting the threshold at \u221e and allowing any positive value. Notice that we do not need to set default_kwarg_values for all kwargs: it is sufficient to set them only for ones for which we would like to set a default value. To keep our implementation simple, we do not override the metric_dependencies or success_keys. python name=\"tests/expectations/core/test_expect_column_mean_to_be_positive.py ExpectColumnMeanToBePositive_class_def\" :::info We could also explicitly override our parent methods to modify the behavior of our new Expectation, for example by updating the configuration validation to require the values we set as defaults not be altered. python name=\"tests/expectations/core/test_expect_column_mean_to_be_positive.py validate_config\" ::: For another example, let's take a look at expect_column_values_to_be_in_set. In this case, we will only be changing our value_set: python name=\"tests/expectations/core/test_expect_column_values_to_be_in_set.py ExpectColumnValuesToBeTwoLetterCountryCode_class_def\"   Congratulations!\ud83c\udf89 You've just built your first Parameterized Custom Expectation! \ud83c\udf89   Contribute (Optional) If you plan to contribute your Expectation to the public open source project, you should include a library_metadata object. For example: python name=\"tests/expectations/core/test_expect_column_mean_to_be_positive.py library_metadata\" This is particularly important because we want to make sure that you get credit for all your hard work! Additionally, you will need to implement some basic examples and test cases before your contribution can be accepted. For guidance on examples and testing, see our guide on implementing examples and test cases. :::note For more information on our code standards and contribution, see our guide on Levels of Maturity for Expectations. :::", "Create a Custom Query Expectation": " title: Create a Custom Query Expectation import Prerequisites from '../creating_custom_expectations/components/prerequisites.jsx' import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; QueryExpectations are a type of Expectation, enabled for SQL and Spark, that enable a higher-complexity type of workflow when compared with core Expectation classes such as ColumnAggregate, ColumnMap, and Table. QueryExpectations allow you to set Expectations against the results of your own custom queries, and make intermediate queries to your database. While this approach can result in extra roundtrips to your database, it can also unlock advanced functionality for your Custom Expectations. They are evaluated for the results of a query, and answer a semantic question about your data returned by that query. For example, expect_queried_table_row_count_to_equal answers how many rows are returned from your table by your query. This guide will walk you through the process of creating your own custom QueryExpectation. Prerequisites   Choose a name for your Expectation First, decide on a name for your own Expectation. By convention, QueryExpectations always start with expect_queried_. All QueryExpectations support the parameterization of your Active Batch; some QueryExpectations also support the parameterization of a Column. This tutorial will detail both approaches.   Your Expectation will have two versions of the same name: a CamelCaseName and a snake_case_name. For example, this tutorial will use:  ExpectQueriedTableRowCountToBe expect_queried_table_row_count_to_be  :::info For more on Expectation naming conventions, see the Expectations section of the Code Style Guide. ::: Copy and rename the template file By convention, each Expectation is kept in its own python file, named with the snake_case version of the Expectation's name. You can find the template file for a custom QueryExpectation here. Download the file, place it in the appropriate directory, and rename it to the appropriate name. bash cp query_expectation_template.py /SOME_DIRECTORY/expect_queried_table_row_count_to_equal.py  Where should I put my Expectation file?            Within a development environment, you don't need to put the file in a specific folder. The Expectation itself should be self-contained, and can be executed anywhere as long as great_expectations is installed, which is sufficient for development and testing. However, to use your new Expectation alongside the other components of Great Expectations (as one would for production purposes), you'll need to make sure the file is in the right place. Where the right place is will depend on what you intend to use it for.        If you're building a Custom Expectation for personal use, you'll need to put it in the great_expectations/plugins/expectations folder of your Great Expectations deployment, and import your Custom Expectation from that directory whenever it will be used. When you instantiate the corresponding DataContext, it will automatically make all Plugins in the directory available for use. If you're building a Custom Expectation to contribute to the open source project, you'll need to put it in the repo for the Great Expectations library itself. Most likely, this will be within a package within contrib/: great_expectations/contrib/SOME_PACKAGE/SOME_PACKAGE/expectations/. To use these Expectations, you'll need to install the package.            See our  guide on how to use a Custom Expectation for more!        Generate a diagnostic checklist for your Expectation Once you've copied and renamed the template file, you can execute it as follows. bash python expect_queried_table_row_count_to_be.py The template file is set up so that this will run the Expectation's print_diagnostic_checklist() method. This will run a diagnostic script on your new Expectation, and return a checklist of steps to get it to full production readiness. Completeness checklist for ExpectQueryToMatchSomeCriteria:   \u2714 Has a valid library_metadata object     Has a docstring, including a one-line short description     Has at least one positive and negative example case, and all test cases pass     Has core logic and passes tests on at least one Execution Engine     Passes all linting checks     Has basic input validation and type checking     Has both Statement Renderers: prescriptive and diagnostic     Has core logic that passes tests for all applicable Execution Engines and SQL dialects     Has a robust suite of tests, as determined by a code owner     Has passed a manual review by a code owner for code standards and style guides When in doubt, the next step to implement is the first one that doesn't have a \u2714 next to it. This guide will walk you through the first five steps, the minimum for a functioning Custom Expectation and all that is required for contribution back to open source at an Experimental level. Change the Expectation class name and add a docstring Now we're going to begin laying the groundwork for the functionality of your Custom Expectation. Let's start by updating your Expectation's name and docstring. Replace the Expectation class name python name=\"tests/integration/docusaurus/expectations/examples/query_expectation_template.py ExpectQueryToMatchSomeCriteria class_def\" with your real Expectation class name, in upper camel case: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_queried_table_row_count_to_be.py ExpectQueriedTableRowCountToBe class_def\" You can also go ahead and write a new one-line docstring, replacing python name=\"tests/integration/docusaurus/expectations/examples/query_expectation_template.py docstring\" with something like: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_queried_table_row_count_to_be.py docstring\" Make sure your one-line docstring begins with \"Expect \" and ends with a period. You'll also need to change the class name at the bottom of the file, by replacing this line: python name=\"tests/integration/docusaurus/expectations/examples/query_expectation_template.py print_diagnostic_checklist\" with this one: python name=\"expect_queried_table_row_count_to_be.py print_diagnostic_checklist\" Later, you can go back and write a more thorough docstring. See Expectation Docstring Formatting. At this point you can re-run your diagnostic checklist. You should see something like this: ``` $ python expect_queried_table_row_count_to_be.py Completeness checklist for ExpectQueriedTableRowCountToBe:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description     Has at least one positive and negative example case, and all test cases pass     Has core logic and passes tests on at least one Execution Engine     Passes all linting checks ... ``` Congratulations! You're one step closer to implementing a Custom Query Expectation.  What about my Metric? If you've built a Custom Expectation before, you may have noticed that the template doesn't contain a Metric class.  While you are still able to create a Custom Metric for your Custom Expectation if needed, the nature of `QueryExpectations` allows us to provide a small number of generic `query.*` Metrics capable of supporting many use-cases.  Add example cases Next, we're going to search for examples = [] in your file, and replace it with at least two test examples. These examples serve a dual purpose:  They provide test fixtures that Great Expectations can execute automatically via pytest. They help users understand the logic of your Expectation by providing tidy examples of paired input and output. If you contribute your Expectation to open source, these examples will appear in the Gallery.  Your examples will look something like this: python name=\"expect_queried_table_row_count_to_be.py examples\" Here's a quick overview of how to create test cases to populate examples. The overall structure is a list of dictionaries. Each dictionary has two keys:  data: defines the input data of the example as a table/data frame. In this example the table test has one column named col1 and a second column named col2. Both columns have 5 rows. (Note: if you define multiple columns, make sure that they have the same number of rows.) tests: a list of test cases to Validate against the data frame defined in the corresponding data. title should be a descriptive name for the test case. Make sure to have no spaces. include_in_gallery: This must be set to True if you want this test case to be visible in the Gallery as an example. in contains exactly the parameters that you want to pass in to the Expectation. \"in\": {\"value\": 5} in the example above is equivalent to expect_queried_table_row_count_to_be(value=5) out is based on the Validation Result returned when executing the Expectation. exact_match_out: if you set exact_match_out=False, then you don\u2019t need to include all the elements of the Validation Result object - only the ones that are important to test.     only_for? only_for is an optional key you can pass to offer more granular control over which backends and SQL dialects your tests are run against.  If you run your Expectation file again, you won't see any new checkmarks, as the logic for your Custom Expectation hasn't been implemented yet. However, you should see that the tests you've written are now being caught and reported in your checklist: ``` $ python expect_queried_table_row_count_to_be.py Completeness checklist for ExpectQueriedTableRowCountToBe:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description ...     Has core logic that passes tests for all applicable Execution Engines and SQL dialects           Only 0 / 2 tests for sqlite are passing             Failing: basic_positive_test, basic_negative_test ... ``` :::note For more information on tests and example cases,  see our guide on how to create example cases for a Custom Expectation. ::: Implement a Query & Connect a Metric to your Expectation The query is the core of a QueryExpectation; this query is what defines the scope of your expectations for your data. To implement your query, replace the query attribute of your Custom Expectation. This: python name=\"tests/integration/docusaurus/expectations/examples/query_expectation_template.py sql_query\" Becomes something like this: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_queried_table_row_count_to_be.py query\" :::warning As noted above, QueryExpectations support parameterization of your Active Batch. We strongly recommend making use of that parameterization as above, by querying against {active_batch}. Not doing so could result in your Custom Expectation unintentionally being run against the wrong data! ::: Metrics for QueryExpectations are a thin wrapper, allowing you to execute that parameterized SQL query with Great Expectations. The results of that query are then validated to judge whether your data meets your expectations. Great Expectations provides a small number of simple, ready-to-use query.* Metrics that can plug into your Custom Expectation, or serve as a basis for your own custom Metrics. :::note Query Metric functions have the @metric_value decorator, with the appropriate engine. The @metric_value decorator allows us to explicitly structure queries and directly access our compute domain. While this can result in extra roundtrips to your database in some situations, it allows for advanced functionality and customization of your Custom Expectations. See an example of a query.table metric here. ::: To connect this Metric to our Custom Expectation, we'll need to include the metric_name for this Metric in our metric_dependencies. This tuple: python name=\"tests/integration/docusaurus/expectations/examples/query_expectation_template.py metric_dependencies\" Becomes: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_queried_table_row_count_to_be.py metric_dependencies\"  Other parameters   Expectation Success Keys - A tuple consisting of values that must / could be provided by the user and defines how the Expectation evaluates success. QueryExpectations must include \"query\" in success_keys.       Expectation Default Kwarg Values (Optional) - Default values for success keys and the defined domain, among other values.       Metric Value Keys (Optional) - Contains any additional arguments passed as parameters to compute the Metric.        Validate In this step, we simply need to validate that the results of our Metrics meet our Expectation. The validate method is implemented as _validate(...): python name=\"expect_queried_table_row_count_to_be.py _validate function signature\" This method takes a dictionary named metrics, which contains all Metrics requested by your Metric dependencies, and performs a simple validation against your success keys (i.e. important thresholds) in order to return a dictionary indicating whether the Expectation has evaluated successfully or not. To do so, we'll be accessing our success keys, as well as the result of our previously-calculated Metrics. For example, here is the definition of a _validate(...) method to validate the results of our query.table Metric against our success keys: python name=\"expect_queried_table_row_count_to_be.py _validate function\" Running your diagnostic checklist at this point should return something like this: ``` $ python expect_queried_table_row_count_to_be.py Completeness checklist for ExpectQueriedTableRowCountToBe:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description   \u2714 Has at least one positive and negative example case, and all test cases pass   \u2714 Has core logic and passes tests on at least one Execution Engine     Passes all linting checks ... ``` Linting Finally, we need to lint our now-functioning Custom Expectation. Our CI system will test your code using black, and ruff. If you've set up your dev environment, these libraries will already be available to you, and can be invoked from your command line to automatically lint your code: console black <PATH/TO/YOUR/EXPECTATION.py> ruff <PATH/TO/YOUR/EXPECTATION.py> --fix :::info If desired, you can automate this to happen at commit time. See our guidance on linting for more on this process. ::: Once this is done, running your diagnostic checklist should now reflect your Custom Expectation as meeting our linting requirements: ``` $ python expect_queried_table_row_count_to_be.py Completeness checklist for ExpectQueriedTableRowCountToBe:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description   \u2714 Has at least one positive and negative example case, and all test cases pass   \u2714 Has core logic and passes tests on at least one Execution Engine   \u2714 Passes all linting checks ... ```   Your Expectation will have two versions of the same name: a CamelCaseName and a snake_case_name. For example, this tutorial will use:  ExpectQueriedColumnValueFrequencyToMeetThreshold expect_queried_column_value_frequency_to_meet_threshold  :::info For more on Expectation naming conventions, see the Expectations section of the Code Style Guide. ::: Copy and rename the template file By convention, each Expectation is kept in its own python file, named with the snake_case version of the Expectation's name. You can find the template file for a custom QueryExpectation here. Download the file, place it in the appropriate directory, and rename it to the appropriate name. bash cp query_expectation_template.py /SOME_DIRECTORY/expect_queried_column_value_frequency_to_meet_threshold.py  Where should I put my Expectation file?            Within a development environment, you don't need to put the file in a specific folder. The Expectation itself should be self-contained, and can be executed anywhere as long as great_expectations is installed, which is sufficient for development and testing. However, to use your new Expectation alongside the other components of Great Expectations (as one would for production purposes), you'll need to make sure the file is in the right place. Where the right place is will depend on what you intend to use it for.        If you're building a Custom Expectation for personal use, you'll need to put it in the great_expectations/plugins/expectations folder of your Great Expectations deployment, and import your Custom Expectation from that directory whenever it will be used. When you instantiate the corresponding DataContext, it will automatically make all Plugins in the directory available for use. If you're building a Custom Expectation to contribute to the open source project, you'll need to put it in the repo for the Great Expectations library itself. Most likely, this will be within a package within contrib/: great_expectations/contrib/SOME_PACKAGE/SOME_PACKAGE/expectations/. To use these Expectations, you'll need to install the package.            See our  guide on how to use a Custom Expectation for more!        Generate a diagnostic checklist for your Expectation Once you've copied and renamed the template file, you can execute it as follows. bash python expect_queried_column_value_frequency_to_meet_threshold.py The template file is set up so that this will run the Expectation's print_diagnostic_checklist() method. This will run a diagnostic script on your new Expectation, and return a checklist of steps to get it to full production readiness. Completeness checklist for ExpectQueriedColumnValueFrequencyToMeetThreshold:   \u2714 Has a valid library_metadata object     Has a docstring, including a one-line short description     Has at least one positive and negative example case, and all test cases pass     Has core logic and passes tests on at least one Execution Engine     Passes all linting checks     Has basic input validation and type checking     Has both Statement Renderers: prescriptive and diagnostic     Has core logic that passes tests for all applicable Execution Engines and SQL dialects     Has a robust suite of tests, as determined by a code owner     Has passed a manual review by a code owner for code standards and style guides When in doubt, the next step to implement is the first one that doesn't have a \u2714 next to it. This guide will walk you through the first five steps, the minimum for a functioning Custom Expectation and all that is required for contribution back to open source at an Experimental level. Change the Expectation class name and add a docstring Now we're going to begin laying the groundwork for the functionality of your Custom Expectation. Let's start by updating your Expectation's name and docstring. Replace the Expectation class name python name=\"tests/integration/docusaurus/expectations/examples/query_expectation_template.py ExpectQueryToMatchSomeCriteria class_def\" with your real Expectation class name, in upper camel case: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_queried_column_value_frequency_to_meet_threshold.py ExpectQueriedColumnValueFrequencyToMeetThreshold class_def\" You can also go ahead and write a new one-line docstring, replacing python name=\"tests/integration/docusaurus/expectations/examples/query_expectation_template.py docstring\" with something like: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_queried_column_value_frequency_to_meet_threshold.py docstring\" You'll also need to change the class name at the bottom of the file, by replacing this line: python name=\"tests/integration/docusaurus/expectations/examples/query_expectation_template.py print_diagnostic_checklist\" with this one: python name=\"expect_queried_column_value_frequency_to_meet_threshold.py print_diagnostic_checklist()\" Later, you can go back and write a more thorough docstring. At this point you can re-run your diagnostic checklist. You should see something like this: ``` $ python expect_queried_column_value_frequency_to_meet_threshold.py Completeness checklist for ExpectQueriedColumnValueFrequencyToMeetThreshold:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description     Has at least one positive and negative example case, and all test cases pass     Has core logic and passes tests on at least one Execution Engine     Passes all linting checks ... ``` Congratulations! You're one step closer to implementing a Custom Query Expectation.  What about my Metric? If you've built a Custom Expectation before, you may have noticed that the template doesn't contain a Metric class.  While you are still able to create a Custom Metric for your Custom Expectation if needed, the nature of `QueryExpectations` allows us to provide a small number of generic `query.*` Metrics capable of supporting many use-cases.  5. Add example cases Next, we're going to search for examples = [] in your file, and replace it with at least two test examples. These examples serve a dual purpose:  They provide test fixtures that Great Expectations can execute automatically via pytest. They help users understand the logic of your Expectation by providing tidy examples of paired input and output. If you contribute your Expectation to open source, these examples will appear in the Gallery.  Your examples will look something like this: python name=\"expect_queried_column_value_frequency_to_meet_threshold.py examples\" Here's a quick overview of how to create test cases to populate examples. The overall structure is a list of dictionaries. Each dictionary has two keys:  data: defines the input data of the example as a table/data frame. In this example the table test has one column named col1 and a second column named col2. Both columns have 5 rows. (Note: if you define multiple columns, make sure that they have the same number of rows.) tests: a list of test cases to Validate against the data frame defined in the corresponding data. title should be a descriptive name for the test case. Make sure to have no spaces. include_in_gallery: This must be set to True if you want this test case to be visible in the Gallery as an example. in contains exactly the parameters that you want to pass in to the Expectation. \"in\": {\"column\": \"col2\", \"value\": \"a\", \"threshold\": 0.6,} in the example above is equivalent to expect_queried_column_value_frequency_to_meet_threshold(column=\"col2\", value=\"a\", threshold=0.6) out is based on the Validation Result returned when executing the Expectation. exact_match_out: if you set exact_match_out=False, then you don\u2019t need to include all the elements of the Validation Result object - only the ones that are important to test.     only_for? only_for is an optional key you can pass to offer more granular control over which backends and SQL dialects your tests are run against.  If you run your Expectation file again, you won't see any new checkmarks, as the logic for your Custom Expectation hasn't been implemented yet. However, you should see that the tests you've written are now being caught and reported in your checklist: ``` $ python expect_queried_column_value_frequency_to_meet_threshold.py Completeness checklist for ExpectQueriedColumnValueFrequencyToMeetThreshold:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description ...     Has core logic that passes tests for all applicable Execution Engines and SQL dialects           Only 0 / 2 tests for sqlite are passing             Failing: basic_positive_test, basic_negative_test ... ``` :::note For more information on tests and example cases,  see our guide on how to create example cases for a Custom Expectation. ::: Implement a Query & Connect a Metric to your Expectation The query is the core of a QueryExpectation; this query is what defines the scope of your expectations for your data. To implement your query, replace the query attribute of your Custom Expectation. This: python name=\"tests/integration/docusaurus/expectations/examples/query_expectation_template.py sql_query\" Becomes something like this: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_queried_column_value_frequency_to_meet_threshold.py query\" :::warning As noted above, QueryExpectations support parameterization of your Active Batch, and can support parameterization of a column name. While parameterizing a column name with {col} is optional and supports flexibility in your Custom Expectations, we strongly recommend making use of batch parameterization, by querying against {active_batch}. Not doing so could result in your Custom Expectation unintentionally being run against the wrong data! ::: Metrics for QueryExpectations are a thin wrapper, allowing you to execute that parameterized SQL query with Great Expectations. The results of that query are then validated to judge whether your data meets your expectations. Great Expectations provides a small number of simple, ready-to-use query.* Metrics that can plug into your Custom Expectation, or serve as a basis for your own custom Metrics. :::note Query Metric functions have the @metric_value decorator, with the appropriate engine. The @metric_value decorator allows us to explicitly structure queries and directly access our compute domain. While this can result in extra roundtrips to your database in some situations, it allows for advanced functionality and customization of your Custom Expectations. See an example of a query.column metric here. ::: To connect this Metric to our Custom Expectation, we'll need to include the metric_name for this Metric in our metric_dependencies. In this case, we'll be using the query.column Metric, allowing us to parameterize both our Active Batch and a column name. This tuple: python name=\"tests/integration/docusaurus/expectations/examples/query_expectation_template.py metric_dependencies\" Becomes: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_queried_column_value_frequency_to_meet_threshold.py metric_dependencies\"  Other parameters   Expectation Success Keys - A tuple consisting of values that must / could be provided by the user and defines how the Expectation evaluates success. QueryExpectations must include \"query\" in success_keys.       Expectation Default Kwarg Values (Optional) - Default values for success keys and the defined domain, among other values.       Metric Value Keys (Optional) - Contains any additional arguments passed as parameters to compute the Metric.        Validate In this step, we simply need to validate that the results of our Metrics meet our Expectation. The validate method is implemented as _validate(...): python name=\"expect_queried_column_value_frequency_to_meet_threshold.py _validate function signature\" This method takes a dictionary named metrics, which contains all Metrics requested by your Metric dependencies, and performs a simple validation against your success keys (i.e. important thresholds) in order to return a dictionary indicating whether the Expectation has evaluated successfully or not. To do so, we'll be accessing our success keys, as well as the result of our previously-calculated Metrics. For example, here is the definition of a _validate(...) method to validate the results of our query.column Metric against our success keys: python name=\"expect_queried_column_value_frequency_to_meet_threshold.py _validate function\" Running your diagnostic checklist at this point should return something like this: ``` $ python expect_queried_column_value_frequency_to_meet_threshold.py Completeness checklist for ExpectQueriedColumnValueFrequencyToMeetThreshold:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description   \u2714 Has at least one positive and negative example case, and all test cases pass   \u2714 Has core logic and passes tests on at least one Execution Engine     Passes all linting checks ... ``` 8. Linting Finally, we need to lint our now-functioning Custom Expectation. Our CI system will test your code using black, and ruff. If you've set up your dev environment as recommended in the Prerequisites, these libraries will already be available to you, and can be invoked from your command line to automatically lint your code: console black <PATH/TO/YOUR/EXPECTATION.py> ruff <PATH/TO/YOUR/EXPECTATION.py> --fix :::info If desired, you can automate this to happen at commit time. See our guidance on linting for more on this process. ::: Once this is done, running your diagnostic checklist should now reflect your Custom Expectation as meeting our linting requirements: ``` $ python expect_queried_column_value_frequency_to_meet_threshold.py Completeness checklist for ExpectQueriedColumnValueFrequencyToMeetThreshold:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description   \u2714 Has at least one positive and negative example case, and all test cases pass   \u2714 Has core logic and passes tests on at least one Execution Engine   \u2714 Passes all linting checks ... ```     Congratulations!\ud83c\udf89 You've just built your first Custom QueryExpectation! \ud83c\udf89   Contribute (Optional) This guide will leave you with a Custom Expectation sufficient for contribution to Great Expectations at an Experimental level. If you plan to contribute your Expectation to the public open source project, you should update the library_metadata object before submitting your Pull Request. For example: python name=\"tests/integration/docusaurus/expectations/examples/query_expectation_template.py library_metadata\" would become python name=\"expect_queried_column_value_frequency_to_meet_threshold.py library_metadata\" This is particularly important because we want to make sure that you get credit for all your hard work! :::note For more information on our code standards and contribution, see our guide on Levels of Maturity for Expectations. To view the full scripts used in this page, see them on GitHub: - expect_queried_table_row_count_to_be.py - expect_queried_column_value_frequency_to_meet_threshold :::", "Create a Custom Regex-Based Column Map Expectation": " title: Create a Custom Regex-Based Column Map Expectation import Prerequisites from '../creating_custom_expectations/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; RegexBasedColumnMapExpectations are a sub-type of ColumnMapExpectation that allow for highly-extensible, regex-powered validation of your data. They are evaluated for a single column and ask a yes/no, regex-based question for every row in that column. Based on the result, they then calculate the percentage of rows that gave a positive answer. If that percentage meets a specified threshold (100% by default), the Expectation considers that data valid. This threshold is configured via the mostly parameter, which can be passed as input to your Custom RegexBasedColumnMapExpectation as a float between 0 and 1. This guide will walk you through the process of creating a Custom RegexBasedColumnMapExpectation. Prerequisites   Choose a name for your Expectation First, decide on a name for your own Expectation. By convention, all ColumnMapExpectations, including RegexBasedColumnMapExpectations, start with expect_column_values_. You can see other naming conventions in the Expectations section  of the code Style Guide. Your Expectation will have two versions of the same name: a CamelCaseName and a snake_case_name. For example, this tutorial will use:  ExpectColumnValuesToOnlyContainVowels expect_column_values_to_only_contain_vowels  Copy and rename the template file By convention, each Expectation is kept in its own python file, named with the snake_case version of the Expectation's name. You can find the template file for a custom RegexBasedColumnMapExpectation here. Download the file, place it in the appropriate directory, and rename it to the appropriate name. bash cp regex_based_column_map_expectation_template.py /SOME_DIRECTORY/expect_column_values_to_only_contain_vowels.py  Where should I put my Expectation file?           During development, you don't actually need to put the file anywhere in particular. It's self-contained, and can be executed anywhere as long as great_expectations is installed.               But to use your new Expectation alongside the other components of Great Expectations, you'll need to make sure the file is in the right place. The right place depends on what you intend to use it for.        If you're building a Custom Expectation for personal use, you'll need to put it in the great_expectations/plugins/expectations folder of your Great Expectations deployment, and import your Custom Expectation from that directory whenever it will be used. When you instantiate the corresponding DataContext, it will automatically make all plugins in the directory available for use. If you're building a Custom Expectation to contribute to the open source project, you'll need to put it in the repo for the Great Expectations library itself. Most likely, this will be within a package within contrib/: great_expectations/contrib/SOME_PACKAGE/SOME_PACKAGE/expectations/. To use these Expectations, you'll need to install the package.            See our  guide on how to use a Custom Expectation for more!        Generate a diagnostic checklist for your Expectation Once you've copied and renamed the template file, you can execute it as follows. bash python expect_column_values_to_only_contain_vowels.py The template file is set up so that this will run the Expectation's print_diagnostic_checklist() method. This will run a diagnostic script on your new Expectation, and return a checklist of steps to get it to full production readiness. Completeness checklist for ExpectColumnValuesToMatchSomeRegex:   \u2714 Has a valid library_metadata object     Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period     Has at least one positive and negative example case, and all test cases pass     Has core logic and passes tests on at least one Execution Engine     Passes all linting checks     Has basic input validation and type checking     Has both Statement Renderers: prescriptive and diagnostic     Has core logic that passes tests for all applicable Execution Engines and SQL dialects     Has a robust suite of tests, as determined by a code owner     Has passed a manual review by a code owner for code standards and style guides When in doubt, the next step to implement is the first one that doesn't have a \u2714 next to it. This guide covers the first five steps on the checklist. Change the Expectation class name and add a docstring Let's start by updating your Expectation's name and docstring. Replace the Expectation class name python name=\"tests/integration/docusaurus/expectations/examples/regex_based_column_map_expectation_template.py ExpectColumnValuesToMatchSomeRegex class_def\" with your real Expectation class name, in upper camel case: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_only_contain_vowels.py ExpectColumnValuesToOnlyContainVowels class_def\" You can also go ahead and write a new one-line docstring, replacing python name=\"tests/integration/docusaurus/expectations/examples/regex_based_column_map_expectation_template.py docstring\" with something like: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_only_contain_vowels.py docstring\" Make sure your one-line docstring begins with \"Expect \" and ends with a period. You'll also need to change the class name at the bottom of the file, by replacing this line: python name=\"tests/integration/docusaurus/expectations/examples/regex_based_column_map_expectation_template.py diagnostics\" with this one: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_only_contain_vowels.py diagnostics\" Later, you can go back and write a more thorough docstring. See Expectation Docstring Formatting. At this point you can re-run your diagnostic checklist. You should see something like this: ``` $ python expect_column_values_to_only_contain_vowels.py Completeness checklist for ExpectColumnValuesToOnlyContainVowels:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period     Has at least one positive and negative example case, and all test cases pass     Has core logic and passes tests on at least one Execution Engine     Passes all linting checks ... ``` Add example cases Next, we're going to search for examples = [] in your file, and replace it with at least two test examples. These examples serve a dual purpose:   They provide test fixtures that Great Expectations can execute automatically via pytest.   They help users understand the logic of your Expectation by providing tidy examples of paired input and output. If you contribute your Expectation to open source, these examples will appear in the Gallery.   Your examples will look something like this: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_only_contain_vowels.py examples\" Here's a quick overview of how to create test cases to populate examples. The overall structure is a list of dictionaries. Each dictionary has two keys:  data: defines the input data of the example as a table/data frame. In this example the table has columns named only_vowels, mixed, longer_vowels, and contains_vowels_but_also_other_stuff. All of these columns have 7 rows. (Note: if you define multiple columns, make sure that they have the same number of rows.) tests: a list of test cases to validate against the data frame defined in the corresponding data. title should be a descriptive name for the test case. Make sure to have no spaces. include_in_gallery: This must be set to True if you want this test case to be visible in the Gallery as an example. in contains exactly the parameters that you want to pass in to the Expectation. \"in\": {\"column\": \"mixed\", \"mostly\": .1} in the example above is equivalent to expect_column_values_to_only_contain_vowels(column=\"mixed\", mostly=0.1) out is based on the Validation Result returned when executing the Expectation. exact_match_out: if you set exact_match_out=False, then you don\u2019t need to include all the elements of the Validation Result object - only the ones that are important to test.    If you run your Expectation file again, you won't see any new checkmarks, as the logic for your Custom Expectation hasn't been implemented yet. However, you should see that the tests you've written are now being caught and reported in your checklist: ``` $ python expect_column_values_to_only_contain_vowels.py Completeness checklist for ExpectColumnValuesToOnlyContainVowels:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period ...     Has core logic that passes tests for all applicable Execution Engines and SQL dialects           Only 0 / 2 tests for pandas are passing           Only 0 / 2 tests for spark are passing           Only 0 / 2 tests for sqlite are passing           Failing: basic_positive_test, basic_negative_test ...     Passes all linting checks ``` :::note For more information on tests and example cases,  see our guide on how to create example cases for a Custom Expectation. ::: Define your regex and connect it to your Expectation This is the stage where you implement the actual business logic for your Expectation. In the case of your Custom RegexBasedColumnMapExpectation, Great Expectations will handle the actual application of the regex to your data. To do this, we replace these: python name=\"tests/integration/docusaurus/expectations/examples/regex_based_column_map_expectation_template.py definition\" with something like this: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_only_contain_vowels.py definition\" For more detail when rendering your Custom Expectation, you can optionally specify the plural form of a Semantic Type you're validating. For example: python name=\"tests/integration/docusaurus/expectations/examples/regex_based_column_map_expectation_template.py plural\" becomes: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_only_contain_vowels.py plural\" Great Expectations will use these values to tell your Custom Expectation to apply your specified regex as a Metric to be utilized in validating your data. This is all that you need to define for now. The RegexBasedColumnMapExpectation class has built-in logic to handle all the machinery of data validation, including standard parameters like mostly, generation of Validation Results, etc.  Other parameters   Expectation Success Keys - A tuple consisting of values that must / could be provided by the user and defines how the Expectation evaluates success.       Expectation Default Kwarg Values (Optional) - Default values for success keys and the defined domain, among other values.        Running your diagnostic checklist at this point should return something like this: ``` $ python expect_column_values_to_only_contain_vowels.py Completeness checklist for ExpectColumnValuesToOnlyContainVowels:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period   \u2714 Has at least one positive and negative example case, and all test cases pass   \u2714 Has core logic and passes tests on at least one Execution Engine     Passes all linting checks ... ``` Linting Finally, we need to lint our now-functioning Custom Expectation. Our CI system will test your code using black, and ruff. If you've set up your dev environment, these libraries will already be available to you, and can be invoked from your command line to automatically lint your code: console black <PATH/TO/YOUR/EXPECTATION.py> ruff <PATH/TO/YOUR/EXPECTATION.py> --fix :::info If desired, you can automate this to happen at commit time. See our guidance on linting for more on this process. ::: Once this is done, running your diagnostic checklist should now reflect your Custom Expectation as meeting our linting requirements: ``` $ python expect_column_values_to_only_contain_vowels.py Completeness checklist for ExpectColumnValuesToOnlyContainVowels:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period   \u2714 Has at least one positive and negative example case, and all test cases pass   \u2714 Has core logic and passes tests on at least one Execution Engine   \u2714 Passes all linting checks ... ```   Congratulations!\ud83c\udf89 You've just built your first Custom Regex-Based Column Map Expectation! \ud83c\udf89   :::note If you've already built a Custom Expectation of a different type, you may notice that we didn't explicitly implement a _validate method or Metric class here. While we have to explicitly create these for other types of Custom Expectations, the RegexBasedColumnMapExpectation class handles Metric creation and result validation implicitly; no extra work needed! ::: Contribute (Optional) This guide will leave you with a Custom Expectation sufficient for contribution back to Great Expectations at an Experimental level. If you plan to contribute your Expectation to the public open source project, you should update the library_metadata object before submitting your Pull Request. For example: python name=\"tests/integration/docusaurus/expectations/examples/regex_based_column_map_expectation_template.py library_metadata\" would become python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_only_contain_vowels.py library_metadata\" This is particularly important because we want to make sure that you get credit for all your hard work! :::note For more information on our code standards and contribution, see our guide on Levels of Maturity for Expectations. To view the full script used in this page, see it on GitHub: - expect_column_values_to_only_contain_vowels.py :::", "Create a Custom Set-Based Column Map Expectation": " title: Create a Custom Set-Based Column Map Expectation import Prerequisites from '../creating_custom_expectations/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; SetBasedColumnMapExpectations are a sub-type of ColumnMapExpectation. They are evaluated for a single column and ask whether each row in that column belongs to the specified set. Based on the result, they then calculate the percentage of rows that gave a positive answer. If that percentage meets a specified threshold (100% by default), the Expectation considers that data valid.  This threshold is configured via the mostly parameter, which can be passed as input to your Custom SetBasedColumnMapExpectation as a float between 0 and 1. This guide will walk you through the process of creating a Custom SetBasedColumnMapExpectation. Prerequisites   Choose a name for your Expectation First, decide on a name for your own Expectation. By convention, all ColumnMapExpectations, including SetBasedColumnMapExpectations, start with expect_column_values_. Your Expectation will have two versions of the same name: a CamelCaseName and a snake_case_name. For example, this tutorial will use:  ExpectColumnValuesToBeInSolfegeScaleSet expect_column_values_to_be_in_solfege_scale_set  Copy and rename the template file By convention, each Expectation is kept in its own python file, named with the snake_case version of the Expectation's name. You can find the template file for a custom SetBasedColumnMapExpectation here. Download the file, place it in the appropriate directory, and rename it to the appropriate name. bash cp set_based_column_map_expectation_template.py /SOME_DIRECTORY/expect_column_values_to_be_in_solfege_scale_set.py  Where should I put my Expectation file?           During development, you don't actually need to put the file anywhere in particular. It's self-contained, and can be executed anywhere as long as great_expectations is installed.               But to use your new Expectation alongside the other components of Great Expectations, you'll need to make sure the file is in the right place. The right place depends on what you intend to use it for.        If you're building a Custom Expectation for personal use, you'll need to put it in the great_expectations/plugins/expectations folder of your Great Expectations deployment, and import your Custom Expectation from that directory whenever it will be used. When you instantiate the corresponding DataContext, it will automatically make all plugins in the directory available for use. If you're building a Custom Expectation to contribute to the open source project, you'll need to put it in the repo for the Great Expectations library itself. Most likely, this will be within a package within contrib/: great_expectations/contrib/SOME_PACKAGE/SOME_PACKAGE/expectations/. To use these Expectations, you'll need to install the package.            See our  guide on how to use a Custom Expectation for more!        Generate a diagnostic checklist for your Expectation Once you've copied and renamed the template file, you can execute it as follows. bash python expect_column_values_to_be_in_solfege_scale_set.py The template file is set up so that this will run the Expectation's print_diagnostic_checklist() method. This will run a diagnostic script on your new Expectation, and return a checklist of steps to get it to full production readiness. Completeness checklist for ExpectColumnValuesToBeInSomeSet:   \u2714 Has a valid library_metadata object     Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period     Has at least one positive and negative example case, and all test cases pass     Has core logic and passes tests on at least one Execution Engine     Passes all linting checks     Has basic input validation and type checking     Has both Statement Renderers: prescriptive and diagnostic     Has core logic that passes tests for all applicable Execution Engines and SQL dialects     Has a robust suite of tests, as determined by a code owner     Has passed a manual review by a code owner for code standards and style guides When in doubt, the next step to implement is the first one that doesn't have a \u2714 next to it. This guide covers the first five steps on the checklist. Change the Expectation class name and add a docstring Let's start by updating your Expectation's name and docstring. Replace the Expectation class name python name=\"tests/integration/docusaurus/expectations/examples/set_based_column_map_expectation_template.py ExpectColumnValuesToBeInSomeSet class_def\" with your real Expectation class name, in upper camel case: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_be_in_solfege_scale_set.py ExpectColumnValuesToBeInSolfegeScaleSet class_def\" You can also go ahead and write a new one-line docstring, replacing python name=\"tests/integration/docusaurus/expectations/examples/set_based_column_map_expectation_template.py docstring\" with something like: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_be_in_solfege_scale_set.py docstring\" Make sure your one-line docstring begins with \"Expect \" and ends with a period. You'll also need to change the class name at the bottom of the file, by replacing this line: python name=\"tests/integration/docusaurus/expectations/examples/set_based_column_map_expectation_template.py diagnostics\" with this one: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_be_in_solfege_scale_set.py diagnostics\" Later, you can go back and write a more thorough docstring. See Expectation Docstring Formatting. At this point you can re-run your diagnostic checklist. You should see something like this: ``` $ python expect_column_values_to_be_in_solfege_scale_set.py Completeness checklist for ExpectColumnValuesToBeInSolfegeScaleSet:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period     Has at least one positive and negative example case, and all test cases pass     Has core logic and passes tests on at least one Execution Engine     Passes all linting checks ... ``` Congratulations! You're one step closer to implementing a Custom Expectation. Add example cases Next, we're going to search for examples = [] in your file, and replace it with at least two test examples. These examples serve a dual purpose:   They provide test fixtures that Great Expectations can execute automatically via pytest.   They help users understand the logic of your Expectation by providing tidy examples of paired input and output. If you contribute your Expectation to open source, these examples will appear in the Gallery.   Your examples will look something like this: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_be_in_solfege_scale_set.py examples\" Here's a quick overview of how to create test cases to populate examples. The overall structure is a list of dictionaries. Each dictionary has two keys:  data: defines the input data of the example as a table/data frame. In this example the table has columns named lowercase_solfege_scale, uppercase_solfege_scale, and mixed. All of these columns have 8 rows. (Note: if you define multiple columns, make sure that they have the same number of rows.) tests: a list of test cases to validate against the data frame defined in the corresponding data. title should be a descriptive name for the test case. Make sure to have no spaces. include_in_gallery: This must be set to True if you want this test case to be visible in the Gallery as an example. in contains exactly the parameters that you want to pass in to the Expectation. \"in\": {\"column\": \"mixed\", \"mostly\": .1} in the example above is equivalent to expect_column_values_to_be_in_solfege_scale_set(column=\"mixed\", mostly=0.1) out is based on the Validation Result returned when executing the Expectation. exact_match_out: if you set exact_match_out=False, then you don\u2019t need to include all the elements of the Validation Result object - only the ones that are important to test.    If you run your Expectation file again, you won't see any new checkmarks, as the logic for your Custom Expectation hasn't been implemented yet. However, you should see that the tests you've written are now being caught and reported in your checklist: ``` $ python expect_column_values_to_be_in_solfege_scale_set.py Completeness checklist for ExpectColumnValuesToBeInSolfegeScaleSet:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period ...     Has core logic that passes tests for all applicable Execution Engines and SQL dialects           Only 0 / 2 tests for pandas are passing           Only 0 / 2 tests for spark are passing           Only 0 / 2 tests for sqlite are passing           Failing: basic_positive_test, basic_negative_test ...     Passes all linting checks ``` :::note For more information on tests and example cases,  see our guide on how to create example cases for a Custom Expectation. ::: Define your set and connect it to your Expectation This is the stage where you implement the actual business logic for your Expectation. In the case of your Custom SetBasedColumnMapExpectation, Great Expectations will handle the actual validation of your data against your set. To do this, we replace these: python name=\"tests/integration/docusaurus/expectations/examples/set_based_column_map_expectation_template.py set\" with something like this: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_be_in_solfege_scale_set.py set\" For more detail when rendering your Custom Expectation, you can optionally specify the semantic name of the set you're validating. For example: python name=\"tests/integration/docusaurus/expectations/examples/set_based_column_map_expectation_template.py semantic_name\" becomes: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_be_in_solfege_scale_set.py semantic_name\" Great Expectations will use these values to tell your Custom Expectation to apply your specified set as a Metric to be utilized in validating your data. This is all that you need to define for now. The SetBasedColumnMapExpectation class has built-in logic to handle all the machinery of data validation, including standard parameters like mostly, generation of Validation Results, etc.  Other parameters   Expectation Success Keys - A tuple consisting of values that must / could be provided by the user and defines how the Expectation evaluates success.       Expectation Default Kwarg Values (Optional) - Default values for success keys and the defined domain, among other values.        Running your diagnostic checklist at this point should return something like this: ``` $ python expect_column_values_to_be_in_solfege_scale_set.py Completeness checklist for ExpectColumnValuesToBeInSolfegeScaleSet:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period   \u2714 Has at least one positive and negative example case, and all test cases pass   \u2714 Has core logic and passes tests on at least one Execution Engine     Passes all linting checks ... ``` Linting Finally, we need to lint our now-functioning Custom Expectation. Our CI system will test your code using black, and ruff. If you've set up your dev environment, these libraries will already be available to you, and can be invoked from your command line to automatically lint your code: console black <PATH/TO/YOUR/EXPECTATION.py> ruff <PATH/TO/YOUR/EXPECTATION.py> --fix :::info If desired, you can automate this to happen at commit time. See our guidance on linting for more on this process. ::: Once this is done, running your diagnostic checklist should now reflect your Custom Expectation as meeting our linting requirements: ``` $ python expect_column_values_to_be_in_solfege_scale_set.py Completeness checklist for ExpectColumnValuesToBeInSolfegeScaleSet:   \u2714 Has a valid library_metadata object   \u2714 Has a docstring, including a one-line short description that begins with \"Expect\" and ends with a period   \u2714 Has at least one positive and negative example case, and all test cases pass   \u2714 Has core logic and passes tests on at least one Execution Engine   \u2714 Passes all linting checks ... ```   Congratulations!\ud83c\udf89 You've just built your first Custom Set-Based Column Map Expectation! \ud83c\udf89   :::note If you've already built a Custom Expectation of a different type, you may notice that we didn't explicitly implement a _validate method or Metric class here. While we have to explicitly create these for other types of Custom Expectations, the SetBasedColumnMapExpectation class handles Metric creation and result validation implicitly; no extra work needed! ::: Contribute (Optional) This guide will leave you with a Custom Expectation sufficient for contribution to Great Expectations at an Experimental level. If you plan to contribute your Expectation to the public open source project, you should update the library_metadata object before submitting your Pull Request. For example: python name=\"tests/integration/docusaurus/expectations/examples/set_based_column_map_expectation_template.py library_metadata\" would become python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_be_in_solfege_scale_set.py library_metadata\" This is particularly important because we want to make sure that you get credit for all your hard work! :::note For more information on our code standards and contribution, see our guide on Levels of Maturity for Expectations. To view the full script used in this page, see it on GitHub: - expect_column_values_to_be_in_solfege_scale_set.py :::", "Use a Custom Expectation": " title: Use a Custom Expectation import Prerequisites from '../creating_custom_expectations/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; Use the information provided here to use Custom Expectations you created or imported from the Great Expectations Experimental Library. Custom Expectations extend the core functionality of Great Expectations for a specific purpose or business need. Often, they are less stable and less mature than the core library. For these reasons they are not available from the core library, and they must be registered and imported when you create an Expectation Suite, and when you define and run a Checkpoint. When you instantiate your Data Context, all plugins in the great_expectations/plugins directory are automatically available, and this allows you to import your Custom Expectation from other locations. Prerequisites   A Custom Expectation or a Custom Expectation from the Great Expectations Experimental Library   Import a custom Expectation you created   Add your Custom Expectation to the great_expectations/plugins/expectations folder of your Great Expectations deployment.   Run a command similar to the following:    ```python from expectations.expect_column_values_to_be_alphabetical import ExpectColumnValuesToBeAlphabetical # ... validator.expect_column_values_to_be_alphabetical(column=\"test\") ```  Import a contributed custom Expectation If you're using a Custom Expectation from the Great Expectations Experimental library, you'll need to import it.    Run pip install great_expectations_experimental.   Run a command similar to the following:    ```python from great_expectations_experimental.expectations.expect_column_values_to_be_alphabetical import ExpectColumnValuesToBeAlphabetical # ... validator.expect_column_values_to_be_alphabetical(column=\"test\") ``` ", "Create an Expectation Suite with the Missingness Data Assistant": " title: Create an Expectation Suite with the Missingness Data Assistant import Prerequisites from '/docs/components/_prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; :::caution Missingness Data Assistant functionality is Experimental. ::: Use the information provided here to learn how you can use the Missingness Data Assistant to profile your data and automate the creation of an Expectation Suite. All the code used in the examples is available in GitHub at this location: how_to_create_an_expectation_suite_with_the_missingness_data_assistant.py. Prerequisites   A configured Data Context. An understanding of how to configure a Datasource. An understanding of how to configure a Batch Request.   Prepare your Batch Request In the following examples, you'll be using a Batch Request with multiple Batches and the Datasource that the Batch Request queries uses existing New York taxi trip data. This is the Datasource configuration: python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_missingness_data_assistant.py datasource_config\" This is the BatchRequest configuration: python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_missingness_data_assistant.py batch_request\" :::caution The Missingness Data Assistant runs multiple queries against your Datasource. Data Assistant performance can vary significantly depending on the number of Batches, the number of records per Batch, and network latency. If Data Assistant runtimes are too long, use a smaller BatchRequest. You can also run the Missingness Data Assistant on a single Batch when you expect the number of null records to be similar across Batches. ::: Prepare a new Expectation Suite Run the following code to prepare a new Expectation Suite with the Data Context add_expectation_suite(...) method: python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_missingness_data_assistant.py expectation_suite\" Run the Missingness Data Assistant To run a Data Assistant, you can call the run(...) method for the assistant. However, there are numerous parameters available for the run(...) method of the Missingness Data Assistant. For instance, the exclude_column_names parameter allows you to define the columns that should not be Profiled.  Run the following code to define the columns to exclude:  python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_missingness_data_assistant.py exclude_column_names\"  Run the following code to run the Missingness Data Assistant:  python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_missingness_data_assistant.py data_assistant_result\" In this example, context is your Data Context instance. :::note   If you consider your BatchRequest data valid, and want to produce Expectations with ranges that are identical to the data in the BatchRequest, you don't need to alter the example code. You're using the default estimation parameter (\"exact\"). To identify potential outliers in your BatchRequest data, pass estimation=\"flag_outliers\" to the run(...) method.   ::: :::note   The Missingness Data Assistant run(...) method can accept other parameters in addition to exclude_column_names such as include_column_names, include_column_name_suffixes, and cardinality_limit_mode. To view the available parameters, see this information.   ::: Save your Expectation Suite  After executing the Missingness Data Assistant's run(...) method and generating Expectations for your data, run the following code to load and save them into your Expectation Suite:  python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_missingness_data_assistant.py get_expectation_suite\"  Run the following code to save the Expectation Suite:  python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_missingness_data_assistant.py save_expectation_suite\" Test your Expectation Suite Run the following code to use a Checkpoint to operate with the Expectation Suite and Batch Request that you defined: python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_missingness_data_assistant.py checkpoint\" You can check the \"success\" key of the Checkpoint's results to verify that your Expectation Suite worked. Plot and inspect Metrics and Expectations  Run the following code to view Batch-level visualizations of the Metrics computed by the Missingness Data Assistant:  python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_missingness_data_assistant.py plot_metrics\"  :::note   Hover over a data point to view more information about the Batch and its calculated Metric value.   :::  Run the following code to view all Metrics computed by the Missingness Data Assistant:  python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_missingness_data_assistant.py metrics_by_domain\"  Run the following code to plot the Expectations and the associated Metrics calculated by the Missingness Data Assistant:  python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_missingness_data_assistant.py plot_expectations_and_metrics\"  :::note   The Expectation and the Metric are not visualized by the plot_expectations_and_metrics() method when an Expectation is not produced by the Missingness Data Assistant for a given Metric.   :::  Run the following command to view the Expectations produced and grouped by Domain:  python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_missingness_data_assistant.py show_expectations_by_domain_type\"  Run the following command to view the Expectations produced and grouped by Expectation type:  python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_missingness_data_assistant.py show_expectations_by_expectation_type\" Edit your Expectation Suite (Optional) The Missingness Data Assistant creates as many Expectations as it can for the permitted columns. Although this can help with data analysis, it might be unnecessary.  It is also possible that you may possess some domain knowledge that is not reflected in the data that was sampled for the Profiling process. In these types of scenarios, you can edit your Expectation Suite to better align with your business requirements. Run the following code to edit an existing Expectation Suite: markdown title=\"Terminal command\" great_expectations suite edit <expectation_suite_name> A Jupyter Notebook opens. You can review, edit, and save changes to the Expectation Suite.", "Create an Expectation Suite with the Onboarding Data Assistant": " title: Create an Expectation Suite with the Onboarding Data Assistant import Prerequisites from '/docs/components/_prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; This guide demonstrates how to use the Onboarding Data Assistant to Profile your data and automate the generation of an Expectation Suite, which you can then adjust to be suited for your specific needs. Prerequisites   A configured Data Context. An understanding of how to configure a Data Source. An understanding of how to configure a Batch Request.   Prepare your Batch Request Data Assistants excel at automating the Profiling process across multiple Batches. Therefore, for this guide you will  be using a Batch Request that covers multiple Batches. For the purposes of this demo, the Data Source that our Batch  Request queries will consist of a sample of the New York taxi trip data. This is the configuration that you will use for your Datasource: python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_onboarding_data_assistant.py datasource_config\" And this is the configuration that you will use for your BatchRequest: python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_onboarding_data_assistant.py batch_request\" :::caution The Onboarding Data Assistant will run a high volume of queries against your Datasource. Data Assistant performance   can vary significantly depending on the number of Batches, count of records per Batch, and network latency. It is   recommended that you start with a smaller BatchRequest if you find that Data Assistant runtimes are too long. ::: Prepare a new Expectation Suite Preparing a new Expectation Suite is done with the Data Context's add_expectation_suite(...) method, as seen in this code example: python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_onboarding_data_assistant.py expectation_suite\" Run the Onboarding Data Assistant Running a Data Assistant is as simple as calling the run(...) method for the appropriate assistant. That said, there are numerous parameters available for the run(...) method of the Onboarding Data Assistant. For  instance, the exclude_column_names parameter allows you to provide a list columns that should not be Profiled. For this guide, you will exclude the following columns: python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_onboarding_data_assistant.py exclude_column_names\" The following code shows how to run the Onboarding Assistant. In this code block, context is an instance of your Data Context. python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_onboarding_data_assistant.py data_assistant_result\" :::note If you consider your BatchRequest data valid, and want to produce Expectations with ranges that are identical to the   data in the BatchRequest, there is no need to alter the command above. You will be using the default estimation parameter (\"exact\").   If you want to identify potential outliers in your BatchRequest data, pass estimation=\"flag_outliers\" to the run(...) method. ::: :::note The Onboarding Data Assistant run(...) method can accept other parameters in addition to exclude_column_names such   as include_column_names, include_column_name_suffixes, and cardinality_limit_mode.   For a description of the available parameters please see this docstring here. ::: :::note If you would like to learn how to edit the Expectation Suite, please refer to How to Guide on How to Edit an Expectation Suite ::: Save your Expectation Suite Once you have executed the Onboarding Data Assistant's run(...) method and generated Expectations for your data, you  need to load them into your Expectation Suite and save them. You will do this by using the Data Assistant result: python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_onboarding_data_assistant.py get_expectation_suite\" And once the Expectation Suite has been retrieved from the Data Assistant result, you can save it like so: python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_onboarding_data_assistant.py save_expectation_suite\" Test your Expectation Suite with a Checkpoint To verify that your Expectation Suite is working, you can use a Checkpoint. First, you will configure one to  operate with the Expectation Suite and Batch Request that you have already defined: python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_onboarding_data_assistant.py checkpoint\" You can check the \"success\" key of the Checkpoint's results to verify that your Expectation Suite worked. Plot and inspect the Data Assistant's calculated Metrics and produced Expectations To see Batch-level visualizations of Metrics computed by the Onboarding Data Assistant run: python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_onboarding_data_assistant.py plot_metrics\"  :::note Hovering over a data point will provide more information about the Batch and its calculated Metric value in a tooltip. ::: To see all Metrics computed by the Onboarding Data Assistant run: python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_onboarding_data_assistant.py metrics_by_domain\" To plot the Expectations produced, and the associated Metrics calculated by the Onboarding Data Assistant run: python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_onboarding_data_assistant.py plot_expectations_and_metrics\"  :::note If no Expectation was produced by the Data Assistant for a given Metric, neither the Expectation nor the Metric will be visualized by the plot_expectations_and_metrics() method. ::: To see the Expectations produced and grouped by Domain run: python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_onboarding_data_assistant.py show_expectations_by_domain_type\" To see the Expectations produced and grouped by Expectation type run: python name=\"tests/integration/docusaurus/expectations/data_assistants/how_to_create_an_expectation_suite_with_the_onboarding_data_assistant.py show_expectations_by_expectation_type\" Edit your Expectation Suite, save, and test again (Optional) The Onboarding Data Assistant will create as many applicable Expectations as it can for the permitted columns. This  provides a solid base for analyzing your data, but may exceed your needs. It is also possible that you may possess  some domain knowledge that is not reflected in the data that was sampled for the Profiling process. In either of these  (or any other) cases, you can edit your Expectation Suite to more closely suite your needs. To edit an existing Expectation Suite (such as the one that you just created and saved with the Onboarding Data  Assistant) you need only execute the following console command: markdown title=\"Terminal command\" great_expectations suite edit NAME_OF_YOUR_SUITE_HERE This will open a Jupyter Notebook that will permit you to review, edit, and save changes to the specified Expectation  Suite. Additional Information :::note Example Code To view the full script used for example code on this page, see it on GitHub: - how_to_create_an_expectation_suite_with_the_onboarding_data_assistant.py :::", "Create example cases for a Custom Expectation": " title: Create example cases for a Custom Expectation import Prerequisites from '../creating_custom_expectations/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; This guide will help you add example cases to document and test the behavior of your Expectation.  Prerequisites   A Custom Expectation   Example cases in Great Expectations serve a dual purpose: * First, they help the users of the Expectation understand its logic by providing examples of input data that the Expectation will evaluate. * Second, they provide test cases that the Great Expectations testing framework can execute automatically. If you decide to contribute your Expectation, its entry in the Expectations Gallery will render these examples. We will explain the structure of these tests using the Custom Expectation implemented in our guide on how to create Custom Column Aggregate Expectations. Decide which tests you want to implement Expectations can have a robust variety of possible applications. We want to create tests that demonstrate (and verify) the capabilities and limitations of our Custom Expectation.  What kind of tests can I create? These tests can include examples intended to pass, fail, or error out, and expected results can be as open-ended as {`{\"success\": False}`}, or as granular as: {`{   \"success\": True,   \"expectation_config\": {       \"expectation_type\": \"expect_column_value_z_scores_to_be_less_than\",       \"kwargs\": {           \"column\": \"a\",           \"mostly\": 0.9,           \"threshold\": 4,           \"double_sided\": True,       },       \"meta\": {},   },   \"result\": {       \"element_count\": 6,       \"unexpected_count\": 0,       \"unexpected_percent\": 0.0,       \"partial_unexpected_list\": [],       \"missing_count\": 0,       \"missing_percent\": 0.0,       \"unexpected_percent_total\": 0.0,       \"unexpected_percent_nonmissing\": 0.0,   },   \"exception_info\": {       \"raised_exception\": False,       \"exception_traceback\": None,       \"exception_message\": None,   } }`}   At a minimum, we want to create tests that show what our Custom Expectation will and will not do.  These basic positive and negative example cases are the minimum amount of test coverage required for a Custom Expectation to be accepted into the Great Expectations codebase at an Experimental level. To begin with, let's implement those two basic tests: one positive example case, and one negative example case.  Define your data Search for examples = [] in the template file you are modifying for your new Custom Expectation.  We're going to populate examples with a list of example cases.  What is an example case? Each example is a dictionary with two keys:   data: defines the input data of the example as a table/dataframe.   tests: a list of test cases that use the data defined above as input to validate against.    title: a descriptive name for the test case. Make sure to have no spaces.   include_in_gallery: set it to True if you want this test case to be visible in the gallery as an example (true for most test cases).   in: contains exactly the parameters that you want to pass in to the Expectation. {`\"in\": {\"column\": \"x\", \"min_value\": 4}`} would be equivalent to expect_column_max_to_be_between_custom(column=\"x\", min_value=4)   out: indicates the results the test requires from the ValidationResult needed to pass.   exact_match_out: if you set exact_match_out=False, then you don\u2019t need to include all the elements of the result object - only the ones that are important to test, such as {`{\"success\": True}`}.     In our example, data will have two columns, \"x\" and \"y\", each with five rows. If you define multiple columns, make sure that they have the same number of rows. When possible, include test data and tests that includes null values (None in the Python test definition). python \"data\": {\"x\": [1, 2, 3, 4, 5], \"y\": [0, -1, -2, 4, None]}, When you define data in your examples, we will mostly guess the type of the columns.  Sometimes you need to specify the precise type of the columns for each backend. Then you use the schemas attribute (on the same level as data and tests in the dictionary): console \"schemas\": {   \"spark\": {     \"x\": \"IntegerType\",   },   \"sqlite\": {     \"x\": \"INTEGER\",   }, :::info While Pandas is fairly flexible in typing, Spark and many SQL dialects are much more strict.  You may find you wish to use data that is incompatible with a given backend, or write different individual tests for different backends.  To do this, you can use the only_for attribute, which accepts a list containing pandas, spark, sqlite, a SQL dialect, or a combination of any of the above: console \"only_for\": [\"spark\", \"pandas\"] Passing this attribute on the same level as data, tests, and schemas  will tell Great Expectations to only instantiate the data specified in that example for the given backend, ensuring you don't encounter any backend-related errors relating to data before your Custom Expectation can even be tested: Passing this attribute within a test (at the same level as title, in, out, etc.) will execute that individual test only for that specified backend. ::: Define your tests In our example, tests will be a list containing dictionaries defining each test.  You will need to: 1. Title your tests (title) 2. Define the input for your tests (in) 3. Decide how precisely you want to test the output of your tests (exact_match_out) 4. Define the expected output for your tests (out) If you are interested in contributing your Custom Expectation back to Great Expectations, you will also need to decide if you want these tests publicly displayed to demonstrate the functionality of your Custom Expectation (include_in_gallery). python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py examples\" :::note The optional only_for and suppress_test_for keys can be specified at the top-level (next to data and tests) or within specific tests (next to title, and so on). Allowed backends include: \"bigquery\", \"mssql\", \"mysql\", \"pandas\", \"postgresql\", \"redshift\", \"snowflake\", \"spark\", \"sqlite\", \"trino\" :::  Can I test for errors? Yes! If you would like to define an example case illustrating when your Custom Expectation should throw an error,  you can pass an empty out key, and include an error key defining a traceback_substring.   For example:  {`\"out\": {}, \"error\": {     \"traceback_substring\" : \"TypeError: Column values, min_value, and max_value must either be None or of the same type.\" }`}   Verify your tests If you now run your file, print_diagnostic_checklist() will attempt to execute these example cases. If the tests are correctly defined, and the rest of the logic in your Custom Expectation is already complete, you will see the following in your Diagnostic Checklist: console \u2714 Has at least one positive and negative example case, and all test cases pass   Congratulations!\ud83c\udf89 You've successfully created example cases & tests for a Custom Expectation! \ud83c\udf89   Contribution (Optional) This guide will leave you with test coverage sufficient for contribution back to Great Expectations at an Experimental level.   If you're interested in having your contribution accepted at a Beta level, these tests will need to pass for all supported backends (Pandas, Spark, & SQLAlchemy). For full acceptance into the Great Expectations codebase at a Production level, we require a more robust test suite.  If you believe your Custom Expectation is otherwise ready for contribution at a Production level, please submit a Pull Request, and we will work with you to ensure adequate testing. :::note For more information on our code standards and contribution, see our guide on Levels of Maturity for Expectations. To view the full script used in this page, see it on GitHub: - expect_column_max_to_be_between_custom.py :::", "Add input validation and type checking for a Custom Expectation": " title: Add input validation and type checking for a Custom Expectation import Prerequisites from '../creating_custom_expectations/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; Prerequisites   Created a Custom Expectation   Expectations will typically be configured using input parameters. These parameters are required to provide your Custom Expectation with the context it needs to Validate your data.  Ensuring that these requirements are fulfilled is the purpose of type checking and validating your input parameters. For example, we might expect the fraction of null values to be mostly=.05, in which case any value above 1 would indicate an impossible fraction of a single whole (since a value above one indicates more than a single whole), and should throw an error. Another example would be if we want to indicate that the mean of a row adheres to a minimum value bound, such as min_value=5. In this case, attempting to pass in a non numerical value should clearly throw an error! This guide will walk you through the process of adding validation and Type Checking to the input parameters of the Custom Expectation built in the guide for how to create a Custom Column Aggregate Expectation. When you have completed this guide, you will have implemented a method to validate that the input parameters provided to this Custom Expectation satisfy the requirements necessary for them to be used as intended by the Custom Expectation's code. Decide what to validate As a general rule, we want to validate any of our input parameters and success keys that are explicitly used by our Expectation class. In the case of our example Expectation expect_column_max_to_be_between_custom, we've defined four parameters to validate:  min_value: An integer or float defining the lowest acceptable bound for our column max max_value: An integer or float defining the highest acceptable bound for our column max strict_min: A boolean value defining whether our column max is (strict_min=False) or is not (strict_min=True) allowed to equal the min_value strict_max: A boolean value defining whether our column max is (strict_max=False) or is not (strict_max=True) allowed to equal the max_value   What don't we need to validate? You may have noticed we're not validating whether the column parameter has been set. Great Expectations implicitly handles the validation of certain parameters universal to each class of Expectation, so you don't have to!  Define the Validation method We define the validate_configuration(...) method of our Custom Expectation class to ensure that the input parameters constitute a valid configuration,  and doesn't contain illogical or incorrect values. For example, if min_value is greater than max_value, max_value=True, or strict_min=Joe, we want to throw an exception. To do this, we're going to write a series of assert statements to catch invalid values for our parameters. To begin with, we want to create our validate_configuration(...) method and ensure that a configuration is set: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py validate_config\" Next, we're going to implement the logic for validating the four parameters we identified above. Access parameters and writing assertions First we need to access the parameters to be evaluated: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py validate_config_params\" Now we can begin writing the assertions to validate these parameters.  We're going to ensure that at least one of min_value or max_value is set: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py validate_config_values\" Check that min_value and max_value are of the correct type: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py validate_config_types\" Verify that, if both min_value and max_value are set, min_value does not exceed max_value: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py validate_config_comparison\" And assert that strict_min and strict_max, if provided, are of the correct type: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py validate_config_none\" If any of these fail, we raise an exception: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py validate_config_except\" Putting this all together, our validate_configuration(...) method should verify that all necessary inputs have been provided,  that all inputs are of the correct types, that they have a correct relationship between each other, and that if any of these conditions aren't met,  we raise an exception. Verify your method If you now run your file, print_diagnostic_checklist() will attempt to execute the validate_configuration(...) using the input provided in your Example Cases. If your input is successfully validated, and the rest the logic in your Custom Expectation is already complete, you will see the following in your Diagnostic Checklist: console  \u2714 Has basic input validation and type checking     \u2714 Custom 'assert' statements in validate_configuration   Congratulations!\ud83c\udf89 You've successfully added input validation & type checking to a Custom Expectation! \ud83c\udf89   Contribution (Optional) The method implemented in this guide is an optional feature for Experimental Expectations, and a requirement for contribution to Great Expectations at Beta and Production levels. If you would like to contribute your Custom Expectation to the Great Expectations codebase, please submit a Pull Request. :::note For more information on our code standards and contribution, see our guide on Levels of Maturity for Expectations. To view the full script used in this page, see it on GitHub: - expect_column_max_to_be_between_custom.py :::", "Add Spark support for Custom Expectations": " title: Add Spark support for Custom Expectations import Prerequisites from '../creating_custom_expectations/components/prerequisites.jsx' import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; This guide will help you implement native Spark support for your Custom Expectation.  Prerequisites   A Custom Expectation   Great Expectations supports a number of Execution Engines, including a Spark Execution Engine. These Execution Engines provide the computing resources used to calculate the Metrics defined in the Metric class of your Custom Expectation. If you decide to contribute your Expectation, its entry in the Expectations Gallery will reflect the Execution Engines that it supports. We will add Spark support for the Custom Expectations implemented in our guides on how to create Custom Column Aggregate Expectations  and how to create Custom Column Map Expectations. Specify your backends To avoid surprises and help clearly define your Custom Expectation, it can be helpful to determine beforehand what backends you plan to support, and test them along the way. Within the examples defined inside your Expectation class, the optional only_for and suppress_test_for keys specify which backends to use for testing. If a backend is not specified, Great Expectations attempts testing on all supported backends. Run the following command to add entries corresponding to the functionality you want to add:  python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py examples\" :::note The optional only_for and suppress_test_for keys may be specified at the top-level (next to data and tests) or within specific tests (next to title, etc). Allowed backends include: \"bigquery\", \"mssql\", \"mysql\", \"pandas\", \"postgresql\", \"redshift\", \"snowflake\", \"spark\", \"sqlite\", \"trino\" ::: Implement the Spark logic for your Custom Expectation Great Expectations provides a variety of ways to implement an Expectation in Spark. Two of the most common include:   Defining a partial function that takes a Spark DataFrame column as input Directly executing queries on Spark DataFrames to determine the value of your Expectation's metric directly     Great Expectations allows for much of the PySpark DataFrame logic to be abstracted away by specifying metric behavior as a partial function.  To do this, we use one of the @column_*_partial decorators:  @column_aggregate_partial for Column Aggregate Expectations @column_condition_partial for Column Map Expectations @column_pair_condition_partial for Column Pair Map Expectations @multicolumn_condition_partial for Multicolumn Map Expectations  These decorators expect an appropriate engine argument. In this case, we'll pass our SparkDFExecutionEngine. The decorated method takes in a Spark Column object and will either return a pyspark.sql.functions.function or a pyspark.sql.Column.function that Great Expectations will use to generate the appropriate SQL queries. For our Custom Column Aggregate Expectation ExpectColumnMaxToBeBetweenCustom, we're going to leverage PySpark's max SQL Function and the @column_aggregate_partial decorator. python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py _spark\" If we need a builtin function from pyspark.sql.functions, usually aliased to F, the import logic in  from great_expectations.compatibility.pyspark import functions as F from great_expectations.compatibility import pyspark allows us to access these functions even when PySpark is not installed.  Applying Python Functions F.udf allows us to use a Python function as a Spark User Defined Function for Column Map Expectations,  giving us the ability to define custom functions and apply them to our data.  Here is an example of F.udf applied to ExpectColumnValuesToEqualThree:  ```python @column_condition_partial(engine=SparkDFExecutionEngine) def _spark(cls, column, strftime_format, **kwargs):     def is_equal_to_three(val):         return (val == 3)      success_udf = F.udf(is_equal_to_three, pyspark.types.BooleanType())     return success_udf(column) ```  For more on F.udf and the functionality it provides, see the Apache Spark UDF documentation.     The most direct way of implementing a metric is by computing its value by constructing or directly executing querys using objects provided by the @metric_* decorators: - @metric_value for Column Aggregate Expectations   - Expects an appropriate engine, metric_fn_type, and domain_type - @metric_partial for all Map Expectations   - Expects an appropriate engine, partial_fn_type, and domain_type Our engine will reflect the backend we're implementing (SparkDFExecutionEngine), while our fn_type and domain_type are unique to the type of Expectation we're implementing. These decorators enable a higher-complexity workflow, allowing you to explicitly structure your queries and make intermediate queries to your database.  While this approach can result in extra roundtrips to your database, it can also unlock advanced functionality for your Custom Expectations. For our Custom Column Map Expectation ExpectColumnValuesToEqualThree, we're going to implement the @metric_partial decorator,  specifying the type of value we're computing (MAP_CONDITION_FN) and the domain over which we're computing (COLUMN): python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_equal_three.py spark_definition\" The decorated method takes in a valid Execution Engine and relevant kwargs, and will return a tuple of: - A pyspark.sql.column.Column defining the query to be executed - compute_domain_kwargs - accessor_domain_kwargs These will be used to execute our query and compute the results of our metric. To do this, we need to access our Compute Domain directly: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_equal_three.py spark_selectable\" This allows us to build and return a query to be executed, providing the result of our metric: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_equal_three.py spark_query\" :::note Because in Spark we are implementing the window function directly, we have to return the unexpected condition: False when column == 3, otherwise True. :::   Verify your implementation If you now run your file, print_diagnostic_checklist() will attempt to execute your example cases using this new backend. If your implementation is correctly defined, and the rest of the core logic in your Custom Expectation is already complete, you will see the following in your Diagnostic Checklist: console \u2714 Has at least one positive and negative example case, and all test cases pass If you've already implemented the Pandas backend covered in our How-To guides for creating Custom Expectations  and the SQLAlchemy backend covered in our guide on how to add SQLAlchemy support for Custom Expectations,  you should see the following in your Diagnostic Checklist: console \u2714 Has core logic that passes tests for all applicable Execution Engines and SQL dialects   Congratulations!\ud83c\udf89 You've successfully implemented Spark support for a Custom Expectation! \ud83c\udf89   Contribution (Optional) This guide will leave you with core functionality sufficient for contribution to Great Expectations at an Experimental level. If you're interested in having your contribution accepted at a Beta level, your Custom Expectation will need to support SQLAlchemy, Spark, and Pandas. For full acceptance into the Great Expectations codebase at a Production level, we require that your Custom Expectation meets our code standards, including test coverage and style.  If you believe your Custom Expectation is otherwise ready for contribution at a Production level, please submit a Pull Request, and we will work with you to ensure your Custom Expectation meets these standards. :::note For more information on our code standards and contribution, see our guide on Levels of Maturity for Expectations. To view the full scripts used in this page, see them on GitHub: - expect_column_max_to_be_between_custom.py - expect_column_values_to_equal_three.py :::", "Add SQLAlchemy support for Custom Expectations": " title: Add SQLAlchemy support for Custom Expectations import Prerequisites from '../creating_custom_expectations/components/prerequisites.jsx' import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; This guide will help you implement native SQLAlchemy support for your Custom Expectation. Prerequisites   A Custom Expectation   Great Expectations supports a number of Execution Engines, including a SQLAlchemy Execution Engine.  These Execution Engines provide the computing resources used to calculate the Metrics defined in the Metric class of your Custom Expectation. If you decide to contribute your Expectation, its entry in the Expectations Gallery will reflect the Execution Engines that it supports. We will add SQLAlchemy support for the Custom Expectations implemented in our guides on how to create Custom Column Aggregate Expectations  and how to create Custom Column Map Expectations. Specify your backends and dialects While SQLAlchemy is able to provide a common interface to a variety of SQL dialects, some functions may not work in a particular dialect, or in some cases they may return different values.  To avoid surprises, it can be helpful to determine beforehand what backends and dialects you plan to support, and test them along the way.  Within the examples defined inside your Expectation class, the optional only_for and suppress_test_for keys specify which backends to use for testing. If a backend is not specified, Great Expectations attempts testing on all supported backends. Run the following command to add entries corresponding to the functionality you want to add:  python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py examples\" :::note The optional only_for and suppress_test_for keys can be specified at the top-level (next to data and tests) or within specific tests (next to title, and so on). Allowed backends include: \"bigquery\", \"mssql\", \"mysql\", \"pandas\", \"postgresql\", \"redshift\", \"snowflake\", \"spark\", \"sqlite\", \"trino\" ::: Implement the SQLAlchemy logic for your Custom Expectation Great Expectations provides a variety of ways to implement an Expectation in SQLAlchemy. Two of the most common include:    Defining a partial function that takes a SQLAlchemy column as input Directly executing queries using SQLAlchemy objects to determine the value of your Expectation's metric directly     Great Expectations allows for much of the SQLAlchemy logic for executing queries be abstracted away by specifying metric behavior as a partial function.  To do this, we use one of the @column_*_partial decorators: - @column_aggregate_partial for Column Aggregate Expectations - @column_condition_partial for Column Map Expectations - @column_pair_condition_partial for Column Pair Map Expectations - @multicolumn_condition_partial for Multicolumn Map Expectations These decorators expect an appropriate engine argument. In this case, we'll pass our SqlAlchemyExecutionEngine.  The decorated method takes in an SQLAlchemy Column object and will either return a sqlalchemy.sql.functions.Function or a sqlalchemy.sql.expression.ColumnOperator that Great Expectations will use to generate the appropriate SQL queries.  For our Custom Column Map Expectation ExpectColumnValuesToEqualThree, we're going to leverage SQLAlchemy's in_ ColumnOperator and the @column_condition_partial decorator. python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_values_to_equal_three.py sqlalchemy\"  Getting func-y? We can also take advantage of SQLAlchemy's func special object instance.   func allows us to pass common generic functions which SQLAlchemy will compile appropriately for the targeted dialect,  giving us the flexibility to not have write that targeted code ourselves!   Here's an example from ExpectColumnSumToBeBetween:  ```python  @column_aggregate_partial(engine=SqlAlchemyExecutionEngine) def _sqlalchemy(cls, column, **kwargs):     return sa.func.sum(column)  ```  For more on func and the func-tionality it provides, see SQLAlchemy's Functions documentation.    The most direct way of implementing a metric is by computing its value by constructing or directly executing querys using objects provided by the @metric_* decorators: - @metric_value for Column Aggregate Expectations   - Expects an appropriate engine, metric_fn_type, and domain_type - @metric_partial for all Map Expectations   - Expects an appropriate engine, partial_fn_type, and domain_type Our engine will reflect the backend we're implementing (SqlAlchemyExecutionEngine), while our fn_type and domain_type are unique to the type of Expectation we're implementing. These decorators enable a higher-complexity workflow, allowing you to explicitly structure your queries and make intermediate queries to your database.  While this approach can result in extra roundtrips to your database, it can also unlock advanced functionality for your Custom Expectations. For our Custom Column Aggregate Expectation ExpectColumnMaxToBeBetweenCustom, we're going to implement the @metric_value decorator,  specifying the type of value we're computing (AGGREGATE_VALUE) and the domain over which we're computing (COLUMN): python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py sql_def\" The decorated method takes in a valid Execution Engine and relevant key word arguments, and will return a computed value. To do this, we need to access our Compute Domain directly: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py sql_selectable\" This allows us to build a query and use our Execution Engine to execute that query against our data to return the actual value we're looking for, instead of returning a query to find that value: python name=\"tests/integration/docusaurus/expectations/creating_custom_expectations/expect_column_max_to_be_between_custom.py sql_query\"  Getting func-y? While this approach allows for highly complex queries, here we're taking advantage of SQLAlchemy's func special object instance.   func allows us to pass common generic functions which SQLAlchemy will compile appropriately for the targeted dialect,  giving us the flexibility to not have write that targeted code ourselves!  For more on func and the func-tionality it provides, see SQLAlchemy's Functions documentation.    Verify your implementation If you now run your file, print_diagnostic_checklist() will attempt to execute your example cases using this new backend. If your implementation is correctly defined, and the rest of the core logic in your Custom Expectation is already complete, you will see the following in your Diagnostic Checklist: console \u2714 Has at least one positive and negative example case, and all test cases pass If you've already implemented the Pandas backend covered in our How-To guides for creating Custom Expectations  and the Spark backend covered in our guide on how to add Spark support for Custom Expectations,  you should see the following in your Diagnostic Checklist: console \u2714 Has core logic that passes tests for all applicable Execution Engines and SQL dialects   Congratulations!\ud83c\udf89 You've successfully implemented SQLAlchemy support for a Custom Expectation! \ud83c\udf89   Contribution (Optional) This guide will leave you with core functionality sufficient for contribution to Great Expectations at an Experimental level. If you're interested in having your contribution accepted at a Beta level, your Custom Expectation will need to support SQLAlchemy, Spark, and Pandas. For full acceptance into the Great Expectations codebase at a Production level, we require that your Custom Expectation meets our code standards, test coverage and style.  If you believe your Custom Expectation is otherwise ready for contribution at a Production level, please submit a Pull Request, and we will work with you to ensure your Custom Expectation meets these standards. :::note For more information on our code standards and contribution, see our guide on Levels of Maturity for Expectations. To view the full scripts used in this page, see them on GitHub: - expect_column_max_to_be_between_custom.py - expect_column_values_to_equal_three.py :::", "Configure credentials": " title: Configure credentials import Prerequisites from '../../connecting_to_your_data/components/prerequisites.jsx' import Tabs from '@theme/Tabs' import TabItem from '@theme/TabItem' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; This guide will explain how to populate credentials either through an environment variable, or configure your Great Expectations project to load credentials from either a YAML file or a secret manager. If your Great Expectations deployment is in an environment without a file system, refer to Instantiate an Ephemeral Data Context. Prerequisites  Using Environment Variables The quickest way to get started is by setting up your credentials as environment variables.  First set values by entering export ENV_VAR_NAME=env_var_value in the terminal or adding the commands to your ~/.bashrc file: bash name=\"tests/integration/docusaurus/setup/configuring_data_contexts/how_to_configure_credentials.py export_env_vars\" These can then be loaded into the connection_string parameter when we are adding a datasource to the Data Context. bash name=\"tests/integration/docusaurus/setup/configuring_data_contexts/how_to_configure_credentials.py add_credentials_as_connection_string\" Using YAML or Secret Manager   Using the config_variables.yml file A more advanced option is to use the config variables YAML file. YAML files make variables more visible, easily editable, and allow for modularization (e.g. one file for dev, another for prod).  If using a YAML file, save desired credentials or config values to great_expectations/uncommitted/config_variables.yml: yaml name=\"tests/integration/docusaurus/setup/configuring_data_contexts/how_to_configure_credentials.py config_variables_yaml\" :::note  If you wish to store values that include the dollar sign character $, please escape them using a backslash \\ so substitution is not attempted. For example in the above example for Postgres credentials you could set password: pa\\$sword if your password is pa$sword. Say that 5 times fast, and also please choose a more secure password! You can also have multiple substitutions for the same item, e.g. database_string: ${USER}:${PASSWORD}@${HOST}:${PORT}/${DATABASE}  ::: Then the config variable can be loaded into the connection_string parameter when we are adding a datasource to the Data Context. bash name=\"tests/integration/docusaurus/setup/configuring_data_contexts/how_to_configure_credentials.py add_credential_from_yml\" Additional Notes  The default config_variables.yml file located at great_expectations/uncommitted/config_variables.yml applies to deployments using  FileSystemDataContexts. To view the full script used in this page, see it on GitHub: how_to_configure_credentials.py    Select one of the following secret manager applications:   Configure your Great Expectations project to substitute variables from the AWS Secrets Manager. Prerequisites   An AWS Secrets Manager instance. See AWS Secrets Manager.   :::warning Secrets store substitution uses the configurations from your config_variables.yml file after all other types of substitution are applied from environment variables. The secrets store substitution works based on keywords. It tries to retrieve secrets from the secrets store for the following values :  AWS: values starting with secret|arn:aws:secretsmanager if the values you provide don't match with the keywords above, the values won't be substituted.  ::: Setup To use AWS Secrets Manager, you may need to install the great_expectations package with its aws_secrets extra requirement: bash pip install 'great_expectations[aws_secrets]' In order to substitute your value by a secret in AWS Secrets Manager, you need to provide an arn of the secret like this one: secret|arn:aws:secretsmanager:123456789012:secret:my_secret-1zAyu6 :::note The last 7 characters of the arn are automatically generated by AWS and are not mandatory to retrieve the secret, thus secret|arn:aws:secretsmanager:region-name-1:123456789012:secret:my_secret will retrieve the same secret. ::: You will get the latest version of the secret by default. You can get a specific version of the secret you want to retrieve by specifying its version UUID like this: secret|arn:aws:secretsmanager:region-name-1:123456789012:secret:my_secret:00000000-0000-0000-0000-000000000000 If your secret value is a JSON string, you can retrieve a specific value like this: secret|arn:aws:secretsmanager:region-name-1:123456789012:secret:my_secret|key Or like this: secret|arn:aws:secretsmanager:region-name-1:123456789012:secret:my_secret:00000000-0000-0000-0000-000000000000|key Example config_variables.yml: ```yaml We can configure a single connection string my_aws_creds:  secret|arn:aws:secretsmanager:${AWS_REGION}:${ACCOUNT_ID}:secret:dev_db_credentials|connection_string Or each component of the connection string separately drivername: secret|arn:aws:secretsmanager:${AWS_REGION}:${ACCOUNT_ID}:secret:dev_db_credentials|drivername host: secret|arn:aws:secretsmanager:${AWS_REGION}:${ACCOUNT_ID}:secret:dev_db_credentials|host port: secret|arn:aws:secretsmanager:${AWS_REGION}:${ACCOUNT_ID}:secret:dev_db_credentials|port username: secret|arn:aws:secretsmanager:${AWS_REGION}:${ACCOUNT_ID}:secret:dev_db_credentials|username password: secret|arn:aws:secretsmanager:${AWS_REGION}:${ACCOUNT_ID}:secret:dev_db_credentials|password database: secret|arn:aws:secretsmanager:${AWS_REGION}:${ACCOUNT_ID}:secret:dev_db_credentials|database ``` Once configured, the credentials can be loaded into the connection_string parameter when we are adding a datasource to the Data Context. ```python  We can use a single connection string pg_datasource = context.sources.add_or_update_sql(     name=\"my_postgres_db\", connection_string=\"${my_aws_creds}\" ) Or each component of the connection string separately pg_datasource = context.sources.add_or_update_sql(     name=\"my_postgres_db\", connection_string=\"${drivername}://${username}:${password}@${host}:${port}/${database}\" ) ```   Configure your Great Expectations project to substitute variables from the GCP Secrets Manager. Prerequisites   Configured a secret manager and secrets in the cloud with GCP Secret Manager   :::warning Secrets store substitution uses the configurations from your config_variables.yml project config after substitutions are applied from environment variables. The secrets store substitution works based on keywords. It tries to retrieve secrets from the secrets store for the following values :  GCP: values matching the following regex ^secret\\|projects\\/[a-z0-9\\_\\-]{6,30}\\/secrets if the values you provide don't match with the keywords above, the values won't be substituted.  ::: Setup To use GCP Secret Manager, you may need to install the great_expectations package with its gcp extra requirement: bash pip install 'great_expectations[gcp]' In order to substitute your value by a secret in GCP Secret Manager, you need to provide a name of the secret like this one: secret|projects/project_id/secrets/my_secret You will get the latest version of the secret by default. You can get a specific version of the secret you want to retrieve by specifying its version id like this: secret|projects/project_id/secrets/my_secret/versions/1 If your secret value is a JSON string, you can retrieve a specific value like this: secret|projects/project_id/secrets/my_secret|key Or like this: secret|projects/project_id/secrets/my_secret/versions/1|key Example config_variables.yml: ```yaml We can configure a single connection string my_gcp_creds: secret|projects/${PROJECT_ID}/secrets/dev_db_credentials|connection_string Or each component of the connection string separately drivername: secret|projects/${PROJECT_ID}/secrets/PROD_DB_CREDENTIALS_DRIVERNAME host: secret|projects/${PROJECT_ID}/secrets/PROD_DB_CREDENTIALS_HOST port: secret|projects/${PROJECT_ID}/secrets/PROD_DB_CREDENTIALS_PORT username: secret|projects/${PROJECT_ID}/secrets/PROD_DB_CREDENTIALS_USERNAME password: secret|projects/${PROJECT_ID}/secrets/PROD_DB_CREDENTIALS_PASSWORD database: secret|projects/${PROJECT_ID}/secrets/PROD_DB_CREDENTIALS_DATABASE ``` Once configured, the credentials can be loaded into the connection_string parameter when we are adding a datasource to the Data Context. ```python  We can use a single connection string pg_datasource = context.sources.add_or_update_sql(     name=\"my_postgres_db\", connection_string=\"${my_gcp_creds}\" ) Or each component of the connection string separately pg_datasource = context.sources.add_or_update_sql(     name=\"my_postgres_db\", connection_string=\"${drivername}://${username}:${password}@${host}:${port}/${database}\" ) ```   Configure your Great Expectations project to substitute variables from the Azure Key Vault. Prerequisites   Set up a working deployment of Great Expectations Configured a secret manager and secrets in the cloud with Azure Key Vault   :::warning Secrets store substitution uses the configurations from your config_variables.yml file after all other types of substitution are applied from environment variables. The secrets store substitution works based on keywords. It tries to retrieve secrets from the secrets store for the following values :  Azure : values matching the following regex ^secret\\|https:\\/\\/[a-zA-Z0-9\\-]{3,24}\\.vault\\.azure\\.net if the values you provide don't match with the keywords above, the values won't be substituted.  ::: Setup To use Azure Key Vault, you may need to install the great_expectations package with its azure_secrets extra requirement: bash pip install 'great_expectations[azure_secrets]' In order to substitute your value by a secret in Azure Key Vault, you need to provide a name of the secret like this one: secret|https://my-vault-name.vault.azure.net/secrets/my-secret You will get the latest version of the secret by default. You can get a specific version of the secret you want to retrieve by specifying its version id (32 lowercase alphanumeric characters) like this: secret|https://my-vault-name.vault.azure.net/secrets/my-secret/a0b00aba001aaab10b111001100a11ab If your secret value is a JSON string, you can retrieve a specific value like this: secret|https://my-vault-name.vault.azure.net/secrets/my-secret|key Or like this: secret|https://my-vault-name.vault.azure.net/secrets/my-secret/a0b00aba001aaab10b111001100a11ab|key Example config_variables.yml: ```yaml We can configure a single connection string my_abs_creds: secret|https://${VAULT_NAME}.vault.azure.net/secrets/dev_db_credentials|connection_string Or each component of the connection string separately drivername: secret|https://${VAULT_NAME}.vault.azure.net/secrets/dev_db_credentials|host host: secret|https://${VAULT_NAME}.vault.azure.net/secrets/dev_db_credentials|host port: secret|https://${VAULT_NAME}.vault.azure.net/secrets/dev_db_credentials|port username: secret|https://${VAULT_NAME}.vault.azure.net/secrets/dev_db_credentials|username password: secret|https://${VAULT_NAME}.vault.azure.net/secrets/dev_db_credentials|password database: secret|https://${VAULT_NAME}.vault.azure.net/secrets/dev_db_credentials|database ``` Once configured, the credentials can be loaded into the connection_string parameter when we are adding a datasource to the Data Context. ```python  We can use a single connection string pg_datasource = context.sources.add_or_update_sql(     name=\"my_postgres_db\", connection_string=\"${my_gcp_creds}\" ) Or each component of the connection string separately pg_datasource = context.sources.add_or_update_sql(     name=\"my_postgres_db\", connection_string=\"${drivername}://${username}:${password}@${host}:${port}/${database}\" ) ```    ", "Convert an Ephemeral Data Context to a Filesystem Data Context": " title: Convert an Ephemeral Data Context to a Filesystem Data Context tag: [how-to, setup] keywords: [Great Expectations, Ephemeral Data Context, Filesystem Data Context]  import Prerequisites from '/docs/components/_prerequisites.jsx' import IfYouStillNeedToSetupGx from '/docs/components/prerequisites/_if_you_still_need_to_setup_gx.md' import ConnectingToDataFluently from '/docs/components/connect_to_data/link_lists/_connecting_to_data_fluently.md' import SetupConfigurations from '/docs/components/setup/link_lists/_setup_configurations.md' An Ephemeral Data Context is a temporary, in-memory Data Context that will not persist beyond the current Python session.  However, if you decide you would like to save the contents of an Ephemeral Data Context for future use you can do so by converting it to a Filesystem Data Context. Prerequisites   A working installation of Great Expectations An Ephemeral Data Context instance       ### If you still need to set up and install GX...        ### If you still need to create a Data Context...    The `get_context()` method will return an Ephemeral Data Context if your system is not set up to work with GX Cloud and a Filesystem Data Context cannot be found.  For more information, see: - [How to quickly instantiate a Data Context](/docs/guides/setup/configuring_data_contexts/instantiating_data_contexts/how_to_quickly_instantiate_a_data_context)  You can also instantiate an Ephemeral Data Context (for those occasions when your system is set up to work with GX Cloud or you do have a previously initialized Filesystem Data Context).  For more information, see: - [How to instantiate an Ephemeral Data Context](/docs/guides/setup/configuring_data_contexts/instantiating_data_contexts/instantiate_data_context)      ### If you aren't certain that your Data Context is Ephemeral...    You can easily check to see if you are working with an Ephemeral Data Context with the following code (in this example, we are assuming your Data Context is stored in the variable `context`):  ```python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_explicitly_instantiate_an_ephemeral_data_context.py check_data_context_is_ephemeral\" ```   Verify that your current working directory does not already contain a GX Filesystem Data Context The method for converting an Ephemeral Data Context to a Filesystem Data Context initializes the new Filesystem Data Context in the current working directory of the Python process that is being executed.  If a Filesystem Data Context already exists at that location, the process will fail. You can determine if your current working directory already has a Filesystem Data Context by looking for a great_expectations.yml file.  The presence of that file indicates that a Filesystem Data Context has already been initialized in the corresponding directory. Convert the Ephemeral Data Context into a Filesystem Data Context Converting an Ephemeral Data Context into a Filesystem Data Context can be done with one line of code: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_explicitly_instantiate_an_ephemeral_data_context.py convert_ephemeral_data_context_filesystem_data_context\" :::info Replacing the Ephemeral Data Context The convert_to_file_context() method does not change the Ephemeral Data Context itself.  Rather, it initializes a new Filesystem Data Context with the contents of the Ephemeral Data Context and then returns an instance of the new Filesystem Data Context.  If you do not replace the Ephemeral Data Context instance with the Filesystem Data Context instance, it will be possible for you to continue using the Ephemeral Data Context.   If you do this, it is important to note that changes to the Ephemeral Data Context will not be reflected in the Filesystem Data Context.  Moreover, convert_to_file_context() does not support merge operations. This means you will not be able to save any additional changes you have made to the content of the Ephemeral Data Context.  Neither will you be able to use convert_to_file_context() to replace the Filesystem Data Context you had previously created: convert_to_file_context() will fail if a Filesystem Data Context already exists in the current working directory. For these reasons, it is strongly advised that once you have converted your Ephemeral Data Context to a Filesystem Data Context you cease working with the Ephemeral Data Context instance and begin working with the Filesystem Data Context instance instead. ::: Next steps Customize Data Context configurations  Connect GX to source data systems ", "\"Host and share Data Docs\"": " sidebar_label: \"Host and share Data Docs\" title: \"Host and share Data Docs\" id: host_and_share_data_docs description: Host and share Data Docs stored on a filesystem or a source data system. toc_min_heading_level: 2 toc_max_heading_level: 2  import Preface from './components_how_to_host_and_share_data_docs_on_amazon_s3/_preface.mdx' import CreateAnS3Bucket from './components_how_to_host_and_share_data_docs_on_amazon_s3/_create_an_s3_bucket.mdx' import ConfigureYourBucketPolicyToEnableAppropriateAccess from './components_how_to_host_and_share_data_docs_on_amazon_s3/_configure_your_bucket_policy_to_enable_appropriate_access.mdx' import ApplyThePolicy from './components_how_to_host_and_share_data_docs_on_amazon_s3/_apply_the_policy.mdx' import AddANewS3SiteToTheDataDocsSitesSectionOfYourGreatExpectationsYml from './components_how_to_host_and_share_data_docs_on_amazon_s3/_add_a_new_s3_site_to_the_data_docs_sites_section_of_your_great_expectationsyml.mdx' import TestThatYourConfigurationIsCorrectByBuildingTheSite from './components_how_to_host_and_share_data_docs_on_amazon_s3/_test_that_your_configuration_is_correct_by_building_the_site.mdx' import AdditionalNotes from './components_how_to_host_and_share_data_docs_on_amazon_s3/_additional_notes.mdx' import Prerequisites from '../../connecting_to_your_data/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; import TabItem from '@theme/TabItem'; import Tabs from '@theme/Tabs'; Data Docs translate Expectations, Validation Results, and other metadata into human-readable documentation. Automatically compiling your data documentation from your data tests in the form of Data Docs keeps your documentation current. Use the information provided here to host and share Data Docs stored on a filesystem or a source data system.   Amazon S3  Create an S3 bucket  Configure your bucket policy  Apply the policy  Add a new S3 site to great_expectations.yml  Test your configuration  Additional notes    Microsoft Azure Blob Storage Host and share Data Docs on Azure Blob Storage. Data Docs are served using an Azure Blob Storage static website with restricted access. Prerequisites   A working deployment of Great Expectations Permissions to create and configure an Azure Storage account   Install Azure Storage Blobs client library for Python Run the following pip command in a terminal to install Azure Storage Blobs client library and its dependencies: markup title=\"Terminal command:\" pip install azure-storage-blob Create an Azure Blob Storage static website   Create a storage account.   In Settings, select Static website.   Select Enabled to enable static website hosting for the storage account.   Write \"index.html\" in the Index document.   Record the Primary endpoint URL. Your team will use this URL to view the Data Doc. A container named $web is added to your storage account to help you map a custom domain to this endpoint.   Configure the config_variables.yml file GX recommends storing Azure Storage credentials in the config_variables.yml file, which is located in the uncommitted/ folder by default, and is not part of source control. To review additional options for configuring the config_variables.yml file or additional environment variables, see Configure credentials.   Get the Connection string of the storage account you created.   Open the config_variables.yml file and then add the following entry below AZURE_STORAGE_CONNECTION_STRING:  yaml AZURE_STORAGE_CONNECTION_STRING: \"DefaultEndpointsProtocol=https;EndpointSuffix=core.windows.net;AccountName=<YOUR-STORAGE-ACCOUNT-NAME>;AccountKey=<YOUR-STORAGE-ACCOUNT-KEY==>\"   Add a new Azure site to the data_docs_sites section of your great_expectations.yml   Open the great_expectations.yml file and add the following entry: yaml data_docs_sites: local_site:     class_name: SiteBuilder     show_how_to_buttons: true     store_backend:     class_name: TupleFilesystemStoreBackend     base_directory: uncommitted/data_docs/local_site/     site_index_builder:     class_name: DefaultSiteIndexBuilder new_site_name:  # this is a user-selected name - you can select your own     class_name: SiteBuilder     store_backend:     class_name: TupleAzureBlobStoreBackend     container: \\$web     connection_string: ${AZURE_STORAGE_CONNECTION_STRING}     site_index_builder:     class_name: DefaultSiteIndexBuilder   Optional. Replace the default local_site to maintain a single Azure Data Docs site.   :::note  Since the container is named $web, setting container: $web in great_expectations.yml would cause GX to unsuccessfully try to find the web variable in config_variables.yml. Use an escape char \\ before the $ so the substitute_config_variable can locate the $web container. ::: You can also configure GX to store your Expectations and Validation Results in the Azure Storage account. See Configure Expectation Stores and Configure Validation Result Stores. Make sure you set container: \\$web correctly. The following options are available:  container: The name of the Azure Blob container to store your data in. connection_string: The Azure Storage connection string.   This can also be supplied by setting the AZURE_STORAGE_CONNECTION_STRING environment variable. prefix: All paths on blob storage will be prefixed with this string. account_url: The URL to the blob storage account. Any other entities included in the URL path (e.g. container or blob) will be discarded.   This URL can be optionally authenticated with a SAS token.   This can only be used if you don't configure the connection_string.    You can also configure this by setting the AZURE_STORAGE_ACCOUNT_URL environment variable.  The following authentication methods are supported:  SAS token authentication: append the SAS token to account_url or make sure it is set in the connection_string. Account key authentication: include the account key in the connection_string. When none of the above authentication methods are specified, the DefaultAzureCredential will be used which supports most common authentication methods.   You still need to provide the account url either through the config file or environment variable.  Build the Azure Blob Data Docs site You can create or modify an Expectation Suite and this will build the Data Docs website. Run the following Python code to build and open your Data Docs: python name=\"tests/integration/docusaurus/reference/glossary/data_docs.py data_docs_site\" Limit Data Docs access (Optional)   On your Azure Storage Account Settings, click Networking.   Allow access from Selected networks.   Optional. Add access to a Virtual Network.   Optional. Add IP ranges to the firewall. See Configure Azure Storage firewalls and virtual networks.     GCS Host and share Data Docs on Google Cloud Storage (GCS). GX recommends using IP-based access, which is achieved by deploying a Google App Engine application. To view the code used in the examples, see how_to_host_and_share_data_docs_on_gcs.py. Prerequisites   A Google Cloud project The Google Cloud SDK The gsutil command line tool Permissions to list and create buckets, deploy Google App Engine apps, add app firewall rules   Create a GCS bucket Run the following command to create a GCS bucket:  bash name=\"tests/integration/docusaurus/setup/configuring_data_docs/how_to_host_and_share_data_docs_on_gcs.py create bucket command\" Modify the project name, bucket name, and region. This is the output after you run the command: bash name=\"tests/integration/docusaurus/setup/configuring_data_docs/how_to_host_and_share_data_docs_on_gcs.py create bucket output\" Create a directory for your Google App Engine app GX recommends adding the directory to your project directory. For example, great_expectations/team_gcs_app.   Create and then open app.yaml and then add the following entry: yaml name=\"tests/integration/docusaurus/setup/configuring_data_docs/how_to_host_and_share_data_docs_on_gcs.py app yaml\"   Create and then open requirements.txt and then add the following entry: yaml name=\"tests/integration/docusaurus/setup/configuring_data_docs/how_to_host_and_share_data_docs_on_gcs.py requirements.txt\"   Create and then open main.py and then dd the following entry: python name=\"tests/integration/docusaurus/setup/configuring_data_docs/how_to_host_and_share_data_docs_on_gcs.py imports\"   Authenticate the gcloud CLI Run the following command to authenticate the gcloud CLI and set the project: bash name=\"tests/integration/docusaurus/setup/configuring_data_docs/how_to_host_and_share_data_docs_on_gcs.py gcloud login and set project\" Deploy your Google App Engine app Run the following CLI command from within the app directory you created previously: bash name=\"tests/integration/docusaurus/setup/configuring_data_docs/how_to_host_and_share_data_docs_on_gcs.py gcloud app deploy\" Set up the Google App Engine firewall See Creating firewall rules. Add a new GCS site to the data_docs_sites section of your great_expectations.yml Open great_expectations.yml and add the following entry: yaml name=\"tests/integration/docusaurus/setup/configuring_data_docs/how_to_host_and_share_data_docs_on_gcs.py data docs sites yaml\" Replace the default local_site to maintain a single GCS Data Docs site. To host a Data Docs site with a private DNS, you can configure a base_public_path for the Data Docs Store.  The following example configures a GCS site with the base_public_path set to www.mydns.com.  Data Docs are still written to the configured location on GCS. For example, https://storage.cloud.google.com/my_org_data_docs/index.html, but you will be able to access the pages from your DNS (http://www.mydns.com/index.html in the following example). yaml   data_docs_sites:     gs_site:  # this is a user-selected name - you may select your own       class_name: SiteBuilder       store_backend:         class_name: TupleGCSStoreBackend         project: <YOUR GCP PROJECT NAME>         bucket: <YOUR GCS BUCKET NAME>         base_public_path: http://www.mydns.com       site_index_builder:         class_name: DefaultSiteIndexBuilder Build the GCS Data Docs site Run the following Python code to build and open your Data Docs: python name=\"tests/integration/docusaurus/reference/glossary/data_docs.py data_docs_site\" Test the configuration In the gcloud CLI run gcloud app browse. If the command runs successfully, the URL is provided to your app and launched in a new browser window. The page displayed is the index page for your Data Docs site. Related documentation  Google App Engine Controlling App Access with Firewalls    Filesystem Host and share Data Docs on a filesystem. Prerequisites   A Great Expectations instance   Review the default settings Filesystem-hosted Data Docs are configured by default for Great Expectations deployments created using great_expectations init.  To create additional Data Docs sites, you may re-use the default Data Docs configuration below. You may replace local_site with your own site name, or leave the default. yaml data_docs_sites:   local_site:  # this is a user-selected name - you may select your own     class_name: SiteBuilder     store_backend:       class_name: TupleFilesystemStoreBackend       base_directory: uncommitted/data_docs/local_site/ # this is the default path but can be changed as required     site_index_builder:       class_name: DefaultSiteIndexBuilder Build the site Run the following Python code to build and open your Data Docs: python name=\"tests/integration/docusaurus/reference/glossary/data_docs.py data_docs\" To share the site, compress and distribute the directory in the base_directory key in your site configuration.  ", "\"Configure Expectation Stores\"": " sidebar_label: \"Configure Expectation Stores\" title: \"Configure Expectation Stores\" id: configure_expectation_stores description: Configure storage locations for Expectations. toc_min_heading_level: 2 toc_max_heading_level: 2  import Preface from './components_how_to_configure_an_expectation_store_in_amazon_s3/_preface.mdx' import InstallBoto3 from './components/_install_boto3_with_pip.mdx' import VerifyAwsCredentials from './components/_verify_aws_credentials_are_configured_properly.mdx' import IdentifyYourDataContextExpectationsStore from './components_how_to_configure_an_expectation_store_in_amazon_s3/_identify_your_data_context_expectations_store.mdx' import UpdateYourConfigurationFileToIncludeANewStoreForExpectationsOnS from './components_how_to_configure_an_expectation_store_in_amazon_s3/_update_your_configuration_file_to_include_a_new_store_for_expectations_on_s.mdx' import CopyExistingExpectationJsonFilesToTheSBucketThisStepIsOptional from './components_how_to_configure_an_expectation_store_in_amazon_s3/_copy_existing_expectation_json_files_to_the_s_bucket_this_step_is_optional.mdx' import ConfirmList from './components_how_to_configure_an_expectation_store_in_amazon_s3/_confirm_list.mdx' import Prerequisites from '../../connecting_to_your_data/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; import TabItem from '@theme/TabItem'; import Tabs from '@theme/Tabs'; An Expectation Store is a connector to store and retrieve information about collections of verifiable assertions about data. By default, new Profiled Expectations are stored as Expectation Suites in JSON format in the expectations/ subdirectory of your great_expectations/ folder. Use the information provided here to configure a store for your Expectations.   Amazon S3  Install boto3 with pip  Verify your AWS credentials  Identify your Data Context Expectations Store  Update your configuration file to include a new Store for Expectations  Copy existing Expectation JSON files to the S3 bucket (Optional)  Confirm Expectation Suite availability    Microsoft Azure Blob Storage Use the information provided here to configure a new storage location for Expectations in Microsoft Azure Blob Storage. Prerequisites   A Data Context. An Expectations Suite. An Azure Storage account. An Azure Blob container. If you need to host and share Data Docs on Azure Blob Storage, then you can set this up first and then use the $web existing container to store your Expectations. A prefix (folder) where to store Expectations. You don't need to create the folder, the prefix is just part of the Azure Blob name.   Configure the config_variables.yml file with your Azure Storage credentials GX recommends that you store Azure Storage credentials in the config_variables.yml file, which is located in the uncommitted/ folder by default, and is not part of source control. The following code adds Azure Storage credentials below the AZURE_STORAGE_CONNECTION_STRING key: yaml AZURE_STORAGE_CONNECTION_STRING: \"DefaultEndpointsProtocol=https;EndpointSuffix=core.windows.net;AccountName=<YOUR-STORAGE-ACCOUNT-NAME>;AccountKey=<YOUR-STORAGE-ACCOUNT-KEY==>\" To learn more about the additional options for configuring the config_variables.yml file, or additional environment variables, see How to configure credentials Identify your Data Context Expectations Store Your Expectations Store configuration is provided in your Data Context. Open great_expectations.yml and find the following entry: ```yaml expectations_store_name: expectations_store stores:   expectations_store:       class_name: ExpectationsStore       store_backend:           class_name: TupleFilesystemStoreBackend           base_directory: expectations/ ``` This configuration tells Great Expectations to look for Expectations in a Store named expectations_store. The default base_directory for expectations_store is expectations/. Update your configuration file to include a new Store for Expectations In the following example, expectations_store_name is set to expectations_AZ_store, but it can be personalized.  You also need to change the store_backend settings.  The class_name is TupleAzureBlobStoreBackend, container is the name of your blob container where Expectations are stored, prefix is the folder in the container where Expectations are located, and connection_string is ${AZURE_STORAGE_CONNECTION_STRING} to reference the corresponding key in the config_variables.yml file. ```yaml expectations_store_name: expectations_AZ_store stores:   expectations_AZ_store:       class_name: ExpectationsStore       store_backend:         class_name: TupleAzureBlobStoreBackend         container:          prefix: expectations         connection_string: ${AZURE_STORAGE_CONNECTION_STRING} ``` :::note If the container for hosting and sharing Data Docs on Azure Blob Storage is named $web, use container: \\$web to allow access to the $webcontainer. ::: Additional authentication and configuration options are available. See Hosting and sharing Data Docs on Azure Blob Storage. Copy existing Expectation JSON files to the Azure blob (Optional) You can use the az storage blob upload command to copy Expectations into Azure Blob Storage. The following command copies the Expectation exp1 from a local folder to Azure Blob Storage:  ```bash export AZURE_STORAGE_CONNECTION_STRING=\"DefaultEndpointsProtocol=https;EndpointSuffix=core.windows.net;AccountName=;AccountKey=\" az storage blob upload -f  -c  -n / example : az storage blob upload -f great_expectations/expectations/exp1.json -c  -n expectations/exp1.json Finished[#############################################################]  100.0000% { \"etag\": \"\\\"0x8D8E08E5DA47F84\\\"\", \"lastModified\": \"2021-03-06T10:55:33+00:00\" } ``` To learn more about other methods that are available to copy Expectation JSON files into Azure Blob Storage, see Introduction to Azure Blob Storage. Confirm that the new Expectation Suites have been added If you copied your existing Expectation Suites to Azure Blob Storage, run the following Python command to confirm that Great Expectations can find them:  ```python import great_expectations as gx context = gx.get_context() context.list_expectation_suite_names() ``` A list of Expectations you copied to Azure Blob Storage is returned. Expectations that weren't copied to the new folder are not listed. Confirm that Expectations can be accessed from Azure Blob Storage Run the following command to confirm your Expectations have been copied to Azure Blob Storage:  bash great_expectations suite list If your Expectations have not been copied to Azure Blob Storage, the message \"No Expectations were found\" is returned.   GCS Use the information provided here to configure a new storage location for Expectations in GCS. To view all the code used in this topic, see how_to_configure_an_expectation_store_in_gcs.py. Prerequisites   A Data Context. An Expectations Suite. A GCP service account with credentials that allow access to GCP resources such as Storage Objects. A GCP project, GCS bucket, and prefix to store Expectations.   Configure your GCP credentials Confirm that your environment is configured with the appropriate authentication credentials needed to connect to the GCS bucket where Expectations will be stored. This includes the following:  A GCP service account. Setting the GOOGLE_APPLICATION_CREDENTIALS environment variable. Verifying authentication by running a Google Cloud Storage client library script.  For more information about validating your GCP authentication credentials, see Authenticate to Cloud services using client libraries. Identify your Data Context Expectations Store The configuration for your Expectations Store is available in your Data Context. Open great_expectations.yml and find the following entry:  yaml name=\"tests/integration/docusaurus/setup/configuring_metadata_stores/how_to_configure_an_expectation_store_in_gcs.py expected_existing_expectations_store_yaml\" This configuration tells Great Expectations to look for Expectations in the expectations_store Store. The default base_directory for expectations_store is expectations/. Update your configuration file to include a new store for Expectations In the following example, expectations_store_name is set to expectations_GCS_store, but it can be personalized.  You also need to change the store_backend settings. The class_name is TupleGCSStoreBackend, project is your GCP project, bucket is the address of your GCS bucket, and prefix is the folder on GCS where Expectations are stored. yaml name=\"tests/integration/docusaurus/setup/configuring_metadata_stores/how_to_configure_an_expectation_store_in_gcs.py configured_expectations_store_yaml\" :::warning If you are also storing Validations in GCS or DataDocs in GCS, make sure that the prefix values are disjoint and one is not a substring of the other. ::: Copy existing Expectation JSON files to the GCS bucket (Optional) Use the gsutil cp command to copy Expectations into GCS. For example, the following command copies the Expectation `my_expectation_suite from a local folder into a GCS bucket: bash name=\"tests/integration/docusaurus/setup/configuring_metadata_stores/how_to_configure_an_expectation_store_in_gcs.py copy_expectation_command\" The following confirmation message is returned: bash name=\"tests/integration/docusaurus/setup/configuring_metadata_stores/how_to_configure_an_expectation_store_in_gcs.py copy_expectation_output\" Additional methods for copying Expectations into GCS are available. See Upload objects from a filesystem. Confirm that the new Expectation Suites have been added If you copied your existing Expectation Suites to GCS, run the following Python command to confirm that Great Expectations can find them:  ```python import great_expectations as gx context = gx.get_context() context.list_expectation_suite_names() ``` A list of Expectation Suites you copied to GCS is returned. Expectation Suites that weren't copied to the new Store aren't listed. Confirm that Expectations can be accessed from GCS Run the following command to confirm your Expectations were copied to GCS: bash name=\"tests/integration/docusaurus/setup/configuring_metadata_stores/how_to_configure_an_expectation_store_in_gcs.py list_expectation_suites_command\" If your Expectations were not copied to Azure Blob Storage, a message indicating no Expectations were found is returned.   Filesystem Use the information provided here to configure a new storage location for Expectations on your Filesystem. Prerequisites   A Data Context. An Expectation Suite. A storage location for Expectations. This can be a local path, or a path to a network filesystem.   Create a new folder for Expectations Run the following command to create a new folder for your Expectations and move your existing Expectations to the new folder: ```bash in the great_expectations/ folder mkdir shared_expectations mv expectations/npi_expectations.json shared_expectations/ ` In this example, the name of the Expectation isnpi_expectationsand the path to the new storage location is/shared_expectations``. Identify your Data Context Expectations Store The configuration for your Expectations Store is available in your Data Context.  Open great_expectations.ymland find the following entry: ```yaml expectations_store_name: expectations_store stores:   expectations_store:       class_name: ExpectationsStore       store_backend:           class_name: TupleFilesystemStoreBackend           base_directory: expectations/ ` This configuration tells Great Expectations to look for Expectations in theexpectations_storeStore. The defaultbase_directoryforexpectations_storeisexpectations/``. Update your configuration file to include a new Store for Expectations results In the following example, expectations_store_name is set to shared_expectations_filesystem_store, but it can be personalized.  Also, base_directory is set to shared_expectations/, but you can set it to another path that is accessible by Great Expectations. ```yaml expectations_store_name: shared_expectations_filesystem_store stores:   shared_expectations_filesystem_store:       class_name: ExpectationsStore       store_backend:           class_name: TupleFilesystemStoreBackend           base_directory: shared_expectations/ ``` Confirm that the new Expectation Suites have been added If you copied your existing Expectation Suites to your filesystem, run the following Python command to confirm that Great Expectations can find them:  ```python import great_expectations as gx context = gx.get_context() context.list_expectation_suite_names() ``` A list of Expectation Suites you copied your filesystem is returned. Expectation Suites that weren't copied to the new Store aren't listed. Version control systems GX recommends that you store Expectations in a version control system such as Git. The JSON format of Expectations allows for informative diff-statements and modification tracking. In the following example, the `expect_table_column_count_to_equal value changes from 333 to 331, and then to 330: ```bash git log -p npi_expectations.json commit cbc127fb27095364c3c1fcbf6e7f078369b07455   changed expect_table_column_count_to_equal to 331 diff --git a/great_expectations/expectations/npi_expectations.json b/great_expectations/expectations/npi_expectations.json --- a/great_expectations/expectations/npi_expectations.json +++ b/great_expectations/expectations/npi_expectations.json @@ -17,7 +17,7 @@    {      \"expectation_type\": \"expect_table_column_count_to_equal\",      \"kwargs\": { -        \"value\": 333 +        \"value\": 331      } commit 05b3c8c1ed35d183bac1717d4877fe13bc574963 changed expect_table_column_count_to_equal to 333 diff --git a/great_expectations/expectations/npi_expectations.json b/great_expectations/expectations/npi_expectations.json --- a/great_expectations/expectations/npi_expectations.json +++ b/great_expectations/expectations/npi_expectations.json    {      \"expectation_type\": \"expect_table_column_count_to_equal\",      \"kwargs\": { -        \"value\": 330 +        \"value\": 333      } ```   PostgreSQL Use the information provided here to configure an Expectations store in a PostgreSQL database. Prerequisites   A Data Context. An Expectations Suite. A PostgreSQL database with appropriate credentials.   Configure the config_variables.yml file with your database credentials GX recommends storing database credentials in the config_variables.yml file, which is located in the uncommitted/ folder by default, and not part of source control.  To add database credentials, open config_variables.yml and add the following entry below the db_creds key:  yaml     db_creds:       drivername: postgresql       host: '<your_host_name>'       port: '<your_port>'       username: '<your_username>'       password: '<your_password>'       database: '<your_database_name>' To configure the config_variables.yml file, or additional environment variables, see How to configure credentials. Identify your Data Context Expectations Store Open great_expectations.ymland find the following entry: ```yaml expectations_store_name: expectations_store stores:   expectations_store:       class_name: ExpectationsStore       store_backend:           class_name: TupleFilesystemStoreBackend           base_directory: expectations/ ``` This configuration tells Great Expectations to look for Expectations in the expectations_store Store. The default base_directory for expectations_store is expectations/. Update your configuration file to include a new Store for Expectations In the following example, expectations_store_name is set to expectations_postgres_store, but it can be personalized. You also need to make some changes to the store_backend settings.  The class_name is DatabaseStoreBackend, and credentials is ${db_creds} to reference the corresponding key in the config_variables.yml file. ```yaml expectations_store_name: expectations_postgres_store stores:   expectations_postgres_store:       class_name: ExpectationsStore       store_backend:           class_name: DatabaseStoreBackend           credentials: ${db_creds} ```  ", "\"Configure Validation Result Stores\"": " sidebar_label: \"Configure Validation Result Stores\" title: \"Configure Validation Result Stores\" id: configure_result_stores description: Configure storage locations for Validation Results. toc_min_heading_level: 2 toc_max_heading_level: 2  import Preface from './components_how_to_configure_a_validation_result_store_in_amazon_s3/_preface.mdx' import ConfigureBotoToConnectToTheAmazonSBucketWhereValidationResultsWillBeStored from './components/_install_boto3_with_pip.mdx' import VerifyYourAwsCredentials from './components/_verify_aws_credentials_are_configured_properly.mdx' import IdentifyYourDataContextValidationResultsStore from './components_how_to_configure_a_validation_result_store_in_amazon_s3/_identify_your_data_context_validation_results_store.mdx' import UpdateYourConfigurationFileToIncludeANewStoreForValidationResultsOnS from './components_how_to_configure_a_validation_result_store_in_amazon_s3/_update_your_configuration_file_to_include_a_new_store_for_validation_results_on_s.mdx' import CopyExistingValidationResultsToTheSBucketThisStepIsOptional from './components_how_to_configure_a_validation_result_store_in_amazon_s3/_copy_existing_validation_results_to_the_s_bucket_this_step_is_optional.mdx' import ConfirmThatTheValidationsResultsStoreHasBeenCorrectlyConfigured from './components_how_to_configure_a_validation_result_store_in_amazon_s3/_confirm_that_the_validations_results_store_has_been_correctly_configured.mdx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; import TabItem from '@theme/TabItem'; import Tabs from '@theme/Tabs'; A Validation Results Store is a connector that is used to store and retrieve information about objects generated when data is Validated against an Expectation. By default, Validation Results are stored in JSON format in the uncommitted/validations/ subdirectory of your great_expectations/ folder. Use the information provided here to configure a store for your Validation Results. :::caution Validation Results can include sensitive or regulated data that should not be committed to a source control system. :::   Amazon S3  Install boto3 in your local environment  Verify your AWS credentials are properly configured  Identify your Data Context Validation Results Store  Update your configuration file to include a new Store for Validation Results  Copy existing Validation results to the S3 bucket (Optional)  Confirm the configuration    Microsoft Azure Blob Storage Use the information provided here to configure a new storage location for Validation Results in Azure Blob Storage. Prerequisites   A Data Context. An Expectations Suite. A Checkpoint. An Azure Storage account and get the connection string. An Azure Blob container. If you want to host and share Data Docs on Azure Blob Storage, you can set this up first and then use the $web existing container to store your Expectations. A prefix (folder) to store Validation Results. You don't need to create the folder, the prefix is just part of the Blob name.   Configure the config_variables.yml file with your Azure Storage credentials GX recommends that you store Azure Storage credentials in the config_variables.yml file, which is located in the uncommitted/ folder by default, and is not part of source control. The following code adds Azure Storage credentials under the key AZURE_STORAGE_CONNECTION_STRING:  yaml AZURE_STORAGE_CONNECTION_STRING: \"DefaultEndpointsProtocol=https;EndpointSuffix=core.windows.net;AccountName=<YOUR-STORAGE-ACCOUNT-NAME>;AccountKey=<YOUR-STORAGE-ACCOUNT-KEY==>\" To learn more about the additional options for configuring the config_variables.yml file, or additional environment variables, see How to configure credentials Identify your Validation Results Store Your Validation Results Store configuration is provided in your Data Context. Open great_expectations.yml and find the following entry:  ```yaml validations_store_name: validations_store stores:   validations_store:       class_name: ValidationsStore       store_backend:           class_name: TupleFilesystemStoreBackend           base_directory: uncommitted/validations/ ` This configuration tells Great Expectations to look for Validation Results in a Store namedvalidations_store. The defaultbase_directoryforvalidations_storeisuncommitted/validations/``. Update your configuration file to include a new Store for Validation Results on Azure Storage account In the following example, validations_store_name is set to validations_AZ_store, but it can be personalized.  You also need to change the store_backend settings.  The class_name is TupleAzureBlobStoreBackend, container is the name of your blob container where Validation Results are stored, prefix is the folder in the container where Validation Result files are located, and connection_string is ${AZURE_STORAGE_CONNECTION_STRING}to reference the corresponding key in the config_variables.yml file. ```yaml validations_store_name: validations_AZ_store stores:   validations_AZ_store:       class_name: ValidationsStore       store_backend:           class_name: TupleAzureBlobStoreBackend           container:            prefix: validations           connection_string: ${AZURE_STORAGE_CONNECTION_STRING} ``` :::note If the container for hosting and sharing Data Docs on Azure Blob Storage is named $web, use container: \\$web to allow access to the $webcontainer. ::: Additional authentication and configuration options are available. See Host and Share Data Docs on Azure Blob Storage. Copy existing Validation Results JSON files to the Azure blob (Optional) You can use the az storage blob upload command to copy Validation Results into Azure Blob Storage. The following command copies one Validation Result from a local folder to the Azure blob:  bash export AZURE_STORAGE_CONNECTION_STRING=\"DefaultEndpointsProtocol=https;EndpointSuffix=core.windows.net;AccountName=<YOUR-STORAGE-ACCOUNT-NAME>;AccountKey=<YOUR-STORAGE-ACCOUNT-KEY==>\" az storage blob upload -f <local/path/to/validation.json> -c <GREAT-EXPECTATION-DEDICATED-AZURE-BLOB-CONTAINER-NAME> -n <PREFIX>/<validation.json> example with a validation related to the exp1 expectation: az storage blob upload -f great_expectations/uncommitted/validations/exp1/20210306T104406.877327Z/20210306T104406.877327Z/8313fb37ca59375eb843adf388d4f882.json -c <blob-container> -n validations/exp1/20210306T104406.877327Z/20210306T104406.877327Z/8313fb37ca59375eb843adf388d4f882.json Finished[#############################################################]  100.0000% { \"etag\": \"\\\"0x8D8E09F894650C7\\\"\", \"lastModified\": \"2021-03-06T12:58:28+00:00\" } To learn more about other methods that are available to copy Validation Result JSON files into Azure Blob Storage, see Quickstart: Upload, download, and list blobs with the Azure portal. Reference the new configuration To make Great Expectations look for Validation Results on the Azure store, set the validations_store_name variable to the name of your Azure Validations Store. In the previous example this was validations_AZ_store. Confirm that the Validation Results Store has been correctly configured Run a Checkpoint to store results in the new Validation Results Store on Azure Blob and then visualize the results by re-building Data Docs.   GCS Use the information provided here to configure a new storage location for Validation Results in GCS. To view all the code used in this topic, see how_to_configure_a_validation_result_store_in_gcs.py. Prerequisites   A Data Context. An Expectations Suite. A Checkpoint. A GCP service account with credentials that allow access to GCP resources such as Storage Objects. A GCP project, GCS bucket, and prefix to store Validation Results.   Configure your GCP credentials Confirm that your environment is configured with the appropriate authentication credentials needed to connect to the GCS bucket where Validation Results will be stored. This includes the following:  A GCP service account. Setting the GOOGLE_APPLICATION_CREDENTIALS environment variable. Verifying authentication by running a Google Cloud Storage client library script.  For more information about validating your GCP authentication credentials, see Authenticate to Cloud services using client libraries. Identify your Data Context Validation Results Store The configuration for your Validation Results Store is available in your Data Context. Open great_expectations.ymland find the following entry:  yaml name=\"tests/integration/docusaurus/setup/configuring_metadata_stores/how_to_configure_a_validation_result_store_in_gcs.py expected_existing_validations_store_yaml\" This configuration tells Great Expectations to look for Validation Results in the validations_store Store. The default base_directory for validations_store is uncommitted/validations/. Update your configuration file to include a new Store for Validation Results In the following example, validations_store_name is set to validations_GCS_store, but it can be personalized.  You also need to change the store_backend settings. The class_name is TupleGCSStoreBackend, project is your GCP project, bucket is the address of your GCS bucket, and prefix is the folder on GCS where Validation Result files are stored. yaml name=\"tests/integration/docusaurus/setup/configuring_metadata_stores/how_to_configure_a_validation_result_store_in_gcs.py configured_validations_store_yaml\" :::warning If you are also storing Expectations in GCS or DataDocs in GCS, make sure that the prefix values are disjoint and one is not a substring of the other. ::: Copy existing Validation Results to the GCS bucket (Optional) Use the gsutil cp command to copy Validation Results into GCS. For example, the following command copies the Validation results validation_1 and validation_2into a GCS bucket:  bash name=\"tests/integration/docusaurus/setup/configuring_metadata_stores/how_to_configure_a_validation_result_store_in_gcs.py copy_validation_command\" The following confirmation message is returned: bash name=\"tests/integration/docusaurus/setup/configuring_metadata_stores/how_to_configure_a_validation_result_store_in_gcs.py copy_validation_output\" Additional methods for copying Validation Results into GCS are available. See Upload objects from a filesystem. Reference the new configuration To make Great Expectations look for Validation Results on the GCS store, set the validations_store_name variable to the name of your GCS Validations Store. In the previous example this was validations_GCS_store. Confirm that the Validation Results Store has been correctly configured Run a Checkpoint to store results in the new Validation Results Store on GCS, and then visualize the results by re-building Data Docs.   Filesystem Use the information provided here to configure a new storage location for Validation Results in your filesystem. You'll learn how to use an Action to update Data Docs sites with new Validation Results from Checkpoint runs. Prerequisites   A Data Context. An Expectation Suite . A Checkpoint. A new storage location to store Validation Results. This can be a local path, or a path to a secure network filesystem.   Create a new folder for Validation Results Run the following command to create a new folder for your Validation Results and move your existing Validation Results to the new folder: ```bash in the great_expectations/ folder mkdir shared_validations mv uncommitted/validations/npi_validations/ uncommitted/shared_validations/ ` In this example, the name of the Validation Result isnpi_validationsand the path to the new storage location isshared_validations/``. Identify your Data Context Validation Results Store The configuration for your Validation Results Store is available in your Data Context.  Open great_expectations.ymland find the following entry:  ```yaml validations_store_name: validations_store stores:    validations_store:        class_name: ValidationsStore        store_backend:            class_name: TupleFilesystemStoreBackend            base_directory: uncommitted/validations/ ``` This configuration tells Great Expectations to look for Validation Results in the validations_store Store. The default base_directory for validations_store is uncommitted/validations/. Update your configuration file to include a new Store for Validation results In the following example, validations_store_name is set to shared_validations_filesystem_store, but it can be personalized.  Also, base_directory is set to uncommitted/shared_validations/, but you can set it to another path that is accessible by Great Expectations. ```yaml validations_store_name: shared_validations_filesystem_store stores:    shared_validations_filesystem_store:        class_name: ValidationsStore        store_backend:            class_name: TupleFilesystemStoreBackend            base_directory: uncommitted/shared_validations/ ``` Confirm that the Validation Results Store has been correctly configured Run a Checkpoint to store results in the new Validation Results Store in your new location, and then visualize the results by re-building Data Docs.   PostgreSQL Use the information provided here to configure Great Expectations to store Validation Results in a PostgreSQL database. Prerequisites   A Data Context. An Expectations Suite. A Checkpoint. A PostgreSQL database with appropriate credentials.   Configure the config_variables.yml file with your database credentials GX recommends storing database credentials in the config_variables.yml file, which is located in the uncommitted/ folder by default, and not part of source control.    To add database credentials, open config_variables.yml and add the following entry below the db_creds key:  yaml db_creds:   drivername: postgresql   host: '<your_host_name>'   port: '<your_port>'   username: '<your_username>'   password: '<your_password>'   database: '<your_database_name>' To configure the config_variables.yml file, or additional environment variables, see How to configure credentials.   Optional. To use a specific schema as the backend, specify schema as an additional keyword argument. For example: yaml db_creds:   drivername: postgresql   host: '<your_host_name>'   port: '<your_port>'   username: '<your_username>'   password: '<your_password>'   database: '<your_database_name>'   schema: '<your_schema_name>'   Identify your Data Context Validation Results Store The configuration for your Validation Results Store is available in your Data Context.  Open great_expectations.ymland find the following entry: ```yaml validations_store_name: validations_store stores:   validations_store:       class_name: ValidationsStore       store_backend:           class_name: TupleFilesystemStoreBackend           base_directory: uncommitted/validations/ ` This configuration tells Great Expectations to look for Validation Results in thevalidations_storeStore. The defaultbase_directoryforvalidations_storeisuncommitted/validations/``. Update your configuration file to include a new Validation Results Store Add the following entry to your great_expectations.yml:  ```yaml validations_store_name: validations_postgres_store stores:   validations_postgres_store:       class_name: ValidationsStore       store_backend:           class_name: DatabaseStoreBackend           credentials: ${db_creds} ``` In the previous example, validations_store_name is set to validations_postgres_store, but it can be personalized.  Also, class_name is set to DatabaseStoreBackend, and credentials is set to ${db_creds}, which references the corresponding key in the config_variables.yml file.   Confirm the addition of the new Validation Results Store In the previous example, a validations_store on the local filesystem and a validations_postgres_store are configured.  Great Expectations looks for Validation Results in PostgreSQL when the validations_store_name variable is set to validations_postgres_store. Run the following command to remove validations_store and confirm the validations_postgres_store configuration: ```bash great_expectations store list   name: validations_store class_name: ValidationsStore store_backend:   class_name: TupleFilesystemStoreBackend   base_directory: uncommitted/validations/   name: validations_postgres_store class_name: ValidationsStore store_backend:   class_name: DatabaseStoreBackend   credentials:       database: ''       drivername: postgresql       host: ''       password: **       port: ''       username: '' ```   Confirm the Validation Results Store is configured correctly Run a Checkpoint to store results in the new Validation Results store in PostgreSQL, and then visualize the results by re-building Data Docs. Great Expectations creates a new table in your database named ge_validations_store, and populates the fields with information from the Validation Results.  ", "Configure a MetricStore": " title: Configure a MetricStore import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; :::note Note: Metric storage is an experimental feature. ::: A MetricStore is a Store that stores Metrics computed during Validation. A MetricStore tracks the run_id of the Validation and the Expectation Suite name in addition to the Metric name and Metric kwargs. Saving Metrics during Validation lets you construct a new data series based on observed dataset characteristics computed by Great Expectations. A data series can serve as the source for a dashboard, or overall data quality metrics. Prerequisites  A Great Expectations instance Completion of the Quickstart A configured Data Context  Add a MetricStore To define a MetricStore, add a Metric Store configuration to the stores section of your great_expectations.yml. The configuration must include the following keys:   class_name - Enter MetricStore. This key determines which class is instantiated to create the StoreBackend. Other fields are passed through to the StoreBackend class on instantiation. The only backend Store under test for use with a MetricStore is the DatabaseStoreBackend with Postgres.   store_backend - Defines how your metrics are persisted.    To use an SQL Database such as Postgres, add the following fields and values:    class_name - Enter DatabaseStoreBackend.    credentials - Point to the credentials defined in your config_variables.yml, or define them inline.   The following is an example of how the MetricStore configuration appears in great_expectations.yml: yaml stores:     #  ...     metric_store:  # You can choose any name as the key for your metric store         class_name: MetricStore         store_backend:             class_name: DatabaseStoreBackend             credentials: ${my_store_credentials}             # alternatively, define credentials inline:             # credentials:             #  username: my_username             #  password: my_password             #  port: 1234             #  host: xxxx             #  database: my_database             #  driver: postgresql The next time your Data Context is loaded, it will connect to the database and initialize a table to store metrics if one has not already been created. Configure a Validation Action When a MetricStore is available, add a StoreMetricsAction validation Action to your Checkpoint to save Metrics during Validation. The validation Action must include the following fields:   class_name - Enter StoreMetricsAction. Determines which class is instantiated to execute the Action.   target_store_name - Enter the key for the MetricStore you added in your great_expectations.yml. In the previous example, the metrics_storefield defines which Store backend to use when persisting the metrics.   requested_metrics - Identify the Expectation Suites and Metrics you want to store.   Add the following entry to great_expectations.yml to generate Validation Result statistics: yaml   expectation_suite_name:     statistics.<statistic name> Add the following entry to great_expectations.yml to generate values from a specific Expectation result field: yaml   expectation_suite_name:     - column:       <column name>:         <expectation name>.result.<value name> To indicate that any Expectation Suite can be used to generate values, use the wildcard \"*\".  :::note Note: If you use an Expectation Suite name as a key, Metrics are only added to the MetricStore when the Expectation Suite runs. When you use the wildcard \"*\", Metrics are added to the MetricStore for each Expectation Suite that runs in the Checkpoint. ::: The following example yaml configuration adds StoreMetricsAction to the taxi_data dataset: ``` action_list: ...  name: store_metrics   action:     class_name: StoreMetricsAction     target_store_name: metric_store  # This should match the name of the store configured above     requested_metrics:       public.taxi_data.warning:  # match a particular expectation suite         - column:             passenger_count:               - expect_column_values_to_not_be_null.result.element_count               - expect_column_values_to_not_be_null.result.partial_unexpected_list         - statistics.successful_expectations       \"*\":  # wildcard to match any expectation suite         - statistics.evaluated_expectations         - statistics.success_percent         - statistics.unsuccessful_expectations ```  Test your MetricStore and StoreMetricsAction Run the following command to run your Checkpoint and test StoreMetricsAction:  python import great_expectations as gx context = gx.get_context() checkpoint_name = \"your checkpoint name here\" context.run_checkpoint(checkpoint_name=checkpoint_name)", "How to configure a Validation Result Store in Amazon S3": " title: How to configure a Validation Result Store in Amazon S3 import Preface from './components_how_to_configure_a_validation_result_store_in_amazon_s3/_preface.mdx' import ConfigureBotoToConnectToTheAmazonSBucketWhereValidationResultsWillBeStored from './components/_install_boto3_with_pip.mdx' import VerifyYourAwsCredentials from './components/_verify_aws_credentials_are_configured_properly.mdx' import IdentifyYourDataContextValidationResultsStore from './components_how_to_configure_a_validation_result_store_in_amazon_s3/_identify_your_data_context_validation_results_store.mdx' import UpdateYourConfigurationFileToIncludeANewStoreForValidationResultsOnS from './components_how_to_configure_a_validation_result_store_in_amazon_s3/_update_your_configuration_file_to_include_a_new_store_for_validation_results_on_s.mdx' import CopyExistingValidationResultsToTheSBucketThisStepIsOptional from './components_how_to_configure_a_validation_result_store_in_amazon_s3/_copy_existing_validation_results_to_the_s_bucket_this_step_is_optional.mdx' import ConfirmThatTheValidationsResultsStoreHasBeenCorrectlyConfigured from './components_how_to_configure_a_validation_result_store_in_amazon_s3/_confirm_that_the_validations_results_store_has_been_correctly_configured.mdx'  1. Install boto3 in your local environment  2. Verify your AWS credentials are properly configured  3. Identify your Data Context Validation Results Store  4. Update your configuration file to include a new Store for Validation Results  5. Copy existing Validation results to the S3 bucket (Optional)  6. Confirm the Validations Results Store configuration ", "How to configure a Validation Result Store in Azure Blob Storage": " title: How to configure a Validation Result Store in Azure Blob Storage import Prerequisites from '../../connecting_to_your_data/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx' By default, Validation Results are stored in JSON format in the uncommitted/validations/ subdirectory of your great_expectations/ folder. Validation Results might include sensitive or regulated data that should not be committed to a source control system. Use the information provided here to configure a new storage location for Validation Results in Azure Blob Storage. Prerequisites   A Data Context. An Expectations Suite. A Checkpoint. An Azure Storage account and get the connection string. An Azure Blob container. If you want to host and share Data Docs on Azure Blob Storage, you can set this up first and then use the $web existing container to store your Expectations. A prefix (folder) to store Validation Results. You don't need to create the folder, the prefix is just part of the Blob name.   1. Configure the config_variables.yml file with your Azure Storage credentials GX recommends that you store Azure Storage credentials in the config_variables.yml file, which is located in the uncommitted/ folder by default, and is not part of source control. The following code adds Azure Storage credentials under the key AZURE_STORAGE_CONNECTION_STRING:  yaml AZURE_STORAGE_CONNECTION_STRING: \"DefaultEndpointsProtocol=https;EndpointSuffix=core.windows.net;AccountName=<YOUR-STORAGE-ACCOUNT-NAME>;AccountKey=<YOUR-STORAGE-ACCOUNT-KEY==>\" To learn more about the additional options for configuring the config_variables.yml file, or additional environment variables, see How to configure credentials 2. Identify your Validation Results Store Your Validation Results Store configuration is provided in your Data Context. Open great_expectations.yml and find the following entry:  ```yaml validations_store_name: validations_store stores:   validations_store:       class_name: ValidationsStore       store_backend:           class_name: TupleFilesystemStoreBackend           base_directory: uncommitted/validations/ ` This configuration tells Great Expectations to look for Validation Results in a Store namedvalidations_store. The defaultbase_directoryforvalidations_storeisuncommitted/validations/``. 3. Update your configuration file to include a new Store for Validation Results on Azure Storage account In the following example, validations_store_name is set to validations_AZ_store, but it can be personalized.  You also need to change the store_backend settings.  The class_name is TupleAzureBlobStoreBackend, container is the name of your blob container where Validation Results are stored, prefix is the folder in the container where Validation Result files are located, and connection_string is ${AZURE_STORAGE_CONNECTION_STRING}to reference the corresponding key in the config_variables.yml file. ```yaml validations_store_name: validations_AZ_store stores:   validations_AZ_store:       class_name: ValidationsStore       store_backend:           class_name: TupleAzureBlobStoreBackend           container:            prefix: validations           connection_string: ${AZURE_STORAGE_CONNECTION_STRING} ``` :::note If the container for hosting and sharing Data Docs on Azure Blob Storage is named $web, use container: \\$web to allow access to the $webcontainer. ::: Additional authentication and configuration options are available. See Hosting and sharing Data Docs on Azure Blob Storage. 4. Copy existing Validation Results JSON files to the Azure blob (Optional) You can use the az storage blob upload command to copy Validation Results into Azure Blob Storage. The following command copies one Validation Result from a local folder to the Azure blob:  bash export AZURE_STORAGE_CONNECTION_STRING=\"DefaultEndpointsProtocol=https;EndpointSuffix=core.windows.net;AccountName=<YOUR-STORAGE-ACCOUNT-NAME>;AccountKey=<YOUR-STORAGE-ACCOUNT-KEY==>\" az storage blob upload -f <local/path/to/validation.json> -c <GREAT-EXPECTATION-DEDICATED-AZURE-BLOB-CONTAINER-NAME> -n <PREFIX>/<validation.json> example with a validation related to the exp1 expectation: az storage blob upload -f great_expectations/uncommitted/validations/exp1/20210306T104406.877327Z/20210306T104406.877327Z/8313fb37ca59375eb843adf388d4f882.json -c <blob-container> -n validations/exp1/20210306T104406.877327Z/20210306T104406.877327Z/8313fb37ca59375eb843adf388d4f882.json Finished[#############################################################]  100.0000% { \"etag\": \"\\\"0x8D8E09F894650C7\\\"\", \"lastModified\": \"2021-03-06T12:58:28+00:00\" } To learn more about other methods that are available to copy Validation Result JSON files into Azure Blob Storage, see Quickstart: Upload, download, and list blobs with the Azure portal. 5. Reference the new configuration To make Great Expectations look for Validation Results on the Azure store, set the validations_store_name variable to the name of your Azure Validations Store. In the previous example this was validations_AZ_store. 6. Confirm that the Validation Results Store has been correctly configured Run a Checkpoint to store results in the new Validation Results Store on Azure Blob and then visualize the results by re-building Data Docs.", "How to configure a Validation Result store in GCS": " title: How to configure a Validation Result store in GCS import Prerequisites from '../../connecting_to_your_data/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; By default, Validation Results are stored in JSON format in the uncommitted/validations/ subdirectory of your great_expectations/ folder. Validation Results can include sensitive or regulated data that should not be committed to a source control system.  Use the information provided here to configure a new storage location for Validation Results in Google Cloud Storage (GCS). To view all the code used in this topic, see how_to_configure_a_validation_result_store_in_gcs.py. Prerequisites   A Data Context. An Expectations Suite. A Checkpoint. A GCP service account with credentials that allow access to GCP resources such as Storage Objects. A GCP project, GCS bucket, and prefix to store Validation Results.   1. Configure your GCP credentials Confirm that your environment is configured with the appropriate authentication credentials needed to connect to the GCS bucket where Validation Results will be stored. This includes the following:  A GCP service account. Setting the GOOGLE_APPLICATION_CREDENTIALS environment variable. Verifying authentication by running a Google Cloud Storage client library script.  For more information about validating your GCP authentication credentials, see Authenticate to Cloud services using client libraries. 2. Identify your Data Context Validation Results Store The configuration for your Validation Results Store is available in your Data Context. Open great_expectations.ymland find the following entry:  yaml name=\"tests/integration/docusaurus/setup/configuring_metadata_stores/how_to_configure_a_validation_result_store_in_gcs.py expected_existing_validations_store_yaml\" This configuration tells Great Expectations to look for Validation Results in the validations_store Store. The default base_directory for validations_store is uncommitted/validations/. 3. Update your configuration file to include a new Store for Validation Results In the following example, validations_store_name is set to validations_GCS_store, but it can be personalized.  You also need to change the store_backend settings. The class_name is TupleGCSStoreBackend, project is your GCP project, bucket is the address of your GCS bucket, and prefix is the folder on GCS where Validation Result files are stored. yaml name=\"tests/integration/docusaurus/setup/configuring_metadata_stores/how_to_configure_a_validation_result_store_in_gcs.py configured_validations_store_yaml\" :::warning If you are also storing Expectations in GCS or DataDocs in GCS, make sure that the prefix values are disjoint and one is not a substring of the other. ::: 4. Copy existing Validation Results to the GCS bucket (Optional) Use the gsutil cp command to copy Validation Results into GCS. For example, the following command copies the Validation results validation_1 and validation_2into a GCS bucket:  bash name=\"tests/integration/docusaurus/setup/configuring_metadata_stores/how_to_configure_a_validation_result_store_in_gcs.py copy_validation_command\" The following confirmation message is returned: bash name=\"tests/integration/docusaurus/setup/configuring_metadata_stores/how_to_configure_a_validation_result_store_in_gcs.py copy_validation_output\" Additional methods for copying Validation Results into GCS are available. See Upload objects from a filesystem. 5. Reference the new configuration To make Great Expectations look for Validation Results on the GCS store, set the validations_store_name variable to the name of your GCS Validations Store. In the previous example this was validations_GCS_store. 6. Confirm that the Validation Results Store has been correctly configured Run a Checkpoint to store results in the new Validation Results Store on GCS, and then visualize the results by re-building Data Docs.", "How to configure a Validation Result store on a filesystem": " title: How to configure a Validation Result store on a filesystem import Prerequisites from '../../../guides/connecting_to_your_data/components/prerequisites.jsx'; import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; By default, Validation Results are stored in the uncommitted/validations/ directory.  Validation Results can include sensitive or regulated data that should not be committed to a source control system. Use the information provided here to configure a new storage location for Validation Results in your filesystem. You'll learn how to use an Action to update Data Docs sites with new Validation Results from Checkpoint runs. Prerequisites   A Data Context. An Expectation Suite . A Checkpoint. A new storage location to store Validation Results. This can be a local path, or a path to a secure network filesystem.   1. Create a new folder for Validation Results Run the following command to create a new folder for your Validation Results and move your existing Validation Results to the new folder: ```bash in the great_expectations/ folder mkdir shared_validations mv uncommitted/validations/npi_validations/ uncommitted/shared_validations/ ` In this example, the name of the Validation Result isnpi_validationsand the path to the new storage location isshared_validations/``. 2. Identify your Data Context Validation Results Store The configuration for your Validation Results Store is available in your Data Context.  Open great_expectations.ymland find the following entry:  ```yaml validations_store_name: validations_store stores:    validations_store:        class_name: ValidationsStore        store_backend:            class_name: TupleFilesystemStoreBackend            base_directory: uncommitted/validations/ ``` This configuration tells Great Expectations to look for Validation Results in the validations_store Store. The default base_directory for validations_store is uncommitted/validations/. 3. Update your configuration file to include a new Store for Validation results In the following example, validations_store_name is set to shared_validations_filesystem_store, but it can be personalized.  Also, base_directory is set to uncommitted/shared_validations/, but you can set it to another path that is accessible by Great Expectations. ```yaml validations_store_name: shared_validations_filesystem_store stores:    shared_validations_filesystem_store:        class_name: ValidationsStore        store_backend:            class_name: TupleFilesystemStoreBackend            base_directory: uncommitted/shared_validations/ ``` 4. Confirm that the Validation Results Store has been correctly configured Run a Checkpoint to store results in the new Validation Results Store in your new location, and then visualize the results by re-building Data Docs.", "How to configure a Validation Result Store to PostgreSQL": " title: How to configure a Validation Result Store to PostgreSQL import Prerequisites from '../../connecting_to_your_data/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; By default, Validation Results are stored in JSON format in the uncommitted/validations/ subdirectory of your great_expectations/ folder. Validation Results can include sensitive or regulated data that should not be committed to a source control system.  Use the information provided here to configure Great Expectations to store Validation Results in a PostgreSQL database. Prerequisites   A Data Context. An Expectations Suite. A Checkpoint. A PostgreSQL database with appropriate credentials.   1. Configure the config_variables.yml file with your database credentials GX recommends storing database credentials in the config_variables.yml file, which is located in the uncommitted/ folder by default, and not part of source control.    To add database credentials, open config_variables.yml and add the following entry below the db_creds key:  yaml db_creds:   drivername: postgresql   host: '<your_host_name>'   port: '<your_port>'   username: '<your_username>'   password: '<your_password>'   database: '<your_database_name>' To configure the config_variables.yml file, or additional environment variables, see How to configure credentials.   Optional. To use a specific schema as the backend, specify schema as an additional keyword argument. For example: yaml db_creds:   drivername: postgresql   host: '<your_host_name>'   port: '<your_port>'   username: '<your_username>'   password: '<your_password>'   database: '<your_database_name>'   schema: '<your_schema_name>'   2. Identify your Data Context Validation Results Store The configuration for your Validation Results Store is available in your Data Context.  Open great_expectations.ymland find the following entry: ```yaml validations_store_name: validations_store stores:   validations_store:       class_name: ValidationsStore       store_backend:           class_name: TupleFilesystemStoreBackend           base_directory: uncommitted/validations/ ` This configuration tells Great Expectations to look for Validation Results in thevalidations_storeStore. The defaultbase_directoryforvalidations_storeisuncommitted/validations/``. 3. Update your configuration file to include a new Validation Results Store Add the following entry to your great_expectations.yml:  ```yaml validations_store_name: validations_postgres_store stores:   validations_postgres_store:       class_name: ValidationsStore       store_backend:           class_name: DatabaseStoreBackend           credentials: ${db_creds} ``` In the previous example, validations_store_name is set to validations_postgres_store, but it can be personalized.  Also, class_name is set to DatabaseStoreBackend, and credentials is set to ${db_creds}, which references the corresponding key in the config_variables.yml file.   4. Confirm the addition of the new Validation Results Store In the previous example, a validations_store on the local filesystem and a validations_postgres_store are configured.  Great Expectations looks for Validation Results in PostgreSQL when the validations_store_name variable is set to validations_postgres_store. Run the following command to remove validations_store and confirm the validations_postgres_store configuration: ```bash great_expectations store list   name: validations_store class_name: ValidationsStore store_backend:   class_name: TupleFilesystemStoreBackend   base_directory: uncommitted/validations/   name: validations_postgres_store class_name: ValidationsStore store_backend:   class_name: DatabaseStoreBackend   credentials:       database: ''       drivername: postgresql       host: ''       password: **       port: ''       username: '' ```   5. Confirm the Validation Results Store is configured correctly Run a Checkpoint to store results in the new Validation Results store in PostgreSQL, and then visualize the results by re-building Data Docs. Great Expectations creates a new table in your database named ge_validations_store, and populates the fields with information from the Validation Results.", "How to configure an Expectation Store to use Amazon S3": " title: How to configure an Expectation Store to use Amazon S3 import Preface from './components_how_to_configure_an_expectation_store_in_amazon_s3/_preface.mdx' import InstallBoto3 from './components/_install_boto3_with_pip.mdx' import VerifyAwsCredentials from './components/_verify_aws_credentials_are_configured_properly.mdx' import IdentifyYourDataContextExpectationsStore from './components_how_to_configure_an_expectation_store_in_amazon_s3/_identify_your_data_context_expectations_store.mdx' import UpdateYourConfigurationFileToIncludeANewStoreForExpectationsOnS from './components_how_to_configure_an_expectation_store_in_amazon_s3/_update_your_configuration_file_to_include_a_new_store_for_expectations_on_s.mdx' import CopyExistingExpectationJsonFilesToTheSBucketThisStepIsOptional from './components_how_to_configure_an_expectation_store_in_amazon_s3/_copy_existing_expectation_json_files_to_the_s_bucket_this_step_is_optional.mdx' import ConfirmList from './components_how_to_configure_an_expectation_store_in_amazon_s3/_confirm_list.mdx'  1. Install boto3 with pip  2. Verify your AWS credentials are properly configured  3. Identify your Data Context Expectations Store  4. Update your configuration file to include a new Store for Expectations  5. Copy existing Expectation JSON files to the S3 bucket (Optional)  6. Confirm Expectation Suite availability ", "How to configure an Expectation Store to use Azure Blob Storage": " title: How to configure an Expectation Store to use Azure Blob Storage import Prerequisites from '../../connecting_to_your_data/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; By default, new Profiled Expectations are stored as Expectation Suites in JSON format in the expectations/ subdirectory of your great_expectations/ folder.  Use the information provided here to configure a new storage location for Expectations in Azure Blob Storage. Prerequisites   A Data Context. An Expectations Suite. An Azure Storage account. An Azure Blob container. If you need to host and share Data Docs on Azure Blob Storage, then you can set this up first and then use the $web existing container to store your Expectations. A prefix (folder) where to store Expectations. You don't need to create the folder, the prefix is just part of the Azure Blob name.   1. Configure the config_variables.yml file with your Azure Storage credentials GX recommends that you store Azure Storage credentials in the config_variables.yml file, which is located in the uncommitted/ folder by default, and is not part of source control. The following code adds Azure Storage credentials below the AZURE_STORAGE_CONNECTION_STRING key: yaml AZURE_STORAGE_CONNECTION_STRING: \"DefaultEndpointsProtocol=https;EndpointSuffix=core.windows.net;AccountName=<YOUR-STORAGE-ACCOUNT-NAME>;AccountKey=<YOUR-STORAGE-ACCOUNT-KEY==>\" To learn more about the additional options for configuring the config_variables.yml file, or additional environment variables, see How to configure credentials 2. Identify your Data Context Expectations Store Your Expectations Store configuration is provided in your Data Context. Open great_expectations.yml and find the following entry: ```yaml expectations_store_name: expectations_store stores:   expectations_store:       class_name: ExpectationsStore       store_backend:           class_name: TupleFilesystemStoreBackend           base_directory: expectations/ ``` This configuration tells Great Expectations to look for Expectations in a Store named expectations_store. The default base_directory for expectations_store is expectations/. 3. Update your configuration file to include a new Store for Expectations In the following example, expectations_store_name is set to expectations_AZ_store, but it can be personalized.  You also need to change the store_backend settings.  The class_name is TupleAzureBlobStoreBackend, container is the name of your blob container where Expectations are stored, prefix is the folder in the container where Expectations are located, and connection_string is ${AZURE_STORAGE_CONNECTION_STRING} to reference the corresponding key in the config_variables.yml file. ```yaml expectations_store_name: expectations_AZ_store stores:   expectations_AZ_store:       class_name: ExpectationsStore       store_backend:         class_name: TupleAzureBlobStoreBackend         container:          prefix: expectations         connection_string: ${AZURE_STORAGE_CONNECTION_STRING} ``` :::note If the container for hosting and sharing Data Docs on Azure Blob Storage is named $web, use container: \\$web to allow access to the $webcontainer. ::: Additional authentication and configuration options are available. See Hosting and sharing Data Docs on Azure Blob Storage. 4. Copy existing Expectation JSON files to the Azure blob (Optional) You can use the az storage blob upload command to copy Expectations into Azure Blob Storage. The following command copies the Expectation exp1 from a local folder to Azure Blob Storage:  ```bash export AZURE_STORAGE_CONNECTION_STRING=\"DefaultEndpointsProtocol=https;EndpointSuffix=core.windows.net;AccountName=;AccountKey=\" az storage blob upload -f  -c  -n / example : az storage blob upload -f great_expectations/expectations/exp1.json -c  -n expectations/exp1.json Finished[#############################################################]  100.0000% { \"etag\": \"\\\"0x8D8E08E5DA47F84\\\"\", \"lastModified\": \"2021-03-06T10:55:33+00:00\" } ``` To learn more about other methods that are available to copy Expectation JSON files into Azure Blob Storage, see Introduction to Azure Blob Storage. 5. Confirm that the new Expectation Suites have been added If you copied your existing Expectation Suites to Azure Blob Storage, run the following Python command to confirm that Great Expectations can find them:  ```python import great_expectations as gx context = gx.get_context() context.list_expectation_suite_names() ``` A list of Expectations you copied to Azure Blob Storage is returned. Expectations that weren't copied to the new folder are not listed. 6. Confirm that Expectations can be accessed from Azure Blob Storage Run the following command to confirm your Expectations have been copied to Azure Blob Storage:  bash great_expectations suite list If your Expectations have not been copied to Azure Blob Storage, the message \"No Expectations were found\" is returned.", "How to configure an Expectation Store to use GCS": " title: How to configure an Expectation Store to use GCS import Prerequisites from '../../connecting_to_your_data/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; By default, newl Profiled Expectations are stored as Expectation Suites in JSON format in the expectations/ subdirectory of your great_expectations/ folder.  Use the information provided here to configure a new storage location for Expectations in Google Cloud Storage (GCS). To view all the code used in this topic, see how_to_configure_an_expectation_store_in_gcs.py. Prerequisites   A Data Context. An Expectations Suite. A GCP service account with credentials that allow access to GCP resources such as Storage Objects. A GCP project, GCS bucket, and prefix to store Expectations.   1. Configure your GCP credentials Confirm that your environment is configured with the appropriate authentication credentials needed to connect to the GCS bucket where Expectations will be stored. This includes the following:  A GCP service account. Setting the GOOGLE_APPLICATION_CREDENTIALS environment variable. Verifying authentication by running a Google Cloud Storage client library script.  For more information about validating your GCP authentication credentials, see Authenticate to Cloud services using client libraries. 2. Identify your Data Context Expectations Store The configuration for your Expectations Store is available in your Data Context. Open great_expectations.yml and find the following entry:  yaml name=\"tests/integration/docusaurus/setup/configuring_metadata_stores/how_to_configure_an_expectation_store_in_gcs.py expected_existing_expectations_store_yaml\" This configuration tells Great Expectations to look for Expectations in the expectations_store Store. The default base_directory for expectations_store is expectations/. 3. Update your configuration file to include a new store for Expectations In the following example, expectations_store_name is set to expectations_GCS_store, but it can be personalized.  You also need to change the store_backend settings. The class_name is TupleGCSStoreBackend, project is your GCP project, bucket is the address of your GCS bucket, and prefix is the folder on GCS where Expectations are stored. yaml name=\"tests/integration/docusaurus/setup/configuring_metadata_stores/how_to_configure_an_expectation_store_in_gcs.py configured_expectations_store_yaml\" :::warning If you are also storing Validations in GCS or DataDocs in GCS, make sure that the prefix values are disjoint and one is not a substring of the other. ::: 4. Copy existing Expectation JSON files to the GCS bucket (Optional) Use the gsutil cp command to copy Expectations into GCS. For example, the following command copies the Expectation `my_expectation_suite from a local folder into a GCS bucket: bash name=\"tests/integration/docusaurus/setup/configuring_metadata_stores/how_to_configure_an_expectation_store_in_gcs.py copy_expectation_command\" The following confirmation message is returned: bash name=\"tests/integration/docusaurus/setup/configuring_metadata_stores/how_to_configure_an_expectation_store_in_gcs.py copy_expectation_output\" Additional methods for copying Expectations into GCS are available. See Upload objects from a filesystem. 5. Confirm that the new Expectation Suites have been added If you copied your existing Expectation Suites to GCS, run the following Python command to confirm that Great Expectations can find them:  ```python import great_expectations as gx context = gx.get_context() context.list_expectation_suite_names() ``` A list of Expectation Suites you copied to GCS is returned. Expectation Suites that weren't copied to the new Store aren't listed. 6. Confirm that Expectations can be accessed from GCS Run the following command to confirm your Expectations were copied to GCS: bash name=\"tests/integration/docusaurus/setup/configuring_metadata_stores/how_to_configure_an_expectation_store_in_gcs.py list_expectation_suites_command\" If your Expectations were not copied to Azure Blob Storage, a message indicating no Expectations were found is returned.", "How to configure an Expectation Store to use a filesystem": " title: How to configure an Expectation Store to use a filesystem import Prerequisites from '../../connecting_to_your_data/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; By default, new Profiled Expectations are stored as Expectation Suites in JSON format in the expectations/ subdirectory of your great_expectations folder. Use the information provided here to configure a new storage location for Expectations on your filesystem. Prerequisites   A Data Context. An Expectation Suite. A storage location for Expectations. This can be a local path, or a path to a network filesystem.   1. Create a new folder for Expectations Run the following command to create a new folder for your Expectations and move your existing Expectations to the new folder: ```bash in the great_expectations/ folder mkdir shared_expectations mv expectations/npi_expectations.json shared_expectations/ ` In this example, the name of the Expectation isnpi_expectationsand the path to the new storage location is/shared_expectations``. 2. Identify your Data Context Expectations Store The configuration for your Expectations Store is available in your Data Context.  Open great_expectations.ymland find the following entry: ```yaml expectations_store_name: expectations_store stores:   expectations_store:       class_name: ExpectationsStore       store_backend:           class_name: TupleFilesystemStoreBackend           base_directory: expectations/ ` This configuration tells Great Expectations to look for Expectations in theexpectations_storeStore. The defaultbase_directoryforexpectations_storeisexpectations/``. 3. Update your configuration file to include a new Store for Expectations results In the following example, expectations_store_name is set to shared_expectations_filesystem_store, but it can be personalized.  Also, base_directory is set to shared_expectations/, but you can set it to another path that is accessible by Great Expectations. ```yaml expectations_store_name: shared_expectations_filesystem_store stores:   shared_expectations_filesystem_store:       class_name: ExpectationsStore       store_backend:           class_name: TupleFilesystemStoreBackend           base_directory: shared_expectations/ ``` 4. Confirm that the new Expectation Suites have been added If you copied your existing Expectation Suites to your filesystem, run the following Python command to confirm that Great Expectations can find them:  ```python import great_expectations as gx context = gx.get_context() context.list_expectation_suite_names() ``` A list of Expectation Suites you copied your filesystem is returned. Expectation Suites that weren't copied to the new Store aren't listed. Version control systems GX recommends that you store Expectations in a version control system such as Git. The JSON format of Expectations allows for informative diff-statements and modification tracking. In the following example, the `expect_table_column_count_to_equal value changes from 333 to 331, and then to 330: ```bash git log -p npi_expectations.json commit cbc127fb27095364c3c1fcbf6e7f078369b07455   changed expect_table_column_count_to_equal to 331 diff --git a/great_expectations/expectations/npi_expectations.json b/great_expectations/expectations/npi_expectations.json --- a/great_expectations/expectations/npi_expectations.json +++ b/great_expectations/expectations/npi_expectations.json @@ -17,7 +17,7 @@    {      \"expectation_type\": \"expect_table_column_count_to_equal\",      \"kwargs\": { -        \"value\": 333 +        \"value\": 331      } commit 05b3c8c1ed35d183bac1717d4877fe13bc574963 changed expect_table_column_count_to_equal to 333 diff --git a/great_expectations/expectations/npi_expectations.json b/great_expectations/expectations/npi_expectations.json --- a/great_expectations/expectations/npi_expectations.json +++ b/great_expectations/expectations/npi_expectations.json    {      \"expectation_type\": \"expect_table_column_count_to_equal\",      \"kwargs\": { -        \"value\": 330 +        \"value\": 333      } ```", "How to configure an Expectation Store to use PostgreSQL": " title: How to configure an Expectation Store to use PostgreSQL import Prerequisites from '../../connecting_to_your_data/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; By default, new Profiled Expectations are stored as Expectation Suites in JSON format in the expectations/ subdirectory of your great_expectations/ folder.  Use the information provided here to configure Great Expectations to store Expectations in a PostgreSQL database. Prerequisites   A Data Context. An Expectations Suite. A PostgreSQL database with appropriate credentials.   1. Configure the config_variables.yml file with your database credentials GX recommends storing database credentials in the config_variables.yml file, which is located in the uncommitted/ folder by default, and not part of source control.  To add database credentials, open config_variables.yml and add the following entry below the db_creds key:  yaml     db_creds:       drivername: postgresql       host: '<your_host_name>'       port: '<your_port>'       username: '<your_username>'       password: '<your_password>'       database: '<your_database_name>' To configure the config_variables.yml file, or additional environment variables, see How to configure credentials. 2. Identify your Data Context Expectations Store Open great_expectations.ymland find the following entry: ```yaml expectations_store_name: expectations_store stores:   expectations_store:       class_name: ExpectationsStore       store_backend:           class_name: TupleFilesystemStoreBackend           base_directory: expectations/ ``` This configuration tells Great Expectations to look for Expectations in the expectations_store Store. The default base_directory for expectations_store is expectations/. 3. Update your configuration file to include a new Store for Expectations In the following example, expectations_store_name is set to expectations_postgres_store, but it can be personalized. You also need to make some changes to the store_backend settings.  The class_name is DatabaseStoreBackend, and credentials is ${db_creds} to reference the corresponding key in the config_variables.yml file. ```yaml expectations_store_name: expectations_postgres_store stores:   expectations_postgres_store:       class_name: ExpectationsStore       store_backend:           class_name: DatabaseStoreBackend           credentials: ${db_creds} ```", "\"Install Great Expectations with source data system dependencies\"": " sidebar_label: \"Install GX with source data system dependencies\" title: \"Install Great Expectations with source data system dependencies\" id: install_gx description: Install Great Expectations locally, or in a hosted environment. toc_min_heading_level: 2 toc_max_heading_level: 2  import Preface from './components_local/_preface.mdx' import CheckPythonVersion from './components_local/_check_python_version.mdx' import ChooseInstallationMethod from './components_local/_choose_installation_method.mdx' import InstallGreatExpectations from './components_local/_install_great_expectations.mdx' import VerifyGeInstallSucceeded from './components_local/_verify_ge_install_succeeded.mdx' import NextSteps from '/docs/guides/setup/components/install_nextsteps.md' import TechnicalTag from '/docs/term_tags/_tag.mdx'; import Prerequisites from '/docs/components/_prerequisites.jsx' import PrereqInstalledAwsCli from '/docs/components/prerequisites/_aws_installed_the_aws_cli.mdx' import PrereqAwsConfiguredCredentials from '/docs/components/prerequisites/_aws_configured_your_credentials.mdx' import AwsVerifyInstallation from '/docs/components/setup/dependencies/_aws_verify_installation.md' import AwsVerifyCredentialsConfiguration from '/docs/components/setup/dependencies/_aws_verify_installation.md' import PythonCheckVersion from '/docs/components/setup/python_environment/_python_check_version.mdx' import PythonCreateVenv from '/docs/components/setup/python_environment/_python_create_venv.md' import TipPythonOrPython3Executable from '/docs/components/setup/python_environment/_tip_python_or_python3_executable.md' import S3InstallDependencies from '/docs/components/setup/dependencies/_s3_install_dependencies.md' import GxVerifyInstallation from '/docs/components/setup/_gx_verify_installation.md' import LinksAfterInstallingGx from '/docs/components/setup/next_steps/_links_after_installing_gx.md' import PrereqGcpServiceAccount from '/docs/components/prerequisites/_gcp_service_account.md' import GcpVerifyCredentials from '/docs/components/setup/dependencies/_gcp_verify_credentials_configuration.md' import GcpInstallDependencies from '/docs/components/setup/dependencies/_gcp_install_dependencies.md' import PrereqAbsConfiguredAnAbsAccount from '/docs/components/prerequisites/_abs_configured_an_azure_storage_account_and_kept_connection_string.md' import AbsInstallDependencies from '/docs/components/setup/dependencies/_abs_install_dependencies.md' import AbsConfigureCredentialsInDataContext from '/docs/components/setup/dependencies/_abs_configure_credentials_in_data_context.md' import AbsFurtherConfiguration from '/docs/components/setup/next_steps/_links_for_adding_azure_blob_storage_configurations_to_data_context.md' import InstallDependencies from '/docs/components/setup/dependencies/_sql_install_dependencies.mdx' import TabItem from '@theme/TabItem'; import Tabs from '@theme/Tabs'; You can install Great Expectations (GX) locally, or in hosted environments such as Databricks, Amazon EMR, or Google Cloud Composer. Installing GX locally lets you test features and functionality to determine if it's suitable for your use case.  :::info Windows Support Windows support for the open source Python version of GX is currently unavailable. If you\u2019re using GX in a Windows environment, you might experience errors or performance issues. :::   Local Install Great Expectations (GX) locally.  Check Python version  Choose installation method  Install GX  Confirm GX installation    Hosted Great Expectations can be deployed in environments such as Databricks, Amazon EMR, or Google Cloud Composer. These environments do not always have a file system that allows a Great Expectations installation. To install Great Expectations in a hosted environment, see one of the following guides:  How to Use Great Expectations in Databricks How to instantiate a Data Context on an EMR Spark cluster    Amazon S3 Create your GX Python environment, install Great Expectations locally, and then configure the necessary dependencies to access data stored on Amazon S3. Prerequisites   The ability to install Python modules with pip     Ensure your AWS CLI version is the most recent  Ensure your AWS credentials are correctly configured  Check your Python version   Create a Python virtual environment  Install GX with optional dependencies for S3  Verify the GX has been installed correctly  Next steps Now that you have installed GX with the necessary dependencies for working with S3, you are ready to initialize your Data Context.  The Data Context will contain your configurations for GX components, as well as provide you with access to GX's Python API.    Microsoft Azure Blob Storage Create your GX Python environment, install Great Expectations locally, and then configure the necessary dependencies to access data stored on Microsoft Azure Blob Storage. Prerequisites   The ability to install Python modules with pip    Check your Python version   Create a Python virtual environment  Install GX with optional dependencies for Azure Blob Storage  Verify that GX has been installed correctly  Configure the config_variables.yml file with your Azure Storage credentials  Next steps    GCS Create your GX Python environment, install Great Expectations locally, and then configure the necessary dependencies to access data stored on GCS. Prerequisites   The ability to install Python modules with pip    Ensure your GCP credentials are correctly configured  Check your Python version   Create a Python virtual environment  Install optional dependencies  Verify that GX has been installed correctly  Next steps Now that you have installed GX with the necessary dependencies for working with GCS, you are ready to initialize your Data Context.  The Data Context will contain your configurations for GX components, as well as provide you with access to GX's Python API.    SQL databases Create your GX Python environment, install Great Expectations locally, and then configure the necessary dependencies to access data stored on SQL databases. Prerequisites   The ability to install Python modules with pip   Check your Python version   Create a Python virtual environment  Install GX with optional dependencies for SQL databases  :::caution Additional dependencies for some SQL dialects The above pip instruction will install GX with basic SQL support through SqlAlchemy.  However, certain SQL dialects require additional dependencies.  Depending on the SQL database type you will be working with, you may wish to use one of the following installation commands, instead:  AWS Athena: pip install 'great_expectations[athena]' BigQuery: pip install 'great_expectations[bigquery]' MSSQL: pip install 'great_expectations[mssql]' PostgreSQL: pip install 'great_expectations[postgresql]' Redshift: pip install 'great_expectations[redshift]' Snowflake: pip install 'great_expectations[snowflake]' Trino: pip install 'great_expectations[trino]'  ::: Verify that GX has been installed correctly  Set up credentials Different SQL dialects have different requirements for connection strings and methods of configuring credentials.  By default, GX allows you to define credentials as environment variables or as values in your Data Context (once you have initialized one). There may also be third party utilities for setting up credentials of a given SQL database type.  For more information on setting up credentials for a given source database, please reference the official documentation for that SQL dialect as well as our guide on how to set up credentials. Next steps Now that you have installed GX with the necessary dependencies for working with SQL databases, you are ready to initialize your Data Context.  The Data Context will contain your configurations for GX components, as well as provide you with access to GX's Python API.   ", "Deploy a scheduled Checkpoint with cron": " title: Deploy a scheduled Checkpoint with cron import Prerequisites from '../../connecting_to_your_data/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; This guide will help you deploy a scheduled Checkpoint with cron. Prerequisites   A Great Expectations instance. A Checkpoint.   Verify Checkpoint suitability Run the following command to verify that your Checkpoint runs: python name=\"tests/integration/docusaurus/reference/glossary/checkpoints.py retrieve_and_run\" Get great_expectations full path To prepare for editing the cron file, you'll need the full path of the project's great_expectations directory.  You can get full path to the great_expectations executable by running: bash which great_expectations /full/path/to/your/environment/bin/great_expectations Open your cron schedule A text editor can be used to open the cron schedule. On most operating systems, crontab -e will open your cron file in an editor. Add your Checkpoint to the cron schedule To run the Checkpoint my_checkpoint every morning at 0300, add the following line in the text editor that opens: bash 0  3  *  *  *    /full/path/to/your/environment/bin/great_expectations checkpoint run ratings --directory /full/path/to/my_project/great_expectations/ :::note - The five fields at the start of your cron schedule correspond to the minute, hour, day of the month, month, and day of the week. - It is critical that you use full paths to both the great_expectations executable in your project's environment and the full path to the project's great_expectations/ directory. ::: Save your changes to the cron schedule Once you have added the line that runs your Checkpoint at the desired time, save the text file of the cron schedule and exit the text editor.", "Use Data Docs URLs in custom Validation Actions": " title: Use Data Docs URLs in custom Validation Actions import Prerequisites from '../../connecting_to_your_data/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; To create a custom Validation Action that includes a link to the Data Docs, you get the Data Docs URL for the Validation Results page from your Validation Results after you run a Checkpoint. This method returns the URLs for any type of Data Docs site setup including S3 or a local setup. The code used in this topic is available on GitHub here: actions.py Prerequisites   An Expectation Suite for Validation. Familiarity with Validation Actions   Instantiate First, within the _run method of your custom Validation Action, instantiate an empty dict to hold your sites: python name=\"great_expectations/checkpoint/actions.py empty dict\" Acquire Next, call get_docs_sites_urls to get the urls for all the suites processed by this Checkpoint: python name=\"great_expectations/checkpoint/actions.py get_docs_sites_urls\" Iterate The above step returns a list of dictionaries containing the relevant information. Now, we need to iterate through the entries to build the object we want: python name=\"great_expectations/checkpoint/actions.py iterate\" Utilize You can now include the urls contained within the data_docs_validation_results dictionary as links in your custom notifications, for example in an email, Slack, or OpsGenie notification, which will allow users to jump straight to the relevant Validation Results page.   Congratulations!\ud83c\udf89 You've just accessed Data Docs URLs for use in custom Validation Actions! \ud83c\udf89  ", "'Manage Checkpoints'": " sidebar_label: 'Manage Checkpoints' title: 'Manage Checkpoints' id: checkpoint_lp description: Add validation data, create and configure Checkpoints, and pass in-mameory DataFrames.  import LinkCardGrid from '/docs/components/LinkCardGrid'; import LinkCard from '/docs/components/LinkCard'; This is where you'll find information about managing your Checkpoints including adding validation data, creating and configuring Checkpoints, and passing in-memory DataFrames.       ", "Add validation data or Expectation suites to a Checkpoint": " title: Add validation data or Expectation suites to a Checkpoint import Prerequisites from '../../../guides/connecting_to_your_data/components/prerequisites.jsx'; import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; Add validation data or Expectation Suites to an existing Checkpoint to aggregate individual validations across Expectation Suites or Datasources into a single Checkpoint. You can also use this process to Validate multiple source files before and after their ingestion into your data lake. Prerequisites   A Data Context. An Expectations Suite. A Checkpoint.   Open your existing Checkpoint Open your Checkpoint in a text editor. Your Checkpoint should appear similar to the following example: yaml name: my_checkpoint config_version: 1 class_name: Checkpoint run_name_template: \"%Y-%m-foo-bar-template-$VAR\" validations:   - batch_request:       datasource_name: my_datasource       data_asset_name: users     expectation_suite_name: users.warning     action_list:         - name: store_validation_result           action:             class_name: StoreValidationResultAction         - name: store_evaluation_params           action:             class_name: StoreEvaluationParametersAction         - name: update_data_docs           action:             class_name: UpdateDataDocsAction     evaluation_parameters:       param1: \"$MY_PARAM\"       param2: 1 + \"$OLD_PARAM\"     runtime_configuration:       result_format:         result_format: BASIC         partial_unexpected_count: 20 Add an Expectation Suite to the Checkpoint To add a second Expectation Suite (in this example we add users.error) to your Checkpoint configuration, modify the file to add an additional batch_request key and corresponding information, including evaluation_parameters, action_list, runtime_configuration, and expectation_suite_name.  In fact, the simplest way to run a different Expectation Suite on the same Batch of data is to make a copy of the original batch_request entry and then edit the expectation_suite_name value to correspond to a different Expectation Suite.  The resulting configuration will look like this: yaml name: my_checkpoint config_version: 1 class_name: Checkpoint run_name_template: \"%Y-%m-foo-bar-template-$VAR\" validations:   - batch_request:       datasource_name: my_datasource       data_asset_name: users     expectation_suite_name: users.warning     action_list:         - name: store_validation_result           action:             class_name: StoreValidationResultAction         - name: store_evaluation_params           action:             class_name: StoreEvaluationParametersAction         - name: update_data_docs           action:             class_name: UpdateDataDocsAction     evaluation_parameters:       param1: \"$MY_PARAM\"       param2: 1 + \"$OLD_PARAM\"     runtime_configuration:       result_format:         result_format: BASIC         partial_unexpected_count: 20   - batch_request:       datasource_name: my_datasource       data_connector_name: my_data_connector       data_asset_name: users       data_connector_query:         index: -1     expectation_suite_name: users.error     action_list:         - name: store_validation_result           action:             class_name: StoreValidationResultAction         - name: store_evaluation_params           action:             class_name: StoreEvaluationParametersAction         - name: update_data_docs           action:             class_name: UpdateDataDocsAction     evaluation_parameters:       param1: \"$MY_PARAM\"       param2: 1 + \"$OLD_PARAM\"     runtime_configuration:       result_format:         result_format: BASIC         partial_unexpected_count: 20 Add validation data to the Checkpoint In the prvious example, the entry you added with your Expectation Suite was paired with the same Batch of data as the original Expectation Suite.  However, you may also specify different Batch Requests (and thus different Batches of data) when you add an Expectation Suite.  The flexibility of easily adding multiple Validations of Batches of data with different Expectation Suites and specific Actions can be demonstrated using the following example of a Checkpoint configuration file: yaml name: my_fancy_checkpoint config_version: 1 class_name: Checkpoint run_name_template: \"%Y-%m-foo-bar-template-$VAR\" expectation_suite_name: users.delivery action_list:     - name: store_validation_result       action:         class_name: StoreValidationResultAction     - name: store_evaluation_params       action:         class_name: StoreEvaluationParametersAction     - name: update_data_docs       action:         class_name: UpdateDataDocsAction validations:   - batch_request:       datasource_name: my_datasource       data_asset_name: users     expectation_suite_name: users.warning   - batch_request:       datasource_name: my_datasource       data_asset_name: users     expectation_suite_name: users.error   - batch_request:       datasource_name: my_datasource       data_asset_name: users       options:         name: Titanic     action_list:       - name: quarantine_failed_data         action:           class_name: CreateQuarantineData       - name: advance_passed_data         action:           class_name: CreateQuarantineData evaluation_parameters:   param1: \"$MY_PARAM\"   param2: 1 + \"$OLD_PARAM\" runtime_configuration:   result_format:     result_format: BASIC     partial_unexpected_count: 20 According to this configuration, the locally-specified Expectation Suite users.warning is run against the batch_request that employs my_data_connector with the results processed by the Actions specified in the top-level action_list. Similarly, the locally-specified Expectation Suite users.error is run against the batch_request that employs my_special_data_connector with the results also processed by the actions specified in the top-level action_list. In addition, the top-level Expectation Suite users.delivery is run against the batch_request that employs my_other_data_connector with the results processed by the union of actions in the locally-specified action_list and in the top-level action_list. See How to configure a new Checkpoint using test_yaml_config for additional Checkpoint configuration examples including the convenient templating mechanism.", "Configure a new Checkpoint using test_yaml_config": " title: Configure a new Checkpoint using test_yaml_config import Prerequsities from '../../connecting_to_your_data/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; Use the information provided here to learn how to configure a Checkpoint using test_yaml_config. To create a new Checkpoint,  see How to create a new Checkpoint. test_yaml_config lets you configure and test the components of a Great Expectations deployment, including Datasources, Stores, and Checkpoints. test_yaml_config is intended for use within a Jupyter notebook, where you can use an edit-run-check loop to quickly test your changes before implementation. test_yaml_config supports iterative testing to help refine your Checkpoint configuration. As an iterative workflow, it is particularly well suited to notebooks. Prerequisites   Set up a working deployment of Great Expectations Connected to Data Created an Expectation Suite   Setup Run the following command in the first cell to load the necessary modules and initialize your Data Context: python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_configure_a_new_checkpoint_using_test_yaml_config.py setup\" List Assets Your Checkpoint configuration includes Data Assets and Expectation Suites. Run the following command to list the available asset names: python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_configure_a_new_checkpoint_using_test_yaml_config.py asset_names\" Run the following command to list the Expectation Suites: python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_configure_a_new_checkpoint_using_test_yaml_config.py suite_names\" Create your Checkpoint Run the following YAML (inline as a Python string) to define SimpleCheckpoint as the starting point: python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_configure_a_new_checkpoint_using_test_yaml_config.py create_checkpoint\" Test your Checkpoint configuration Run the following command to test your YAML configuration and ensure it's correct: python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_configure_a_new_checkpoint_using_test_yaml_config.py test_checkpoint\" Modifying the previous configuration and testing with the previous cell helps ensure your configuration changes are correct. Save your Checkpoint Run the following command to save your Checkpoint and add it to the Data Context: python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_configure_a_new_checkpoint_using_test_yaml_config.py save_checkpoint\" Checkpoint configuration examples If you require more fine-grained configuration options, you can use the Checkpoint base class instead of SimpleCheckpoint. In this example, the Checkpoint configuration uses the nesting of batch_request sections inside the validations block so as to use the defaults defined at the top level. python  config = \"\"\"  name: my_fancy_checkpoint  config_version: 1  class_name: Checkpoint  run_name_template: \"%Y-%M-foo-bar-template-$VAR\"  validations:    - batch_request:        datasource_name: my_datasource        data_asset_name: users    - batch_request:        datasource_name: my_datasource        data_asset_name: users  expectation_suite_name: users.delivery  action_list:      - name: store_validation_result        action:          class_name: StoreValidationResultAction      - name: store_evaluation_params        action:          class_name: StoreEvaluationParametersAction      - name: update_data_docs        action:          class_name: UpdateDataDocsAction  evaluation_parameters:    param1: \"$MY_PARAM\"    param2: 1 + \"$OLD_PARAM\"  runtime_configuration:    result_format:      result_format: BASIC      partial_unexpected_count: 20  \"\"\" The following Checkpoint configuration runs the top-level action_list against the top-level batch_request as well as the locally-specified action_list against the top-level batch_request. python  config = \"\"\"  name: airflow_users_node_3  config_version: 1  class_name: Checkpoint  batch_request:      datasource_name: my_datasource      data_asset_name: users  validations:    - expectation_suite_name: users.warning  # runs the top-level action list against the top-level batch_request    - expectation_suite_name: users.error  # runs the locally-specified action_list union with the top-level action-list against the top-level batch_request      action_list:      - name: quarantine_failed_data        action:            class_name: CreateQuarantineData      - name: advance_passed_data        action:            class_name: CreatePassedData  action_list:      - name: store_validation_result        action:          class_name: StoreValidationResultAction      - name: store_evaluation_params        action:          class_name: StoreEvaluationParametersAction      - name: update_data_docs        action:          class_name: UpdateDataDocsAction  evaluation_parameters:      environment: $GE_ENVIRONMENT      tolerance: 0.01  runtime_configuration:      result_format:        result_format: BASIC        partial_unexpected_count: 20  \"\"\" The Checkpoint mechanism also offers the convenience of templates.  The first Checkpoint configuration is that of a valid Checkpoint in the sense that it can be run as long as all the parameters not present in the configuration are specified in the run_checkpoint API call. python  config = \"\"\"  name: my_base_checkpoint  config_version: 1  class_name: Checkpoint  run_name_template: \"%Y-%M-foo-bar-template-$VAR\"  action_list:  - name: store_validation_result    action:      class_name: StoreValidationResultAction  - name: store_evaluation_params    action:      class_name: StoreEvaluationParametersAction  - name: update_data_docs    action:      class_name: UpdateDataDocsAction  evaluation_parameters:    param1: \"$MY_PARAM\"    param2: 1 + \"$OLD_PARAM\"  runtime_configuration:      result_format:        result_format: BASIC        partial_unexpected_count: 20  \"\"\" The above Checkpoint can be run using the code below, providing missing parameters from the configured Checkpoint at runtime. ```python  checkpoint_run_result: CheckpointResult checkpoint_run_result = data_context.run_checkpoint(      checkpoint_name=\"my_base_checkpoint\",      validations=[          {              \"batch_request\": {                  \"datasource_name\": \"my_datasource\",                  \"data_asset_name\": \"users\",              },              \"expectation_suite_name\": \"users.delivery\",          },          {              \"batch_request\": {                  \"datasource_name\": \"my_datasource\",                  \"data_asset_name\": \"users\",              },              \"expectation_suite_name\": \"users.delivery\",          },      ],  )  ``` However, the run_checkpoint method can be simplified by configuring a separate Checkpoint that uses the above Checkpoint as a template and includes the settings previously specified in the run_checkpoint method: python  config = \"\"\"  name: my_fancy_checkpoint  config_version: 1  class_name: Checkpoint  template_name: my_base_checkpoint  validations:  - batch_request:      datasource_name: my_datasource      data_asset_name: users  - batch_request:      datasource_name: my_datasource      data_asset_name: users  expectation_suite_name: users.delivery  \"\"\" Now the run_checkpoint method is as simple as in the previous examples: python  checkpoint_run_result = context.run_checkpoint(      checkpoint_name=\"my_fancy_checkpoint\",  ) The checkpoint_run_result in both cases (the parameterized run_checkpoint method and the configuration that incorporates another configuration as a template) are the same. The final example presents a Checkpoint configuration that is suitable for the use in a pipeline managed by Airflow. python  config = \"\"\"  name: airflow_checkpoint  config_version: 1  class_name: Checkpoint  validations:  - batch_request:      datasource_name: my_datasource      data_asset_name: IN_MEMORY_DATA_ASSET  expectation_suite_name: users.delivery  action_list:      - name: store_validation_result        action:          class_name: StoreValidationResultAction      - name: store_evaluation_params        action:          class_name: StoreEvaluationParametersAction      - name: update_data_docs        action:          class_name: UpdateDataDocsAction  \"\"\" To run this Checkpoint, the batch_request with the batch_data nested under the runtime_parameters attribute needs to be specified explicitly as part of the run_checkpoint() API call, because the data to be Validated is accessible only dynamically during the execution of the pipeline. python checkpoint_run_result: CheckpointResult = data_context.run_checkpoint(     checkpoint_name=\"airflow_checkpoint\",     batch_request={         \"runtime_parameters\": {             \"batch_data\": my_data_frame,         },         \"data_connector_query\": {             \"batch_filter_parameters\": {                 \"airflow_run_id\": airflow_run_id,             }         },     },     run_name=airflow_run_id, )", "Create a new Checkpoint": " title: Create a new Checkpoint import TechnicalTag from '@site/docs/term_tags/tag.mdx'; import Preface from './components_how_to_create_a_new_checkpoint/_preface.mdx' import StepsForCheckpoints from './components_how_to_create_a_new_checkpoint/_steps_for_checkpoints.mdx' import AdditionalResources from './components_how_to_create_a_new_checkpoint/_additional_resources.mdx'   Create a Checkpoint To modify the following code for your use case, replace batch_request and expectation_suite_name with your own paremeters. python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_create_a_new_checkpoint.py create checkpoint batch_request\" There are other configuration options for more advanced deployments. See How to configure a new Checkpoint using test_yaml_config. Run your Checkpoint (Optional) python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_create_a_new_checkpoint.py run checkpoint batch_request\" The returned checkpoint_result contains information about the checkpoint run. Build Data Docs (Optional) Run the following Python code to build Data Docs with the latest checkpoint run results: python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_create_a_new_checkpoint.py build data docs\" Retrieve your Checkpoint (Optional) Run the following Python code to retrieve the Checkpoint: python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_create_a_new_checkpoint.py get checkpoint\" Related documentation ", "Validate data with Expectations and Checkpoints": " title: Validate data with Expectations and Checkpoints import Prerequisites from '../../../guides/connecting_to_your_data/components/prerequisites.jsx'; import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; This guide will help you pass an in-memory DataFrame to a Checkpoint that is defined at runtime. This is especially useful if you already have your data in memory due to an existing process such as a pipeline runner. The full script used in the following code examples, is available in GitHub here: how_to_pass_an_in_memory_dataframe_to_a_checkpoint.py.   Configured a Data Context.   Set up Great Expectations Run the following command to import the required libraries and load your DataContext python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_pass_an_in_memory_dataframe_to_a_checkpoint.py setup\" Read a DataFrame and create a Checkpoint The following example uses the read_* method on the PandasDatasource to directly return a Validator. To use Validators to interactively build an Expectation Suite, see How to create Expectations interactively in Python. The Validator can be passed directly to a Checkpoint python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_pass_an_in_memory_dataframe_to_a_checkpoint.py read_dataframe\" Alternatively, you can use add_* methods to add the asset and then retrieve a Batch Request. This method is consistent with how other Data Assets work, and can integrate in-memory data with other Batch Request workflows and configurations. python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_pass_an_in_memory_dataframe_to_a_checkpoint.py add_dataframe\" In both examples, batch_metadata is an optional parameter that can associate meta-data with the batch or DataFrame. When you work with DataFrames, this can help you distinguish Validation results.", "Validate multiple Batches from a Batch Request with a single Checkpoint": " title: Validate multiple Batches from a Batch Request with a single Checkpoint import Prerequisites from '/docs/components/_prerequisites.jsx'; By default, a Checkpoint only validates the last Batch included in a Batch Request. Use the information provided here to learn how you can use a Python loop and the Checkpoint validations parameter to validate multiple Batches identified by a single Batch Request.  Prerequisites   A configured Data Context. A Data Asset with multiple Batches. An Expectation Suite.    Create a Batch Request with multiple Batches The following Python code creates a Batch Request that includes every available Batch in a Data Asset named asset: python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_validate_multiple_batches_within_single_checkpoint.py build_a_batch_request_with_multiple_batches\" :::tip A Batch Request can only retrieve multiple Batches from a Data Asset that has been configured to include more than the default single Batch. When working with a Filesystem Data Source and organizing Batches, the batching_regex argument determines the inclusion of multiple Batches into a single Data Asset, with each file that matches the batching_regex resulting in a single Batch. SQL Data source data Assets include a single Batch by default. You can use splitters to split the single Batch into multiple Batches. For more information on partitioning a Data Asset into Batches, see Manage Data Assets. ::: Get a list of Batches from the Batch Request Use the same Data Asset that your Batch Request was built from to retrieve a list of Batches with the following code: python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_validate_multiple_batches_within_single_checkpoint.py batch_list\" Convert the list of Batches into a list of Batch Requests A Checkpoint validates Batch Requests, but only validates the last Batch found in a Batch Request. You'll need to convert the list of Batches into a list of Batch Requests that return the corresponding individual Batch. python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_validate_multiple_batches_within_single_checkpoint.py batch_request_list\" Build a validations list A Checkpoint class's validations parameter consists of a list of dictionaries.  Each dictionary pairs one Batch Request with the Expectation Suite it should be validated against.  The following code creates a valid validations list and associates each Batch Request with an Expectation Suite named example_suite. python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_validate_multiple_batches_within_single_checkpoint.py add_validations\" Run Checkpoint The validations list, containing the pairings of Batch Requests and Expectation Suites, can now be passed to a single Checkpoint instance which validates each Batch Request against its corresponding Expectation Suite. This effectively validates each Batch included in the original multiple-Batch Batch Request. python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_validate_multiple_batches_within_single_checkpoint.py add_checkpoint\" Review the Validation Results After the validations run, use the following code to build and view the Validation Results as Data Docs. python name=\"tests/integration/docusaurus/validation/checkpoints/how_to_validate_multiple_batches_within_single_checkpoint.py review data_docs\"", "'Configure Actions'": " sidebar_label: 'Configure Actions' title: 'Configure Actions' id: actions_lp description: Configure Actions to send Validation Result notifications, update Data Docs, and store Validation Results.  import LinkCardGrid from '/docs/components/LinkCardGrid'; import LinkCard from '/docs/components/LinkCard'; This is where you'll find information about using Actions to send Validation Result notifications, update Data Docs, and store Validation Results.        ", "Collect OpenLineage metadata using an Action": " title: Collect OpenLineage metadata using an Action import Prerequisites from '../../../guides/connecting_to_your_data/components/prerequisites.jsx'; import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; OpenLineage is an open framework for collection and analysis of data lineage. It tracks the movement of data over time, tracing relationships between datasets. Data engineers can use data lineage metadata to determine the root cause of failures, identify performance bottlenecks, and simulate the effects of planned changes. Enhancing the metadata in OpenLineage with results from an Expectation Suite makes it possible to answer questions like: * have there been failed assertions in any upstream datasets? * what jobs are currently consuming data that is known to be of poor quality? * is there something in common among failed assertions that seem otherwise unrelated? This guide will explain how to use an Action to emit results to an OpenLineage backend, where their effect on related datasets can be studied. Prerequisites   Created at least one Expectation Suite Created at least one Checkpoint - you will need it in order to test that the OpenLineage Validation is working.   Ensure that the openlineage-integration-common package has been installed in your Python environment. bash  % pip3 install openlineage-integration-common Update the action_list key in your Validation Operator config. Add the OpenLineageValidationAction action to the action_list key your Checkpoint configuration. yaml action_list:  - name: openlineage    action:      class_name: OpenLineageValidationAction      module_name: openlineage.common.provider.great_expectations      openlineage_host: ${OPENLINEAGE_URL}      openlineage_apiKey: ${OPENLINEAGE_API_KEY}      job_name: ge_validation # This is user-definable      openlineage_namespace: ge_namespace # This is user-definable The openlineage_host and openlineage_apiKey values can be set via the environment, as shown above, or can be implemented as variables in uncommitted/config_variables.yml. The openlineage_apiKey value is optional, and is not required by all OpenLineage backends. A Great Expecations Checkpoint is recorded as a Job in OpenLineage, and will be named according to the job_name value. Similarly, the openlineage_namespace value can be optionally set. For more information on job naming, consult the Naming section of the OpenLineage spec. Test your Action by Validating a Batch of data. Run the following command to retrieve and run a Checkpoint to Validate a Batch of data and then emit lineage events to the OpenLineage backend: python name=\"tests/integration/docusaurus/reference/glossary/checkpoints.py retrieve_and_run\" :::note Reminder Our guide on how to Validate data by running a Checkpoint has more detailed instructions for this step, including instructions on how to run a checkpoint from a Python script instead of from the CLI. ::: Related documentation  Checkpoints overview page Actions overview page The OpenLineage Spec Blog: Expecting Great Quality with OpenLineage Facets ", "Trigger Email as an Action": " title: Trigger Email as an Action import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; This guide will help you trigger emails as an Action . It will allow you to send an email including information about a Validation Result, including whether or not the Validation succeeded. Prerequisites  An email account configured on the SMTP server you're going to use to send email Identified the email addresses for sent messages A Checkpoint configured to send email  Edit your configuration variables with email related information Open uncommitted/config_variables.yml file and add the following variables by adding the following lines: yaml smtp_address: [address of the smtp server] smtp_port: [port used by the smtp server] sender_login: [login used to send the email] sender_password: [password used to send the email] sender_alias: [optional alias used to send the email (default = sender_login)] receiver_emails: [addresses you want to send the email to]  # each address must be separated by commas Update action_list in your Checkpoint configuration Locate the relevant checkpoint that you want to set up an email trigger for. Open the checkpoint's configuration file that is located at great_expectations/checkpoints/ (for example, great_expectations/checkpoints/checkpoint_01.yml) and add an email action to the action_list. Follow the snippet below to set up the detials of your email trigger. yaml action_list:   - name: send_email_on_validation_result # name can be set to any value     action:       class_name: EmailAction       notify_on: all # possible values: \"all\", \"failure\", \"success\"       notify_with: # optional list containing the DataDocs sites to include in the notification. Defaults to including links to all configured sites.       # You can choose between using SSL encryption, TLS encryption or none of them (not advised)       use_tls: False       use_ssl: True       renderer:         module_name: great_expectations.render.renderer.email_renderer         class_name: EmailRenderer       # put the actual following information in the uncommitted/config_variables.yml file       # or pass in as environment variable       smtp_address: ${smtp_address}       smtp_port: ${smtp_port}       sender_login: ${sender_login}       sender_password: ${sender_password}       sender_alias: ${sender_alias}       receiver_emails: ${receiver_emails}  # string containing email addresses separated by commas Test your updated Action list Run your Checkpoint to Validate a Batch of data and receive an email on the success or failure of the Validation. :::note Reminder Our guide on how to Validate data by running a Checkpoint has instructions for this step. ::: If successful, you should receive an email that looks like this:  Additional notes If your great_expectations.yml contains multiple configurations for Data Docs sites, all of them will be included in the email by default. If you would like to be more specific, you can configure the notify_with variable in your Checkpoint configuration. The following example will configure the email to include links Data Docs at local_site and s3_site. ```yaml Example data_docs_sites configuration in great_expectations.yml data_docs_sites:   local_site:     class_name: SiteBuilder     show_how_to_buttons: true     store_backend:       class_name: TupleFilesystemStoreBackend       base_directory: uncommitted/data_docs/local_site/     site_index_builder:       class_name: DefaultSiteIndexBuilder   s3_site:  # this is a user-selected name - you may select your own     class_name: SiteBuilder     store_backend:       class_name: TupleS3StoreBackend       bucket: data-docs.my_org  # UPDATE the bucket name here to match the bucket you configured above.     site_index_builder:       class_name: DefaultSiteIndexBuilder       show_cta_footer: true ``` ```yaml Example Checkpoint configuration action_list:   - name: send_email_on_validation_result # name can be set to any value     action:       class_name: EmailAction       notify_on: all # possible values: \"all\", \"failure\", \"success\"       #--------------------------------       # This is what was configured       #--------------------------------       notify_with:         - local_site         - gcs_site       use_ssl: True       use_tls: False       renderer:         module_name: great_expectations.render.renderer.email_renderer         class_name: EmailRenderer       # put the actual following information in the uncommitted/config_variables.yml file       # or pass in as environment variable       smtp_address: ${smtp_address}       smtp_port: ${smtp_port}       sender_login: ${sender_login}       sender_password: ${sender_password}       sender_alias: ${sender_alias}       receiver_emails: ${receiver_emails} # string containing email addresses separated by commas ``` Additional resources The EmailAction uses smtplib. For more information about this module, see smtplib \u2014 SMTP protocol client.", "Trigger Opsgenie notifications as an Action": " title: Trigger Opsgenie notifications as an Action import Prerequisites from '../../connecting_to_your_data/components/prerequisites.jsx' import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; This guide will help you set up Opsgenie alert notifications when running Great Expectations. This is useful as it can provide alerting when Great Expectations is run, or certain Expectations begin failing (or passing!). Prerequisites   Set up a working deployment of Great Expectations An Opsgenie account Created a Checkpoint that will be configured with the notification Action.   Set up a new API integration within Opsgenie  Navigate to Settings > Integration list within Opsgenie using the sidebar menu.    Select add on the 'API' integration, this will generally be the first available option. Name the integration something meaningful such as 'Great Expectations' Assign the alerts to any relevant team. Click the copy icon next to the API Key - you'll need this for the next step. Add any required responders. Ensure 'Create and Update Access' is checked along with the 'Enabled' checkbox. Click 'Save Integration' to save the newly created integration.  Update your Great Expectations configuration variables Using the API Key you copied from Step 1, update your Great Expectations configuration variables in your config_variables.yml file yaml opsgenie_api_key: YOUR-API-KEY Add send_opsgenie_alert_on_validation_result operator to your Checkpoint configuration Next, update your Checkpoint configuration file to add a new action to the Actions list in great_expectations.yml yaml action_list:  - name: send_opsgenie_alert_on_validation_result    action:      class_name: OpsgenieAlertAction      notify_on: all      api_key: ${opsgenie_api_key}      priority: P3      renderer:        module_name: great_expectations.render.renderer.opsgenie_renderer        class_name: OpsgenieRenderer      tags:       - Production       - Non-Critical  Set notify_on to one of, \"all\", \"failure\", or \"success\" Optionally set a priority (from P1 - P5, defaults to P3) Set region: eu if you are using the European Opsgenie endpoint Optionally include 'tags' in your settings which will be included in your API call to OpsGenie. e.g.: 'Production'  Validate a Batch of data to test your alerts Run your Checkpoint to Validate a Batch of data and receive an Opsgenie alert on the success or failure of the Validation. :::note Reminder Our guide on how to Validate data by running a Checkpoint has instructions for this step. :::", "Trigger Slack notifications as an Action": " title: Trigger Slack notifications as an Action import Prerequisites from '../../../guides/connecting_to_your_data/components/prerequisites.jsx'; import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; This guide will help you trigger Slack notifications as an Action. It will allow you to send a Slack message including information about a Validation Result, including whether or not the Validation succeeded. Great Expectations is able to use a Slack webhook or Slack app to send notifications. Prerequisites     A Slack app with the Webhook function enabled. See Sending messages using Incoming Webhooks. A Webhook address for your Slack app. A Slack channel to send messages to. A Checkpoint configured to send the notification.       A Slack app with a Bot Token. See Sending messages using Incoming Webhooks. A Bot Token for your Slack app. A Slack channel to send messages to. A Checkpoint configured to send the notification.     Edit your configuration variables to include the Slack webhook Open uncommitted/config_variables.yml file and add validation_notification_slack_webhook variable by adding the following line: yaml validation_notification_slack_webhook: [address to web hook] Include the send_slack_notification_on_validation_result Action in your Checkpoint configuration Open the .yml configuration file in great_expectations/checkpoints that corresponds to the Checkpoint you want to add Slack notifications to.   Add the send_slack_notification_on_validation_result Action to the action_list section of the configuration. Make sure the following section exists in the Checkpoint configuration.   Webhook config yaml action_list:     #--------------------------------     # here is what you will be adding     #--------------------------------     - name: send_slack_notification_on_validation_result # name can be set to any value       action:         class_name: SlackNotificationAction         # put the actual webhook URL in the uncommitted/config_variables.yml file         slack_webhook: ${validation_notification_slack_webhook}         notify_on: all # possible values: \"all\", \"failure\", \"success\"         notify_with: # optional list containing the DataDocs sites to include in the notification. Defaults to including links to all configured sites.         renderer:           module_name: great_expectations.render.renderer.slack_renderer           class_name: SlackRenderer   Slack bot config yaml action_list:     #--------------------------------     # here is what you will be adding     #--------------------------------     - name: send_slack_notification_on_validation_result # name can be set to any value       action:         class_name: SlackNotificationAction         # put the actual bot token in the uncommitted/config_variables.yml file         slack_token: {bot_token}         slack_channel: <channel-name>         notify_on: all # possible values: \"all\", \"failure\", \"success\"         notify_with: # optional list containing the DataDocs sites to include in the notification. Defaults to including links to all configured sites.         renderer:           module_name: great_expectations.render.renderer.slack_renderer           class_name: SlackRenderer   Test your Slack notifications Run your Checkpoint to Validate a Batch of data and receive Slack notification on the success or failure of the Expectation Suite's Validation.   :::note Reminder Our guide on how to Validate data by running a Checkpoint has instructions for this step. ::: If successful, you should receive a Slack message that looks like this:  Additional notes  If your great_expectations.yml contains multiple configurations for Data Docs sites, all of them will be included in the Slack notification by default. If you would like to be more specific, you can configure the notify_with variable in your Checkpoint configuration. The following example will configure the Slack message to include links Data Docs at local_site and s3_site.  yaml     # Example data_docs_sites configuration     data_docs_sites:       local_site:         class_name: SiteBuilder         show_how_to_buttons: true         store_backend:           class_name: TupleFilesystemStoreBackend           base_directory: uncommitted/data_docs/local_site/         site_index_builder:           class_name: DefaultSiteIndexBuilder       s3_site:  # this is a user-selected name - you may select your own         class_name: SiteBuilder         store_backend:           class_name: TupleS3StoreBackend           bucket: data-docs.my_org  # UPDATE the bucket name here to match the bucket you configured above.         site_index_builder:           class_name: DefaultSiteIndexBuilder           show_cta_footer: true yaml     # Example action_list in Checkpoint configuration        action_list:         - name: send_slack_notification_on_validation_result # name can be set to any value               action:                 class_name: SlackNotificationAction                 # put the actual webhook URL in the uncommitted/config_variables.yml file                 slack_webhook: ${validation_notification_slack_webhook}                 notify_on: all # possible values: \"all\", \"failure\", \"success\"                 #--------------------------------                 # This is what was configured                 #--------------------------------                 notify_with:                   - local_site                   - s3_site                 renderer:                   module_name: great_expectations.render.renderer.slack_renderer                   class_name: SlackRenderer Related documentation  Sending messages using Incoming Webhooks. ", "Update Data Docs after Validating a Checkpoint": " title: Update Data Docs after Validating a Checkpoint import Prerequisites from '../../../guides/connecting_to_your_data/components/prerequisites.jsx'; import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; This guide will explain how to use an Action to update Data Docs sites with new Validation Results from running a Checkpoint. Prerequisites   A minimum of one Expectation Suite A minimum of one Checkpoint.   Update your Checkpoint A Checkpoint's action_list contains a list of Actions.  After the Checkpoint is Validated, these Actions are called in order.  Add an Action to the end of the action_list and name it update_data_docs. Actions are required to have a single field, action.  Inside the action field, a class_name field must be defined, which determines which class will be instantiated to execute this Action.  Add class_name: UpdateDataDocsAction to the Action. :::note Note: The StoreValidationResultAction Action must appear before  UpdateDataDocsAction Action, since Data Docs are rendered from Validation Results from the Store. ::: yaml  action_list:    - name: store_validation_result      action:        class_name: StoreValidationResultAction    - name: store_evaluation_params      action:        class_name: StoreEvaluationParametersAction    - name: update_data_docs      action:        class_name: UpdateDataDocsAction Specify Data Docs sites (Optional)  By default, the UpdateDataDocsAction updates all Data Docs sites found within your project.    To specify which Data Docs sites to update, provide a site_names key to the action config inside your UpdateDataDocsAction.   This field accepts a list of Data Docs site names, and when provided, will only update the specified sites.  yaml  action_list:    - name: store_validation_result      action:        class_name: StoreValidationResultAction    - name: store_evaluation_params      action:        class_name: StoreEvaluationParametersAction    - name: update_data_docs      action:        class_name: UpdateDataDocsAction        site_names:          - team_site Test your configuration Test that your new Action is configured correctly: Run the following command to run the Checkpoint and verify that no errors are returned: python import great_expectations as gx context = gx.get_context() checkpoint_name = \"your checkpoint name here\" context.run_checkpoint(checkpoint_name=checkpoint_name) Finally, check your Data Docs sites to confirm that a new Validation Result has been added. Additional notes The UpdateDataDocsAction generates an HTML file for the latest Validation Result and updates the index page to link to the new file, and re-renders pages for the Expectation Suite used for that Validation. It does not perform a full rebuild of Data Docs sites. This means that if you wish to render older Validation Results, you should run full Data Docs rebuild (via CLI's great_expectations docs build command or by calling context.build_data_docs()). Related documentation  Checkpoints overview page Actions overview page ", "Quickstart with GX Cloud": " title: Quickstart with GX Cloud tag: [tutorial, getting started, quickstart, cloud]  Quickstart with Great Expectations Cloud import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; import SetupAndInstallForSqlData from '/docs/components/setup/link_lists/_setup_and_install_for_sql_data.md' import SetupAndInstallForFilesystemData from '/docs/components/setup/link_lists/_setup_and_install_for_filesystem_data.md' import SetupAndInstallForHostedData from '/docs/components/setup/link_lists/_setup_and_install_for_hosted_data.md' import SetupAndInstallForCloudData from '/docs/components/setup/link_lists/_setup_and_install_for_cloud_data.md' import Prerequisites from '/docs/components/_prerequisites.jsx' import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; Few things are as daunting as taking your first steps with a new piece of software. This guide will introduce you to GX Cloud and demonstrate the ease with which you can implement the basic GX workflow. We will walk you through the entire process of connecting to your data, building your first Expectation based off of an initial Batch of that data, validating your data with that Expectation, and finally reviewing the results of your validation. Once you have completed this guide you will have a foundation in the basics of using GX Cloud. In the future you will be able to adapt GX to suit your specific needs by customizing the execution of the individual steps you will learn here. Prerequisites   Installed Great Expectations OSS on your machine. Followed invitation email instructions from the GX team after signing up for Early Access. Successfully logged in to GX Cloud at https://app.greatexpectations.io.   Steps 1. Setup 1.1 Generate access token Go to \u201cSettings\u201d > \u201cTokens\u201d in the navigation panel and generate an access token. Both admin and editor roles will suffice for this guide. These tokens are view-once and stored as a hash in Great Expectation Cloud's backend database. Once you copy the API key and close the dialog, the Cloud UI will never show the token value again. 1.2 Import modules :::tip Any Python Interpreter or script file will work for the remaining steps in the guide, we recommend using a Jupyter Notebook, since they are included in the OSS GX installation and give the best experience of both composing a script file and running code in a live interpreter. ::: Switch to Jupyter Notebook and import modules we're going to use in this tutorial. python title=\"Jupyter Notebook\" import great_expectations as gx import pandas as pd import os 1.3 Create Data Context Paste this snippet into the next notebook cell to instantiate Cloud Data Context. :::caution Please note that access tokens are sensitive information and should not be committed to version control software. Alternatively, add these as Data Context config variables ::: ```python title=\"Jupyter Notebook\" os.environ[\"GX_CLOUD_ACCESS_TOKEN\"] = \"\" your organization_id is indicated on https://app.greatexpectations.io/tokens page os.environ[\"GX_CLOUD_ORGANIZATION_ID\"] = \"\" context = gx.get_context() ``` 2. Create Data Source Modify the following snippet code to connect to your Data Source. In case you don't have some data handy to test in this guide, we can use the NYC taxi data. This is an open data set which is updated every month. Each record in the data corresponds to one taxi ride. You can find a link to it in the snippet below. :::caution Please note you should not include sensitive info/credentials directly in the config while connecting to your Data Source, since this would be persisted in plain text in the database and presented in Cloud UI. If credentials/full connection string is required, you should use a config variables file. ::: ```python title=\"Jupyter Notebook\" Give your datasource a name datasource_name = None datasource = context.sources.add_pandas(datasource_name) Give your first Asset a name asset_name = None path_to_data = None to use sample data uncomment next line path_to_data = \"https://raw.githubusercontent.com/great-expectations/gx_tutorials/main/data/yellow_tripdata_sample_2019-01.csv\" asset = datasource.add_csv_asset(asset_name, filepath_or_buffer=path_to_data) Build batch request batch_request = asset.build_batch_request() ``` In case you need more details on how to connect to your specific data system, we have step by step how-to guides that cover many common cases. Start here 3. Create Expectations 3.1 Create Expectation Suite An Expectation Suite is a collection of verifiable assertions about data. Run this snippet to create a new, empty Expectation Suite: ```python title=\"Jupyter Notebook\" expectation_suite_name = None assert expectation_suite_name is not None, \"Please set expectation_suite_name.\" expectation_suite = context.add_expectation_suite(     expectation_suite_name=expectation_suite_name ) ``` 3.2 Add Expectation Modify and run this snippet to add an Expectation to the Expectation Suite you just created: ```python title=\"Jupyter Notebook\" Get an existing Expectation Suite expectation_suite_id = expectation_suite.ge_cloud_id expectation_suite = context.get_expectation_suite(ge_cloud_id=expectation_suite_id) column_name = None # set column name you want to test here assert column_name is not None, \"Please set column_name.\" Look up all expectations types here - https://greatexpectations.io/expectations/ expectation_configuration = gx.core.ExpectationConfiguration(**{   \"expectation_type\": \"expect_column_min_to_be_between\",   \"kwargs\": {     \"column\": column_name,     \"min_value\": 0.1   },   \"meta\":{}, }) expectation_suite.add_expectation(     expectation_configuration=expectation_configuration ) print(expectation_suite) Save the Expectation Suite context.save_expectation_suite(expectation_suite=expectation_suite) ``` With the Expectation defined above, we are stating that we expect the column of your choice to always be populated. That is: none of the column's values should be null. 4. Validate data 4.1 Create and run Checkpoint Now that we have connected to data and defined an Expectation, it is time to validate whether our data meets the Expectation. To do this, we define a Checkpoint, which will allow us to repeat the Validation in the future. Once we have created the Checkpoint, we will run it and get back the results from our Validation. ```python title=\"Jupyter Notebook\" checkpoint_name = None # name your checkpoint here assert checkpoint_name is not None, \"Please set checkpoint_name.\" checkpoint_config = {   \"name\": checkpoint_name,   \"validations\": [{       \"expectation_suite_name\": expectation_suite_name,       \"expectation_suite_ge_cloud_id\": expectation_suite.ge_cloud_id,       \"batch_request\": {           \"datasource_name\": datasource.name,           \"data_asset_name\": asset.name,       },   }],   \"config_version\": 1,   \"class_name\": \"Checkpoint\" } context.add_or_update_checkpoint(**checkpoint_config) checkpoint = context.get_checkpoint(checkpoint_name) checkpoint.run() ``` 4.2 Review your results After you run the Checkpoint, you should see a validation_result_url in the result, that takes you directly to GX Cloud, so you can see your Expectations and Validation Results in the GX Cloud UI. Alternatively, you can visit the Checkpoints page and filter by the Checkpoint, Expectation Suite, or Data Asset you want to see the results for. 4.3 (Optional) Add Slack notifications Add the send_slack_notification_on_validation_result Action to the Checkpoint configuration.   Webhook config ```python title=\"Jupyter Notebook\" slack_webhook = None # put the actual webhook URL assert slack_webhook is not None, \"Please set slack_webhook.\" checkpoint_config = {     ...     \"action_list\": [         {             \"name\": \"send_slack_notification_on_validation_result\", # name can be set to any value             \"action\": {                 \"class_name\": \"SlackNotificationAction\",                 \"slack_webhook\": slack_webhook,                 \"notify_on\": \"all\", # possible values: \"all\", \"failure\", \"success\"                 \"renderer\": {                     \"module_name\": \"great_expectations.render.renderer.slack_renderer\",                     \"class_name\": \"SlackRenderer\",                 },             },         },         {             \"name\": \"store_validation_result\",             \"action\": {                 \"class_name\": \"StoreValidationResultAction\",             }         },         {             \"name\": \"store_evaluation_params\",             \"action\": {                 \"class_name\": \"StoreEvaluationParametersAction\",             }         },     ], } ```   Slack bot config ```python title=\"Jupyter Notebook\" bot_token = None # put the actual bot token assert bot_token is not None, \"Please set bot_token.\" channel_name = None # put the actual Slack channel name assert channel_name is not None, \"Please set channel_name.\" checkpoint_config = {     ...     \"action_list\": [         {             \"name\": \"send_slack_notification_on_validation_result\", # name can be set to any value             \"action\": {                 \"class_name\": \"SlackNotificationAction\",                 \"slack_token\": bot_token,                 \"slack_channel\": channel_name,                 \"notify_on\": \"all\", # possible values: \"all\", \"failure\", \"success\"                 \"renderer\": {                     \"module_name\": \"great_expectations.render.renderer.slack_renderer\",                     \"class_name\": \"SlackRenderer\",                 },             },         },         {             \"name\": \"store_validation_result\",             \"action\": {                 \"class_name\": \"StoreValidationResultAction\",             }         },         {             \"name\": \"store_evaluation_params\",             \"action\": {                 \"class_name\": \"StoreEvaluationParametersAction\",             }         },     ], } ```   Run your Checkpoint to validate a Batch of data and receive Slack notification on the success or failure of the Expectation Suite's Validation.  Find additional information here Next Steps Now that you've seen how to implement the GX workflow, it is time to customize the workflow to suit your specific use cases! To help with this we have prepared more detailed guides tailored to specific environments and resources. To get all the snippets above in one script, visit GX OSS repository To invite additional team members to the app visit \u201cSettings\u201d > \u201cUsers\u201d. For more details on installing GX for use with local filesystems, please see:  For guides on installing GX for use with cloud storage systems, please reference:  For information on installing GX for use with SQL databases, see:  And for instructions on installing GX for use with hosted data systems, read: ", "How to set up GX to work with PostgreSQL": " title: How to set up GX to work with PostgreSQL tag: [how-to, setup] keywords: [Great Expectations, SQL, PostgreSQL]  How to set up Great Expectations to work with PostgreSQL import TechnicalTag from '/docs/term_tags/_tag.mdx'; import Prerequisites from '/docs/components/_prerequisites.jsx'   import PythonCheckVersion from '/docs/components/setup/python_environment/_python_check_version.mdx'  import PythonCreateVenv from '/docs/components/setup/python_environment/_python_create_venv.md' import TipPythonOrPython3Executable from '/docs/components/setup/python_environment/_tip_python_or_python3_executable.md'  import InstallDependencies from '/docs/components/setup/dependencies/_sql_install_dependencies.mdx'  import GxVerifyInstallation from '/docs/components/setup/_gx_verify_installation.md'  import InitializeDataContextFromCli from '/docs/components/setup/data_context/_filesystem_data_context_initialize_with_cli.md' import VerifyDataContextInitializedFromCli from '/docs/components/setup/data_context/_filesystem_data_context_verify_initialization_from_cli.md'  import ConfigureCredentialsInDataContext from '/docs/components/setup/dependencies/_postgresql_configure_credentials_in_config_variables_yml.md'  import PostgreSqlFurtherConfiguration from '/docs/components/setup/next_steps/_links_for_adding_postgresql_configurations_to_data_context.md' This guide will walk you through best practices for creating your GX Python environment and demonstrate how to locally install Great Expectations along with the necessary dependencies for working with PostgreSQL. Prerequisites   The ability to install Python modules with pip   Steps 1. Check your Python version   2. Create a Python virtual environment  3. Install GX with optional dependencies for PostgreSQL  4. Verify that GX has been installed correctly  5. Initialize a Data Context to store your PostgreSQL credentials  :::info Verifying the Data Context initialized successfully  ::: 6. Configure the config_variables.yml file with your PostgreSQL credentials  Next steps ", "Request data from a Data Asset": " title: Request data from a Data Asset tag: [how-to, connect to data] description: A technical guide demonstrating how to request data from a Data Asset. keywords: [Great Expectations, Data Asset, Batch Request, fluent configuration method]   import Prerequisites from '/docs/components/_prerequisites.jsx'  import ImportGxAndInstantiateADataContext from '/docs/components/setup/data_context/_import_gx_and_instantiate_a_data_context.md'  import GetExistingDataAssetFromExistingDatasource from '/docs/components/setup/datasource/data_asset/_get_existing_data_asset_from_existing_datasource.md' This guide demonstrates how you can request data from a Data Source that has been defined with the context.sources.add_* method. Prerequisites    An installation of GX A Data Source with a configured Data Asset    Import GX and instantiate a Data Context  Retrieve your Data Asset  Build an options dictionary for your Batch Request (Optional) An options dictionary can be used to limit the Batches returned by a Batch Request.  Omitting the options dictionary will result in all available Batches being returned. The structure of the options dictionary will depend on the type of Data Asset being used.  The valid keys for the options dictionary can be found by checking the Data Asset's batch_request_options property. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/get_existing_data_asset_from_existing_datasource_pandas_filesystem_example.py my_batch_request_options\" The batch_request_options property is a tuple that contains all the valid keys that can be used to limit the Batches returned in a Batch Request. You can create a dictionary of keys pulled from the batch_request_options tuple and values that you want to use to specify the Batch or Batches your Batch Request should return, then pass this dictionary in as the options parameter when you build your Batch Request. Build your Batch Request We will use the build_batch_request(...) method of our Data Asset to generate a Batch Request. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/get_existing_data_asset_from_existing_datasource_pandas_filesystem_example.py my_batch_request\" For dataframe Data Assets, the dataframe is always specified as the argument of exactly one API method: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/get_existing_data_asset_from_existing_datasource_pandas_filesystem_example.py build_batch_request_with_dataframe\" Verify that the correct Batches were returned The get_batch_list_from_batch_request(...) method will return a list of the Batches a given Batch Request refers to. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/get_existing_data_asset_from_existing_datasource_pandas_filesystem_example.py my_batch_list\" Because Batch definitions are quite verbose, it is easiest to determine what data the Batch Request will return by printing just the batch_spec of each Batch. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/get_existing_data_asset_from_existing_datasource_pandas_filesystem_example.py print_batch_spec\" Next steps Now that you have a retrieved data from a Data Asset, you may be interested in creating Expectations about your data: - How to create Expectations while interactively evaluating a set of data - How to use the Onboarding Data Assistant to evaluate data", "How to connect to data on Azure Blob Storage using Pandas": " title: How to connect to data on Azure Blob Storage using Pandas tag: [how-to, connect to data] description: A brief how-to guide covering ... keywords: [Great Expectations, Azure Blob Storage, Pandas]   import Prerequisites from '/docs/components/_prerequisites.jsx' import PrereqInstallGxWithDependencies from '/docs/components/prerequisites/_gx_installed_with_abs_dependencies.md' import Introduction from '/docs/components/connect_to_data/intros/_abs_pandas_or_spark.mdx'  import ImportGxAndInstantiateADataContext from '/docs/components/setup/data_context/_import_gx_and_instantiate_a_data_context.md'  import BatchingRegexExplaination from '/docs/components/connect_to_data/cloud/_batching_regex_explaination.mdx' import AbsFluentAddDataAssetConfigKeys from '/docs/components/connect_to_data/cloud/_abs_fluent_data_asset_config_keys.mdx'  import AfterCreateNonSqlDatasource from '/docs/components/connect_to_data/next_steps/_after_create_non_sql_datasource.md'  Prerequisites    Access to data in Azure Blob Storage    Steps 1. Import GX and instantiate a Data Context  2. Create a Data Source We can define an Azure Blob Storage datasource by providing these pieces of information: - name: In our example, we will name our Data Source \"my_datasource\" - azure_options: We provide authentication settings here python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_azure_blob_storage_using_pandas.py define_add_pandas_abs_args\" We can create a Data Source that points to our Azure Blob Storage with the code: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_azure_blob_storage_using_pandas.py create_datasource\" :::tip Where did that connection string come from? In the above example, the value for account_url will be substituted for the contents of the AZURE_STORAGE_CONNECTION_STRING key you configured when you installed GX and set up your Azure Blob Storage dependancies. ::: 3. Add ABS data to the Data Source as a Data Asset  Once these values have been defined, we will create our DataAsset with the code: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_azure_blob_storage_using_pandas.py add_asset\"  Next steps  Additional information Related reading For more details regarding storing credentials for use with GX, please see our guide: How to configure credentials", "How to connect to data on Azure Blob Storage using Spark": " title: How to connect to data on Azure Blob Storage using Spark tag: [how-to, connect to data] description: A brief how-to guide covering ... keywords: [Great Expectations, Azure Blob Storage, Spark]   import Prerequisites from '/docs/components/_prerequisites.jsx' import PrereqInstallGxWithDependencies from '/docs/components/prerequisites/_gx_installed_with_abs_dependencies.md' import Introduction from '/docs/components/connect_to_data/intros/_abs_pandas_or_spark.mdx'  import ImportGxAndInstantiateADataContext from '/docs/components/setup/data_context/_import_gx_and_instantiate_a_data_context.md'  import BatchingRegexExplaination from '/docs/components/connect_to_data/cloud/_batching_regex_explaination.mdx' import AbsFluentAddDataAssetConfigKeys from '/docs/components/connect_to_data/cloud/_abs_fluent_data_asset_config_keys.mdx'  import AfterCreateNonSqlDatasource from '/docs/components/connect_to_data/next_steps/_after_create_non_sql_datasource.md'  Prerequisites    Access to data in Azure Blob Storage    Steps 1. Import GX and instantiate a Data Context  2. Create a Data Source We can define an Azure Blob Storage datasource by providing these pieces of information: - name: In our example, we will name our Data Source \"my_datasource\" - azure_options: We provide authentication settings here python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_azure_blob_storage_using_spark.py define_add_spark_abs_args\" We can create a Data Source that points to our Azure Blob Storage with the code: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_azure_blob_storage_using_spark.py create_datasource\" :::tip Where did that connection string come from? In the above example, the value for account_url will be substituted for the contents of the AZURE_STORAGE_CONNECTION_STRING key you configured when you installed GX and set up your Azure Blob Storage dependancies. ::: 3. Add ABS data to the Data Source as a Data Asset  Once these values have been defined, we will create our DataAsset with the code: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_azure_blob_storage_using_spark.py add_asset\"  Next steps  Additional information Related reading For more details regarding storing credentials for use with GX, please see our guide: How to configure credentials", "How to connect to data on GCS using Pandas": " title: How to connect to data on GCS using Pandas tag: [how-to, connect to data] description: A technical guide demonstrating how to connect Great Expectations to dat stored on Google Cloud Storage using Pandas. keywords: [Great Expectations, Google Cloud Storage, GCS, Pandas]   import Prerequisites from '/docs/components/_prerequisites.jsx'  import ImportGxAndInstantiateADataContext from '/docs/components/setup/data_context/_import_gx_and_instantiate_a_data_context.md'  import BatchingRegexExplaination from '/docs/components/connect_to_data/cloud/_batching_regex_explaination.mdx' import GCSFluentAddDataAssetConfigKeys from '/docs/components/connect_to_data/cloud/_gcs_fluent_data_asset_config_keys.mdx'  import AfterCreateNonSqlDatasource from '/docs/components/connect_to_data/next_steps/_after_create_non_sql_datasource.md' In this guide we will demonstrate how to use Pandas to connect to data stored on Google Cloud Storage.  In our examples, we will specifically be connecting to csv files.  However, Great Expectations supports most types of files that Pandas has read methods for. Prerequisites   An installation of GX set up to work with GCS Access to data on a GCS bucket    Steps 1. Import GX and instantiate a Data Context  2. Create a Data Source We can define a GCS datasource by providing three pieces of information: - name: In our example, we will name our Data Source \"my_gcs_datasource\" - bucket_or_name: In this example, we will provide a GCS bucket - gcs_options: We can provide various additional options here, but in this example we will leave this empty and use the default values. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_gcs_using_pandas.py define_add_pandas_gcs_args\" Once we have those three elements, we can define our Data Source like so: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_gcs_using_pandas.py create_datasource\" 3. Add GCS data to the Data Source as a Data Asset  We will define these values and create our DataAsset with the code: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_gcs_using_pandas.py add_asset\"  Next steps  Additional information   External APIs For more information on Google Cloud and authentication, please visit the following: * gcloud CLI Tutorial * GCS Python API Docs Related reading For more details regarding storing credentials for use with GX, please see our guide: How to configure credentials", "How to connect to data on GCS using Spark": " title: How to connect to data on GCS using Spark tag: [how-to, connect to data] description: A technical guide demonstrating how to connect Great Expectations to dat stored on Google Cloud Storage using Spark. keywords: [Great Expectations, Google Cloud Storage, GCS, Spark]   import Prerequisites from '/docs/components/_prerequisites.jsx'  import ImportGxAndInstantiateADataContext from '/docs/components/setup/data_context/_import_gx_and_instantiate_a_data_context.md'  import BatchingRegexExplaination from '/docs/components/connect_to_data/cloud/_batching_regex_explaination.mdx' import GCSFluentAddDataAssetConfigKeys from '/docs/components/connect_to_data/cloud/_gcs_fluent_data_asset_config_keys.mdx'  import AfterCreateNonSqlDatasource from '/docs/components/connect_to_data/next_steps/_after_create_non_sql_datasource.md' In this guide we will demonstrate how to use Spark to connect to data stored on Google Cloud Storage.  In our examples, we will specifically be connecting to csv files. Prerequisites   An installation of GX set up to work with GCS Access to data on a GCS bucket    Steps 1. Import GX and instantiate a Data Context  2. Create a Data Source We can define a GCS datasource by providing three pieces of information: - name: In our example, we will name our Data Source \"my_gcs_datasource\" - bucket_or_name: In this example, we will provide a GCS bucket - gcs_options: We can provide various additional options here, but in this example we will leave this empty and use the default values. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_gcs_using_spark.py define_add_spark_gcs_args\" Once we have those three elements, we can define our Data Source like so: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_gcs_using_spark.py create_datasource\" 3. Add GCS data to the Data Source as a Data Asset  We will define these values and create our DataAsset with the code: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_gcs_using_spark.py add_asset\" :::info Optional parameters: header and infer_schema In the above example there are two parameters that are optional, depending on the structure of your file.  If the file does not have a header line, the header parameter can be left out: it will default to false.  Likewise, if you do not want GX to infer the schema of your file, you can leave off the infer_schema parameter; it will also default to false. :::  Next steps  Additional information   External APIs For more information on Google Cloud and authentication, please visit the following: * gcloud CLI Tutorial * GCS Python API Docs Related reading For more details regarding storing credentials for use with GX, please see our guide: How to configure credentials", "How to connect to data on S3 using Pandas": " title: How to connect to data on S3 using Pandas tag: [how-to, connect to data] description: A technical guide demonstrating how to connect Great Expectations to dat stored on a Amazon Web Services S3 bucket using Pandas. keywords: [Great Expectations, Amazon Web Services, AWS S3, Pandas]   import Prerequisites from '/docs/components/_prerequisites.jsx'  import ImportGxAndInstantiateADataContext from '/docs/components/setup/data_context/_import_gx_and_instantiate_a_data_context.md'  import BatchingRegexExplaination from '/docs/components/connect_to_data/cloud/_batching_regex_explaination.mdx' import S3FluentAddDataAssetConfigKeys from '/docs/components/connect_to_data/cloud/_s3_fluent_data_asset_config_keys.mdx'  import AfterCreateNonSqlDatasource from '/docs/components/connect_to_data/next_steps/_after_create_non_sql_datasource.md' In this guide we will demonstrate how to use Pandas to connect to data stored on AWS S3.  In our examples, we will specifically be connecting to csv files.  However, Great Expectations supports most types of files that Pandas has read methods for. Prerequisites   An installation of GX set up to work with S3 Access to data on a S3 bucket    Steps 1. Import GX and instantiate a Data Context  2. Create a Data Source We can define a S3 datasource by providing three pieces of information: - name: In our example, we will name our Data Source \"my_s3_datasource\" - bucket_name: The name of our S3 bucket - boto3_options: We can provide various additional options here, but in this example we will leave this empty and use the default values. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_s3_using_pandas.py define_add_pandas_s3_args\" :::tip What can boto3_options specify? The parameter boto3_options will allow you to pass such things as: - endpoint_url: specifies an S3 endpoint.  You can use an environment variable such as \"${S3_ENDPOINT}\" to securely include this in your code.  The string \"${S3_ENDPOINT}\" will be replaced with the value of the corresponding environment variable. - region_name: Your AWS region name. ::: Once we have those three elements, we can define our Data Source like so: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_s3_using_pandas.py create_datasource\" 3. Add S3 data to the Data Source as a Data Asset  We will define these values and create our DataAsset with the code: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_s3_using_pandas.py add_asset\"  Next steps  Additional information   Related reading For more details regarding storing credentials for use with GX, please see our guide: How to configure credentials", "How to connect to data on S3 using Spark": " title: How to connect to data on S3 using Spark tag: [how-to, connect to data] description: A technical guide demonstrating how to connect Great Expectations to dat stored on a Amazon Web Services S3 bucket using Spark. keywords: [Great Expectations, Amazon Web Services, AWS S3, Spark]   import Prerequisites from '/docs/components/_prerequisites.jsx'  import ImportGxAndInstantiateADataContext from '/docs/components/setup/data_context/_import_gx_and_instantiate_a_data_context.md'  import BatchingRegexExplaination from '/docs/components/connect_to_data/cloud/_batching_regex_explaination.mdx' import S3FluentAddDataAssetConfigKeys from '/docs/components/connect_to_data/cloud/_s3_fluent_data_asset_config_keys.mdx'  import AfterCreateNonSqlDatasource from '/docs/components/connect_to_data/next_steps/_after_create_non_sql_datasource.md' In this guide we will demonstrate how to use Spark to connect to data stored on AWS S3.  In our examples, we will specifically be connecting to csv files. Prerequisites   An installation of GX set up to work with S3 Access to data on a S3 bucket    Steps 1. Import GX and instantiate a Data Context  2. Create a Data Source We can define a S3 datasource by providing three pieces of information: - name: In our example, we will name our Data Source \"my_s3_datasource\" - bucket_name: The name of our S3 bucket - boto3_options: We can provide various additional options here, but in this example we will leave this empty and use the default values. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_s3_using_spark.py define_add_spark_s3_args\" :::tip What can boto3_options specify? The parameter boto3_options will allow you to pass such things as: - endpoint_url: specifies an S3 endpoint.  You can use an environment variable such as \"${S3_ENDPOINT}\" to securely include this in your code.  The string \"${S3_ENDPOINT}\" will be replaced with the value of the corresponding environment variable. - region_name: Your AWS region name. ::: Once we have those three elements, we can define our Data Source like so: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_s3_using_spark.py create_datasource\" 3. Add S3 data to the Data Source as a Data Asset  We will define these values and create our DataAsset with the code: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_s3_using_spark.py add_asset\"  Next steps  Additional information   Related reading For more details regarding storing credentials for use with GX, please see our guide: How to configure credentials", "Organize Batches in a file-based Data Asset": " title: Organize Batches in a file-based Data Asset tag: [how-to, connect to data] description: A technical guide demonstrating how to organize Batches of data in a file-based Data Asset. keywords: [Great Expectations, Data Asset, Batch Request, fluent configuration method, GCS, Google Cloud Storage, AWS S3, Amazon Web Services S3, Azure Blob Storage, Local Filesystem]  import TechnicalTag from '/docs/term_tags/_tag.mdx';   import Prerequisites from '/docs/components/_prerequisites.jsx'  import ImportGxAndInstantiateADataContext from '/docs/components/setup/data_context/_import_gx_and_instantiate_a_data_context.md'  import TipFilesystemDatasourceNestedSourceDataFolders from '/docs/components/connect_to_data/filesystem/_tip_filesystem_datasource_nested_source_data_folders.md'  import AfterCreateAndConfigureDataAsset from '/docs/components/connect_to_data/next_steps/_after_create_and_configure_data_asset.md' This guide demonstrates how to organize Batches in a file-based Data Asset. This includes how to use a regular expression to indicate which files should be returned as Batches and how to add Batch Sorters to a Data Asset to specify the order in which Batches are returned. :::caution Datasources defined with the block-config method If you are using a Data Source that was created with the advanced block-config method, see the following resources: - How to configure a Spark Data Source - How to configure a Pandas Data Source ::: Prerequisites   A working installation of Great Expectations A Data Source that connects to a location with source data files   Import GX and instantiate a Data Context  Retrieve a file-based Data Source For this guide, we will use a previously defined Data Source named \"my_datasource\".  For purposes of our demonstration, this Data Source is a Pandas Filesystem Data Source that uses a folder named \"data\" as its base_folder. To retrieve this Data Source, we will supply the get_datasource(...) method of our Data Context with the name of the Data Source we wish to retrieve: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/organize_batches_in_pandas_filesystem_datasource.py my_datasource\" Create a batching_regex In a file-based Data Asset, any file that matches a provided regular expression (the batching_regex parameter) will be included as a Batch in the Data Asset.  Therefore, to organize multiple files into Batches in a single Data Asset we must define a regular expression that will match one or more of our source data files. For this example, our Data Source points to a folder that contains the following files: - \"yellow_tripdata_sample_2019-03.csv\" - \"yellow_tripdata_sample_2020-07.csv\" - \"yellow_tripdata_sample_2021-02.csv\" To create a batching_regex that matches multiple files, we will include a named group in our regular expression: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/organize_batches_in_pandas_filesystem_datasource.py my_batching_regex\" In the above example, the named group \"year\" will match any four numeric characters in a file name.  This will result in each of our source data files matching the regular expression. :::tip Why use named groups? By naming the group in your batching_regex you make it something you can reference in the future.  When requesting data from this Data Asset, you can use the names of your regular expression groups to limit the Batches that are returned. For more information, please see: How to request data from a Data Asset :::  For more information on how to format regular expressions, we recommend referencing Python's official how-to guide for working with regular expressions. Add a Data Asset using the batching_regex Now that we have put together a regular expression that will match one or more of the files in our Data Source's base_folder, we can use it to create our Data Asset.  Since the files in this particular Data Source's base_folder are csv files, we will use the add_pandas_csv(...) method of our Data Source to create the new Data Asset: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/organize_batches_in_pandas_filesystem_datasource.py my_asset\" :::tip What if I don't provide a batching_regex? If you choose to omit the batching_regex parameter, your Data Asset will automatically use the regular expression \".*\" to match all files. ::: Add Batch Sorters to the Data Asset (Optional) We will now add a Batch Sorter to our Data Asset.  This will allow us to explicitly state the order in which our Batches are returned when we request data from the Data Asset.  To do this, we will pass a list of sorters to the add_sorters(...) method of our Data Asset. The items in our list of sorters will correspond to the names of the groups in our batching_regex that we want to sort our Batches on.  The names are prefixed with a + or a - depending on if we want to sort our Batches in ascending or descending order based on the given group. When there are multiple named groups, we can include multiple items in our sorter list and our Batches will be returned in the order specified by the list: sorted first according to the first item, then the second, and so forth. In this example we have two named groups, \"year\" and \"month\", so our list of sorters can have up to two elements.  We will add an ascending sorter based on the contents of the regex group \"year\" and a descending sorter based on the contents of the regex group \"month\": python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/organize_batches_in_pandas_filesystem_datasource.py add_sorters\" Use a Batch Request to verify the Data Asset works as desired To verify that our Data Asset will return the desired files as Batches, we will define a quick Batch Request that will include all the Batches available in the Data asset.  Then we will use that Batch Request to get a list of the returned Batches. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/organize_batches_in_pandas_filesystem_datasource.py my_batch_list\" Because a Batch List contains a lot of metadata, it will be easiest to verify which files were included in the returned Batches if we only look at the batch_spec of each returned Batch: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/organize_batches_in_pandas_filesystem_datasource.py print_batch_spec\" Related documentation  How to request data from a Data Asset Use a Data Asset to create Expectations while interactively evaluating a set of data Use the Onboarding Data Assistant to evaluate one or more Batches of data and create Expectations ", "How to organize Batches in a SQL-based Data Asset": " title: How to organize Batches in a SQL-based Data Asset tag: [how-to, connect to data] description: A technical guide demonstrating how to split the data returned by a SQL Data Asset into multiple Batches and explicitly sort those Batches. keywords: [Great Expectations, Data Asset, Batch Request, fluent configuration method, SQL]  import TechnicalTag from '/docs/term_tags/_tag.mdx'; import AfterRequestDataFromADataAsset from '/docs/components/connect_to_data/next_steps/_after_request_data_from_a_data_asset.md'   import Prerequisites from '/docs/components/_prerequisites.jsx' import SetupAndInstallForSqlData from '/docs/components/setup/link_lists/_setup_and_install_for_sql_data.md' import ConnectingToSqlDatasourcesFluently from '/docs/components/connect_to_data/link_lists/_connecting_to_sql_datasources_fluently.md'  import ImportGxAndInstantiateADataContext from '/docs/components/setup/data_context/_import_gx_and_instantiate_a_data_context.md'  import AfterCreateAndConfigureDataAsset from '/docs/components/connect_to_data/next_steps/_after_create_and_configure_data_asset.md' In this guide we will demonstrate the ways in which Batches can be organized in a SQL-based Data Asset.  We will discuss how to use Splitters to divide the data in a table or query based on the contents of a provided field.  We will also show how to add Batch Sorters to a Data Asset in order to specify the order in which Batches are returned. Prerequisites   A working installation of Great Expectations A Data Asset in a SQL-based Data Source    :::caution Datasources defined with the block-config method If you're using a Data Source that was created with the advanced block-config method, see How to configure a SQL Data Source with the block-config method. ::: Steps 1. Import GX and instantiate a Data Context  2. Retrieve a SQL Data Source and Data Asset For this guide, we will use a previously defined SQL Data Source named \"my_datasource\" with a Table Data Asset called \"my_asset\" which points to a table with taxi data.  To retrieve this Data Source, we will supply the get_datasource(...) method of our Data Context with the name of the Data Source we wish to retrieve: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/organize_batches_in_sqlite_datasource.py my_datasource\" 3. Add a Splitter to the Data Asset Our table has a datetime column called \"pickup_datetime\" which we will use to split our TableAsset into Batches. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/organize_batches_in_sqlite_datasource.py add_splitter_year_and_month\" 4. (Optional) Add Batch Sorters to the Data Asset We will now add Batch Sorters to our Data Asset.  This will allow us to explicitly state the order in which our Batches are returned when we request data from the Data Asset.  To do this, we will pass a list of sorters to the add_sorters(...) method of our Data Asset. In this example we split \"pickup_datetime\" column on \"year\" and \"month\", so our list of sorters can have up to two elements.  We will add an ascending sorter based on the contents of the splitter group \"year\" and a descending sorter based on the contents of the splitter group \"month\": python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/organize_batches_in_sqlite_datasource.py add_sorters\" 5. Use a Batch Request to verify the Data Asset works as desired To verify that our Data Asset will return the desired files as Batches, we will define a quick Batch Request that will include all the Batches available in the Data asset.  Then we will use that Batch Request to get a list of the returned Batches. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/organize_batches_in_sqlite_datasource.py my_batch_list\" Because a Batch List contains a lot of metadata, it will be easiest to verify which files were included in the returned Batches if we only look at the batch_spec of each returned Batch: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/organize_batches_in_sqlite_datasource.py print_batch_spec\" Next steps Now that you have further configured a file-based Data Asset, you may want to look into: ", "\"Connect to SQL database source data\"": " sidebar_label: \"Connect to SQL database source data\" title: \"Connect to SQL database source data\" id: connect_sql_source_data description: Connect to source data stored on SQL databases. toc_min_heading_level: 2 toc_max_heading_level: 2  import Prerequisites from '/docs/components/_prerequisites.jsx' import ImportGxAndInstantiateADataContext from '/docs/components/setup/data_context/_import_gx_and_instantiate_a_data_context.md' import AfterCreateSqlDatasource from '/docs/components/connect_to_data/next_steps/_after_create_sql_datasource.md' import PostgreSqlConfigureCredentialsInConfigVariablesYml from '/docs/components/setup/dependencies/_postgresql_configure_credentials_in_config_variables_yml.md' import TabItem from '@theme/TabItem'; import Tabs from '@theme/Tabs'; Use the information provided here to connect to source data stored in SQL databases. Great Expectations (GX) uses SQLAlchemy to connect to SQL source data, and most of the SQL dialects supported by SQLAlchemy are also supported by GX. For more information about the SQL dialects supported by SQLAlchemy, see Dialects.   SQL Connect GX to a SQL database to access source data. Prerequisites   An installation of GX set up to work with SQL Source data stored in a SQL database    Import GX and instantiate a Data Context  Determine your connection string GX supports numerous SQL source data systems.  However, most SQL dialects have their own specifications for defining a connection string. See the dialect documentation to determine the connection string for your SQL database. :::info Some examples of different connection strings: The following are some of the connection strings that are available for different SQL dialects:  AWS Athena: awsathena+rest://@athena.<REGION>.amazonaws.com/<DATABASE>?s3_staging_dir=<S3_PATH> BigQuery: bigquery://<GCP_PROJECT>/<BIGQUERY_DATASET>?credentials_path=/path/to/your/credentials.json MSSQL: mssql+pyodbc://<USERNAME>:<PASSWORD>@<HOST>:<PORT>/<DATABASE>?driver=<DRIVER>&charset=utf&autocommit=true MySQL: mysql+pymysql://<USERNAME>:<PASSWORD>@<HOST>:<PORT>/<DATABASE> PostgreSQL: postgresql+psycopg2://<USERNAME>:<PASSWORD>@<HOST>:<PORT>/<DATABASE> Redshift: postgresql+psycopg2://<USER_NAME>:<PASSWORD>@<HOST>:<PORT>/<DATABASE>?sslmode=<SSLMODE> Snowflake: snowflake://<USER_NAME>:<PASSWORD>@<ACCOUNT_NAME>/<DATABASE_NAME>/<SCHEMA_NAME>?warehouse=<WAREHOUSE_NAME>&role=<ROLE_NAME>&application=great_expectations_oss SQLite: sqlite:///<PATH_TO_DB_FILE> Trino: trino://<USERNAME>:<PASSWORD>@<HOST>:<PORT>/<CATALOG>/<SCHEMA>  ::: The following code examples use a PostgreSQL connection string. A PostgreSQL connection string connects GX to the SQL database. Run the following code to store the connection string in the connection_string variable with plain text credentials: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sql_data.py sql_connection_string\" :::tip Is there a more secure way to include my credentials? You can use environment variables or a key in config_variables.yml to store connection string passwords.  After you define your password, you reference it in your connection string similar to this example: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sql_data.py connection_string\" In the previous example MY_PASSWORD is the name of the environment variable, or the key to the value in config_variables.yml that corresponds to your password. If you include a password as plain text in your connection string when you define your Data Source, GX automatically removes it, adds it to config_variables.yml, and substitutes it in the Data Source saved configuration with a variable. ::: Create a SQL Data Source Run the following Python code to create a SQL Data Source: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sql_data.py add_sql\" Next steps    PostgreSQL Connect GX to a PostgreSQL database to access source data. Prerequisites   An installation of GX set up to work with PostgreSQL Source data stored in a PostgreSQL database    Import GX and instantiate a Data Context  Determine your connection string The following code examples use a PostgreSQL connection string. A PostgreSQL connection string connects GX to the PostgreSQL database. The following code is an example of a PostgreSQL connection string format: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_postgreql_data.py connection_string\" :::tip Is there a more secure way to store my credentials than plain text in a connection string?  ::: Create a PostgreSQL Data Source   Run the following Python code to set the name and connection_string variables: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_postgresql_data.py connection_string2\"   Run the following Python code to create a PostgreSQL Data Source: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_postgresql_data.py add_postgres\"   Connect to a specific set of data with a Data Asset To connect the Data Source to a specific set of data in the database, you define a Data Asset in the Data Source. A Data Source can contain multiple Data Assets. Each Data Asset acts as the interface between GX and the specific set of data it is configured for. With SQL databases, you can use Table or Query Data Assets. The Table Data Asset connects GX to the data contained in a single table in the source database. The Query Data Asset connects GX to the data returned by a SQL query. :::tip Maximum allowable Data Assets for a Data Source Although there isn't a maximum number of Data Assets you can define for a Data Source, you must create a single Data Asset to allow GX to retrieve data from your Data Source. ::: Connect a Data Asset to the data in a table (Optional)   Run the following Python code to identify the table to connect to with a Table Data Asset: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_postgresql_data.py asset_name\"   Run the following Python code to create the Data Asset: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_postgresql_data.py add_table_asset\"   Connect a Data Asset to the data returned by a query (Optional)   Run the following Python code to define a Query Data Asset: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_postgresql_data.py asset_query\"   Run the following Python code to create the Data Asset: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_postgresql_data.py add_query_asset\"   Connect to additional tables or queries (Optional) Repeat the previous steps to add additional Data Assets.   SQLite Connect GX to a SQLite database to access source data. Prerequisites   An installation of GX set up to work with SQLite Source data stored in a SQLite database    Import GX and instantiate a Data Context  Determine your connection string The following code examples use a SQLite connection string. A SQLite connection string connects GX to the SQLite database. The following code is an example of a SQLite connection string format: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sqlite_data.py connection_string\" Create a SQLite Data Source   Run the following Python code to set the name and connection_string variables: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sqlite_data.py datasource_name\"   Run the following Python code to create a SQLite Data Source: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sqlite_data.py datasource\" :::caution Using add_sql(...) instead of add_sqlite(...) The SQL Data Source created with add_sql can connect to data in a SQLite database. However, add_sqlite(...) is the preferred method. SQLite stores datetime values as strings.  Because of this, a general SQL Data Source sees datetime columns as string columns. A SQLite Data Source has additional handling in place for these fields, and also has additional error reporting for SQLite specific issues. If you are working with SQLite source data, use add_sqlite(...) to create your Data Source. :::   Connect to the data in a table (Optional)   Run the following Python code to set the asset_name and asset_table_name variables: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sqlite_data.py asset_name\"   Run the following Python code to create the Data Asset: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sqlite_data.py table_asset\"   Connect to the data in a query (Optional)   Run the following Python code to define a Query Data Asset: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sqlite_data.py asset_query\" 2. Run the following Python code to create the Data Asset: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sqlite_data.py query_table_asset\"   Add additional tables or queries (Optional) Repeat the previous steps to add additional Data Assets.   Snowflake Connect GX to a Snowflake database to access source data. Prerequisites   An installation of GX set up to work with SQL Source data stored in a Snowflake database    Import GX and instantiate a Data Context  Determine your connection string The following code examples use a Snowflake connection string. A Snowflake connection string connects GX to the Snowflake database. The following code is an example of a Snowflake connection string format: python  my_connection_string = \"snowflake://<USER_NAME>:<PASSWORD>@<ACCOUNT_NAME_OR_LOCATOR>/<DATABASE_NAME>/<SCHEMA_NAME>?warehouse=<WAREHOUSE_NAME>&role=<ROLE_NAME>\" :::info Account Names and Locators Snowflake accepts both account names and account locators as valid account identifiers when constructing a connection string.  Account names uniquely identify an account within your organization and are the preferred method of account identification. Account locators act in the same manner but are auto-generated by Snowflake based on the cloud platform and region used. For more information on both methods, please visit Snowflake's official documentation on account identifiers ::: Create a Snowflake Data Source   Run the following Python code to set the name and connection_string variables: python datasource_name = \"my_snowflake_datasource\"   Run the following Python code to create a Snowflake Data Source: python  datasource = context.sources.add_snowflake(     name=datasource_name,      connection_string=my_connection_string, # Or alternatively, individual connection args )   :::info Passing individual connection arguments instead of connection_string Although a connection string is the standard way to yield a connection to a database, the Snowflake datasource supports  individual connection arguments to be passed in as an alternative. The following arguments are supported:   - account   - user   - password   - database   - schema   - warehouse   - role   - numpy Passing these values as keyword args to add_snowflake is functionally equivalent to passing in a connection_string. For more information, check out Snowflake's official documentation on the Snowflake SQLAlchemy toolkit. ::: Connect to the data in a table (Optional)   Run the following Python code to set the asset_name and asset_table_name variables: python asset_name = \"my_asset\" asset_table_name = my_table_name   Run the following Python code to create the Data Asset: python table_asset = datasource.add_table_asset(name=asset_name, table_name=asset_table_name)   Connect to the data in a query (Optional)   Run the following Python code to define a Query Data Asset: python asset_name = \"my_query_asset\" query = \"SELECT * from yellow_tripdata_sample_2019_01\" 2. Run the following Python code to create the Data Asset: python query_asset = datasource.add_query_asset(name=asset_name, query=query)   Add additional tables or queries (Optional) Repeat the previous steps to add additional Data Assets.   Databricks SQL Connect GX to Databricks to access source data. Prerequisites   An installation of GX set up to work with SQL Source data stored in a Databricks cluster    Import GX and instantiate a Data Context  Determine your connection string The following code examples use a Databricks SQL connection string. A connection string connects GX to Databricks. The following code is an example of a Databricks SQL connection string format: python my_connection_string = f\"databricks://token:{token}@{host}:{port}/{database}?http_path={http_path}&catalog={catalog}&schema={schema}\" Create a Databricks SQL Data Source   Run the following Python code to set the name and connection_string variables: python datasource_name = \"my_databricks_sql_datasource\"   Run the following Python code to create a Snowflake Data Source: python  datasource = context.sources.add_databricks_sql(     name=datasource_name,      connection_string=my_connection_string, )   Connect to the data in a table (Optional)   Run the following Python code to set the asset_name and asset_table_name variables: python asset_name = \"my_asset\" asset_table_name = my_table_name   Run the following Python code to create the Data Asset: python table_asset = datasource.add_table_asset(name=asset_name, table_name=asset_table_name)   Connect to the data in a query (Optional)   Run the following Python code to define a Query Data Asset: python asset_name = \"my_query_asset\" query = \"SELECT * from yellow_tripdata_sample_2019_01\" 2. Run the following Python code to create the Data Asset: python query_asset = datasource.add_query_asset(name=asset_name, query=query)   Add additional tables or queries (Optional) Repeat the previous steps to add additional Data Assets.   Related documentation   How to organize Batches in a SQL based Data Asset   How to request data from a Data Asset   Use a Data Asset to create Expectations while interactively evaluating a set of data   Use the Onboarding Data Assistant to evaluate one or more Batches of data and create Expectations  ", "How to connect to a SQL table": " title: How to connect to a SQL table tag: [how-to, connect to data] description: A technical guide demonstrating how to connect Great Expectations to a SQL table. keywords: [Great Expectations, SQL]   import Prerequisites from '/docs/components/_prerequisites.jsx'  import ImportGxAndInstantiateADataContext from '/docs/components/setup/data_context/_import_gx_and_instantiate_a_data_context.md' import SetupAndInstallForSqlData from '/docs/components/setup/link_lists/_setup_and_install_for_sql_data.md' import ConnectingToSqlDatasourcesFluently from '/docs/components/connect_to_data/link_lists/_connecting_to_sql_datasources_fluently.md' In this guide we will demonstrate how to connect Great Expectations to a generic SQL table.  GX uses SQLAlchemy to connect to SQL data, and therefore supports most SQL dialects that SQLAlchemy does.  For more information on the SQL dialects supported by SQLAlchemy, please see SQLAlchemy's official documentation on dialects. If you would like to connect to the results of a SQL query instead of the contents of a SQL table, please see our guide on how to connect to SQL data using a query, instead. Prerequisites   An installation of GX set up to work with SQL Source data stored in a SQL database A SQL-based Data Source       ### If you still need to set up and install GX...    Please reference the appropriate one of these guides:       ### If you still need to connect a Data Source to a SQL database...    Please reference the appropriate one of these guides:    :::caution Datasources defined with the block-config method If you're using a Data Source that was created with the advanced block-config method, see How to configure a SQL Data Source with the block-config method. ::: Steps 1. Import GX and instantiate a Data Context  2. Retrieve a SQL Data Source For this guide, we will use a previously defined Data Source named \"my_datasource\" which connects to a SQL database. To retrieve this Data Source, we will supply the get_datasource(...) method of our Data Context with the name of the Data Source we wish to retrieve: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_a_sql_table.py datasource 3. Add a table to the Data Source as a Data Asset We will indicate a table to connect to by defining a Data Asset.  This is as simple as providing the add_table_asset(...) method a name by which we will reference the Data Asset in the future and a table_name to specify the table we wish the Data Asset to connect to. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_a_sql_table.py create_datasource 4. (Optional) Repeat step 3 as needed to add additional tables If you wish to connect to additional tables in the same SQL Database, simply repeat the step above to add them as table Data Assets. Next steps Now that you have connected to a SQL table, you may want to look into: Configuring SQL Data Assets further  How to organize Batches in a SQL based Data Asset  Requesting Data from a Data Asset  How to request data from a Data Asset  Using Data Assets to create Expectations  Use a Data Asset to create Expectations while interactively evaluating a set of data Use the Onboarding Data Assistant to evaluate one or more Batches of data and create Expectations ", "How to connect to a PostgreSQL database": " title: How to connect to a PostgreSQL database tag: [how-to, connect to data] description: A technical guide demonstrating how to connect Great Expectations to data in a PostgreSQL database. keywords: [Great Expectations, Postgres, PostgreSQL, SQL]   import Prerequisites from '/docs/components/_prerequisites.jsx'  import ImportGxAndInstantiateADataContext from '/docs/components/setup/data_context/_import_gx_and_instantiate_a_data_context.md' import PostgreSqlConfigureCredentialsInConfigVariablesYml from '/docs/components/setup/dependencies/_postgresql_configure_credentials_in_config_variables_yml.md' In this guide we will demonstrate how to connect Great Expectations to data in a PostgreSQL database.  We will demonstrate how to create a PostgreSQL Data Source.  With our PostgreSQL Data Source we will then show the methods for connecting to data in a PostgreSQL table and connecting to data from a PostgreSQL query. Prerequisites   An installation of GX set up to work with PostgreSQL Source data stored in a PostgreSQL database    Steps 1. Import GX and instantiate a Data Context  2. Determine your connection string For this example we will use a connection string to connect to our PostgreSQL database.  In PostgreSQL, connection strings are formatted like: pythonname=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_postgresql_data.py connection_string\" :::tip Is there a more secure way to store my credentials than plain text in a connection string?  ::: 3. Create a PostgreSQL Data Source Creating a PostgreSQL Data Source is as simple as providing the add_postgres(...) method a name by which to reference it in the future and the connection_string with which to access it. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_postgresql_data.py connection_string2\" With these two values, we can create our Data Source: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_postgresql_data.py add_postgres\" 4. Connect to a specific set of data with a Data Asset Now that our Data Source has been created, we will use it to connect to a specific set of data in the database it is configured for.  This is done by defining a Data Asset in the Data Source.  A Data Source may contain multiple Data Assets, each of which will serve as the interface between GX and the specific set of data it has been configured for. With SQL databases, there are two types of Data Assets that can be used.  The first is a Table Data Asset, which connects GX to the data contained in a single table in the source database.  The other is a Query Data Asset, which connects GX to the data returned by a SQL query.  We will demonstrate how to create both of these in the following steps.   :::tip How many Data Assets can my Data Source contain? Although there is no set maximum number of Data Assets you can define for a datasource, there is a functional minimum.  In order for GX to retrieve data from your Data Source you will need to create at least one Data Asset. ::: 5. (Optional) Connect a Data Asset to the data in a table We will indicate a table to connect to with a Table Data Asset.  This is done by providing the add_table_asset(...) method a name by which we will reference the Data Asset in the future and a table_name to specify the table we wish the Data Asset to connect to. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_postgresql_data.py asset_name\" With these two values, we can create our Data Asset: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_postgresql_data.py add_table_asset\" 6. (Optional) Connect a Data Asset to the data returned by a query To indicate the query that provides data to connect to we will define a Query Data Asset.  This done by providing the add_query_asset(...) method a name by which we will reference the Data Asset in the future and a query which will provide the data we wish the Data Asset to connect to. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_postgresql_data.py asset_query\" Once we have these two values, we can create our Data Asset with: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_postgresql_data.py add_query_asset\" 7. (Optional) Repeat steps 5 and 6 as needed to connect to additional tables or queries If you wish to connect to additional tables or queries in the same PostgreSQL Database, simply repeat the step above to add them as additional Data Assets. Next steps Now that you have connected to a PostgreSQL database and created a Data Asset, you may want to look into: Configuring SQL Data Assets further  How to organize Batches in a SQL based Data Asset  Requesting Data from a Data Asset  How to request data from a Data Asset  Using Data Assets to create Expectations  Use a Data Asset to create Expectations while interactively evaluating a set of data Use the Onboarding Data Assistant to evaluate one or more Batches of data and create Expectations ", "How to connect to a SQL database": " title: How to connect to a SQL database tag: [how-to, connect to data] description: A technical guide demonstrating how to connect Great Expectations to a SQL database. keywords: [Great Expectations, SQL]   import Prerequisites from '/docs/components/_prerequisites.jsx'  import ImportGxAndInstantiateADataContext from '/docs/components/setup/data_context/_import_gx_and_instantiate_a_data_context.md'  import AfterCreateSqlDatasource from '/docs/components/connect_to_data/next_steps/_after_create_sql_datasource.md' In this guide we will demonstrate how to connect Great Expectations to SQL databases.  GX uses SQLAlchemy to connect to SQL data, and therefore supports most SQL dialects that SQLAlchemy does.  For more information on the SQL dialects supported by SQLAlchemy, please see SQLAlchemy's official documentation on dialects. Prerequisites   An installation of GX set up to work with SQL Source data stored in a SQL database    Steps 1. Import GX and instantiate a Data Context  2. Determine your connection string GX supports a variety of different SQL source data systems.  However, most SQL dialects have their own specifications for how to define a connection string.  You should reference the corresponding dialect's official documentation to determine the connection string for your SQL Database. :::info Some examples of different connection strings: The following are examples of connection strings for different SQL dialects:  AWS Athena: awsathena+rest://@athena.<REGION>.amazonaws.com/<DATABASE>?s3_staging_dir=<S3_PATH> BigQuery: bigquery://<GCP_PROJECT>/<BIGQUERY_DATASET>?credentials_path=/path/to/your/credentials.json MSSQL: mssql+pyodbc://<USERNAME>:<PASSWORD>@<HOST>:<PORT>/<DATABASE>?driver=<DRIVER>&charset=utf&autocommit=true MySQL: mysql+pymysql://<USERNAME>:<PASSWORD>@<HOST>:<PORT>/<DATABASE> PostGreSQL: postgresql+psycopg2://<USERNAME>:<PASSWORD>@<HOST>:<PORT>/<DATABASE> Redshift: postgresql+psycopg2://<USER_NAME>:<PASSWORD>@<HOST>:<PORT>/<DATABASE>?sslmode=<SSLMODE> Snowflake]: snowflake://<USER_NAME>:<PASSWORD>@<ACCOUNT_NAME>/<DATABASE_NAME>/<SCHEMA_NAME>?warehouse=<WAREHOUSE_NAME>&role=<ROLE_NAME>&application=great_expectations_oss SQLite: sqlite:///<PATH_TO_DB_FILE> Trino: trino://<USERNAME>:<PASSWORD>@<HOST>:<PORT>/<CATALOG>/<SCHEMA>  ::: For purposes of this guide's examples, we will connect to a PostGreSQL database.  Here is an example of our connection string, stored in the variable connection_string with plain text credentials: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sql_data.py sql_connection_string\" :::tip Is there a more secure way to include my credentials? You can use either environment variables or a key in config_variables.yml to safely store any passwords needed by your connection string.  After defining your password in one of those ways, you can reference it in your connection string like this: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sql_data.py connection_string\" In the above example MY_PASSWORD would be the name of the environment variable or the key to the value in config_variables.yml that corresponds to your password. If you include a password as plain text in your connection string when you define your Data Source, GX will automatically strip it out, add it to config_variables.yml and substitute it in the Data Source's saved configuration with a variable as was shown above. ::: 3. Create a SQL Data Source Creating a SQL Data Source is as simple as providing the add_sql(...) method a name by which to reference it in the future and the connection_string with which to access it. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sql_data.py add_sql\" Next steps Now that you have connected to a SQL database, next you will want to: ", "How to connect to SQL data using a query": " title: How to connect to SQL data using a query tag: [how-to, connect to data] description: A technical guide demonstrating how to connect Great Expectations to the data returned by a SQL query. keywords: [Great Expectations, SQL]   import Prerequisites from '/docs/components/_prerequisites.jsx'  import ImportGxAndInstantiateADataContext from '/docs/components/setup/data_context/_import_gx_and_instantiate_a_data_context.md' import SetupAndInstallForSqlData from '/docs/components/setup/link_lists/_setup_and_install_for_sql_data.md' import ConnectingToSqlDatasourcesFluently from '/docs/components/connect_to_data/link_lists/_connecting_to_sql_datasources_fluently.md' In this guide we will demonstrate how to connect Great Expectations to the data returned by a query in a generic SQL database.  GX uses SQLAlchemy to connect to SQL data, and therefore supports most SQL dialects that SQLAlchemy does.  For more information on the SQL dialects supported by SQLAlchemy, see Dialects. To connect to the contents of a SQL table instead of the results of a SQL query, see our guide on how to connect to a SQL table, instead. Prerequisites   An installation of GX set up to work with SQL. See How to set up GX to work with SQL databases. Source data stored in a SQL database.    If you still need to connect a Data Source to a SQL database  Please reference the appropriate one of these guides:   :::caution Datasources defined with the block-config method If you're using a Data Source that was created with the advanced block-config method, see How to configure a SQL Data Source with the block-config method, instead. ::: Steps 1. Import GX and instantiate a Data Context  2. Retrieve a SQL Data Source For this guide, we will use a previously defined Data Source named \"my_datasource\".  For purposes of our demonstration, this Data Source was configured to connect to a SQL database. To retrieve this Data Source, we will supply the get_datasource(...) method of our Data Context with the name of the Data Source we wish to retrieve: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sql_data_using_a_query.py datasource\" 3. Add a query to the Data Source as a Data Asset To indicate the query that provides the data to connect to we will define a Data Asset.  This is done by providing the add_query_asset(...) method a name by which we will reference the Data Asset in the future and a query which will provide the data we wish the Data Asset to connect to. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sql_data_using_a_query.py add_query_asset\" 4. (Optional) Repeat step 3 as needed to add additional queries If you wish to connect to the contents of additional queries in the same SQL Database, simply repeat the steps above to add them as query Data Assets. Next steps Now that you have connected to the data returned by a SQL query, you may want to look into: Configuring SQL Data Assets further  How to organize Batches in a SQL based Data Asset  Requesting Data from a Data Asset  How to request data from a Data Asset  Using Data Assets to create Expectations  Use a Data Asset to create Expectations while interactively evaluating a set of data Use the Onboarding Data Assistant to evaluate one or more Batches of data and create Expectations ", "How to connect to SQLite data": " title: How to connect to SQLite data tag: [how-to, connect to data] description: A technical guide demonstrating how to connect Great Expectations to data in a SQLite database. keywords: [Great Expectations, SQLite, SQL]   import Prerequisites from '/docs/components/_prerequisites.jsx'  import ImportGxAndInstantiateADataContext from '/docs/components/setup/data_context/_import_gx_and_instantiate_a_data_context.md' In this guide we will demonstrate how to connect Great Expectations to data in a SQLite database.  We will demonstrate how to create a SQLite Data Source.  With our SQLite Data Source we will then show the methods for connecting to data in a SQLite table and connecting to data from a SQLite query. Prerequisites   An installation of GX set up to work with SQL Source data stored in a SQLite database    Steps 1. Import GX and instantiate a Data Context  2. Determine your connection string For this example we will use a connection string to connect to our PostgreSQL database.  In PostgreSQL, connection strings are formatted like: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sqlite_data.py connection_string\" 3. Create a SQLite Data Source Creating a PostgreSQL Data Source is as simple as providing the add_sqlite(...) method a name by which to reference it in the future and the connection_string with which to access it. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sqlite_data.py datasource_name\" With these two values, we can create our Data Source: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sqlite_data.py datasource\" :::caution Using add_sql(...) vs add_sqlite(...) to create a Data Source The basic SQL Data Source created with add_sql can connect to data in a SQLite database, but it won't work as well as a SQLite Data Source from add_sqlite(...). SQLite stores datetime values as strings.  Because of this, a general SQL Data Source will see datetime columns as string columns, instead.  The SQLite Data Source has additional handling in place for these fields, and also has additional error reporting for SQLite specific issues. If you are working with SQLite data, you should always use add_sqlite(...) to create your Data Source!  The add_sql(...) method may connect to your SQLite database, but it won't handle datetime columns properly or report errors as clearly. ::: 4. (Optional) Connect to the data in a table We will indicate a table to connect to with a Table Data Asset.  This is done by providing the add_table_asset(...) method a name by which we will reference the Data Asset in the future and a table_name to specify the table we wish the Data Asset to connect to. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sqlite_data.py asset_name\" With these two values, we can create our Data Asset: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sqlite_data.py table_asset\" 5. (Optional) Connect to the data in a query To indicate the query that provides data to connect to we will define a Query Data Asset.  This done by providing the add_query_asset(...) method a name by which we will reference the Data Asset in the future and a query which will provide the data we wish the Data Asset to connect to. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sqlite_data.py asset_query\" Once we have these two values, we can create our Data Asset with: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sqlite_data.py query_table_asset\" 6. (Optional) Repeat steps 4 or 5 as needed to add additional tables or queries If you wish to connect to additional tables or queries in the same PostgreSQL Database, simply repeat the steps above to add them as table Data Assets. Next steps Now that you have connected to a SQLite Database and created a Data Asset, you may want to look into: Configuring SQL Data Assets further  How to organize Batches in a SQL based Data Asset  Requesting Data from a Data Asset  How to request data from a Data Asset  Using Data Assets to create Expectations  Use a Data Asset to create Expectations while interactively evaluating a set of data Use the Onboarding Data Assistant to evaluate one or more Batches of data and create Expectations ", "\"Manage SQL Data Assets\"": " sidebar_label: \"Manage SQL Data Assets\" title: \"Manage SQL Data Assets\" id: sql_data_assets description: Connect Great Expectations to SQL Data Assets. toc_min_heading_level: 2 toc_max_heading_level: 2 keywords: [Great Expectations, Data Asset, Batch Request, fluent configuration method, SQL]  import Prerequisites from '/docs/components/_prerequisites.jsx' import ImportGxAndInstantiateADataContext from '/docs/components/setup/data_context/_import_gx_and_instantiate_a_data_context.md' import SetupAndInstallForSqlData from '/docs/components/setup/link_lists/_setup_and_install_for_sql_data.md' import ConnectingToSqlDatasourcesFluently from '/docs/components/connect_to_data/link_lists/_connecting_to_sql_datasources_fluently.md' import AfterRequestDataFromADataAsset from '/docs/components/connect_to_data/next_steps/_after_request_data_from_a_data_asset.md' import AfterCreateAndConfigureDataAsset from '/docs/components/connect_to_data/next_steps/_after_create_and_configure_data_asset.md' import TechnicalTag from '/docs/term_tags/_tag.mdx'; import TabItem from '@theme/TabItem'; import Tabs from '@theme/Tabs'; A Data Asset is a collection of records within a Data Source that define how Great Expectations (GX) organizes data into Batches. Use the information provided here to connect GX to SQL tables and data returned by SQL database queries and learn how to organize Batches in a SQL Data Asset. Great Expectations (GX) uses SQLAlchemy to connect to SQL source data, and most of the SQL dialects supported by SQLAlchemy are also supported by GX. For more information about the SQL dialects supported by SQLAlchemy, see Dialects. :::caution Datasources defined with the block-config method If you're using a Data Source created with the block-config method, see How to configure a SQL Data Source with the block-config method. :::   Connect to a SQL table Connect GX to a SQL table to access source data. The following code examples use a previously defined Data Source named \"my_datasource\" to connect to a SQL database. Prerequisites   An installation of GX set up to work with SQL Source data stored in a SQL database A SQL-based Data Source   Import GX and instantiate a Data Context  Retrieve a SQL Data Source Run the following Python code to retrieve the Data Source: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_a_sql_table.py datasource Add a table to the Data Source as a Data Asset You create a Data Asset to identify the table to connect to.  Run the following Python code to define the name and table_name variables: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_a_sql_table.py create_datasource Add additional tables (Optional) To connect to additional tables in the same SQL Database, repeat the previous steps to add them as table Data Assets. Related documentation  How to organize Batches in a SQL based Data Asset How to request data from a Data Asset Use a Data Asset to create Expectations while interactively evaluating a set of data Use the Onboarding Data Assistant to evaluate one or more Batches of data and create Expectations    Connect to SQL data using a query Connect GX to the data returned by a query in a SQL database. The following code examples use a previously defined Data Source named \"my_datasource\" to connect to a SQL database. Prerequisites   An installation of GX set up to work with SQL. See How to set up GX to work with SQL databases. Source data stored in a SQL database.    Import GX and instantiate a Data Context  Retrieve a SQL Data Source Run the following Python code to retrieve the Data Source: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sql_data_using_a_query.py datasource\" Add a query to the Data Source as a Data Asset Run the following Python code to define a Data Asset and the name and query variables: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_sql_data_using_a_query.py add_query_asset\" Add additional queries (Optional) To connect to the contents of additional queries in the same SQL Database, repeat the previous steps to add them as query Data Assets. Related documentation  How to organize Batches in a SQL based Data Asset How to request data from a Data Asset Use a Data Asset to create Expectations while interactively evaluating a set of data Use the Onboarding Data Assistant to evaluate one or more Batches of data and create Expectations    Organize Batches Organize Batches in a SQL-based Data Asset. This includes using Splitters to divide the data in a table or query based on the contents of a provided field and adding Batch Sorters to a Data Asset to specify the order in which Batches are returned. The following code examples use a previously defined Data Source named \"my_datasource\" to connect to a SQL database. Prerequisites   A working installation of Great Expectations A Data Asset in a SQL-based Data Source   Import GX and instantiate a Data Context  Retrieve a SQL Data Source and Data Asset Run the following Python code to retrieve the Data Source: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/organize_batches_in_sqlite_datasource.py my_datasource\" Add a Splitter to the Data Asset Run the following Python code to split the TableAsset into Batches: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/organize_batches_in_sqlite_datasource.py add_splitter_year_and_month\" Add Batch Sorters to the Data Asset (Optional) Adding Batch Sorters to your Data Asset lets you explicitly state the order in which your Batches are returned when you request data from the Data Asset. To add Batch Sorters, pass a list of sorters to the add_sorters(...) method of your Data Asset. Run the following Python code to split the \"pickup_datetime\" column on \"year\" and \"month\", so your list of sorters can have up to two elements. The code also adds an ascending sorter based on the contents of the splitter group \"year\" and a descending sorter based on the contents of the splitter group \"month\": python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/organize_batches_in_sqlite_datasource.py add_sorters\" Use a Batch Request to verify Data Asset functionality Run the following Python code to verify that your Data Asset returns the necessary files as Batches: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/organize_batches_in_sqlite_datasource.py my_batch_list\" A Batch List can contain a lot of metadata. To verify which files were included in the returned Batches, run the following Python code to review the batch_spec for each returned Batch: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/organize_batches_in_sqlite_datasource.py print_batch_spec\" Related documentation  How to request data from a Data Asset Use a Data Asset to create Expectations while interactively evaluating a set of data Use the Onboarding Data Assistant to evaluate one or more Batches of data and create Expectations    Related documentation  How to set up GX to work with SQL databases How to connect to SQL data How to connect to PostgreSQL data How to connect to SQLite data ", "\"Connect to filesystem source data\"": " sidebar_label: \"Connect to filesystem source data\" title: \"Connect to filesystem source data\" id: connect_filesystem_source_data description: Connect to source data stored in filesystem files. toc_min_heading_level: 2 toc_max_heading_level: 2  import TechnicalTag from '@site/docs/term_tags/_tag.mdx'; import Prerequisites from '/docs/components/_prerequisites.jsx' import Introduction from '/docs/components/connect_to_data/filesystem/_intro_connect_to_one_or_more_files_pandas_or_spark.mdx' import ImportGxAndInstantiateADataContext from '/docs/components/setup/data_context/_import_gx_and_instantiate_a_data_context.md' import InfoUsingPandasToConnectToDifferentFileTypes from '/docs/components/connect_to_data/filesystem/_info_using_pandas_to_connect_to_different_file_types.mdx' import AfterCreateValidator from '/docs/components/connect_to_data/next_steps/_after_create_validator.md' import InfoFilesystemDatasourceRelativeBasePaths from '/docs/components/connect_to_data/filesystem/_info_filesystem_datasource_relative_base_paths.md' import TipFilesystemDatasourceNestedSourceDataFolders from '/docs/components/connect_to_data/filesystem/_tip_filesystem_datasource_nested_source_data_folders.md' import TipFilesystemDataAssetWhatIfBatchingRegexMatchesMultipleFiles from '/docs/components/connect_to_data/filesystem/_tip_filesystem_data_asset_if_batching_regex_matches_multiple_files.md' import TipUsingPandasToConnectToDifferentFileTypes from '/docs/components/connect_to_data/filesystem/_info_using_pandas_to_connect_to_different_file_types.mdx' import DefiningMultipleDataAssets from '/docs/components/connect_to_data/filesystem/_defining_multiple_data_assets.md' import AfterCreateNonSqlDatasource from '/docs/components/connect_to_data/next_steps/_after_create_non_sql_datasource.md' import BatchingRegexExplaination from '/docs/components/connect_to_data/cloud/_batching_regex_explaination.mdx' import PrereqInstallGxWithDependencies from '/docs/components/prerequisites/_gx_installed_with_abs_dependencies.md' import AbsFluentAddDataAssetConfigKeys from '/docs/components/connect_to_data/cloud/_abs_fluent_data_asset_config_keys.mdx' import TabItem from '@theme/TabItem'; import Tabs from '@theme/Tabs'; Use the information provided here to connect to source data stored on Amazon S3, Google Cloud Storage (GCS), Microsoft Azure Blob Storage, or local filesystems. Great Expectations (GX) uses the term source data when referring to data in its original format, and the term source data system when referring to the storage location for source data.   Amazon S3 source data Connect to source data on Amazon S3.   The following examples connect to .csv data. However, GX supports most of the Pandas read methods. Prerequisites   An installation of GX set up to work with S3 Access to data on a S3 bucket    Import GX and instantiate a Data Context  Create a Data Source The following information is required when you create an Amazon S3 Data Source:   name: The Data Source name. In the following examples, this is \"my_s3_datasource\"   bucket_name: The Amazon S3 bucket name.   boto3_options: Optional. Additional options for the Data Source. In the following examples, the default values are used.   Run the following Python code to define name, bucket_name and boto3_options: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_s3_using_pandas.py define_add_pandas_s3_args\" :::tip Additional options for boto3_options The parameter boto3_options allows you to pass the following information: - endpoint_url: specifies an S3 endpoint.  You can use an environment variable such as \"${S3_ENDPOINT}\" to securely include this in your code.  The string \"${S3_ENDPOINT}\" will be replaced with the value of the corresponding environment variable. - region_name: Your AWS region name. :::   Run the following Python code to pass name, bucket_name, and boto3_options as parameters when you create your Data Source:: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_s3_using_pandas.py create_datasource\"   Add data to the Data Source as a Data Asset Run the following Python code: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_s3_using_pandas.py add_asset\"  Next steps    The following examples connect to .csv data. Prerequisites   An installation of GX set up to work with S3 Access to data on a S3 bucket    Import GX and instantiate a Data Context  Create a Data Source The following information is required when you create an Amazon S3 Data Source:   name: The Data Source name. In the following examples, this is \"my_s3_datasource\"   bucket_name: The Amazon S3 bucket name.   boto3_options: Optional. Additional options for the Data Source. In the following examples, the default values are used.   Run the following Python code to define name, bucket_name, and boto3_options: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_s3_using_spark.py define_add_spark_s3_args\" :::tip Additional options for boto3_options The parameter boto3_options allows you to pass the following information: - endpoint_url: Specifies an S3 endpoint.  You can use an environment variable such as \"${S3_ENDPOINT}\" to securely include this in your code.  The string \"${S3_ENDPOINT}\" will be replaced with the value of the corresponding environment variable. - region_name: Your AWS region name. :::   Run the following Python code to pass name, bucket_name, and boto3_options as parameters when you create your Data Source:: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_s3_using_spark.py create_datasource\"   Add data to the Data Source as a Data Asset Run the following Python code: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_s3_using_spark.py add_asset\"  Next steps      Microsoft Azure Blob Storage Connect to source data on Microsoft Azure Blob Storage.    Prerequisites    Access to data in Azure Blob Storage    Import GX and instantiate a Data Context  Create a Data Source The following information is required when you create a Microsoft Azure Blob Storage Data Source:   name: The Data Source name. In the following examples, this is \"my_datasource\".   azure_options: Authentication settings.   Run the following Python code to define name and azure_options: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_azure_blob_storage_using_pandas.py define_add_pandas_abs_args\" 2. Run the following Python code to pass name and azure_options as parameters when you create your Data Source: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_azure_blob_storage_using_pandas.py create_datasource\" :::tip Where did that connection string come from? In the previous example, the value for account_url is substituted for the contents of the AZURE_STORAGE_CONNECTION_STRING key you configured when you installed GX and set up your Azure Blob Storage dependencies. :::   Add data to the Data Source as a Data Asset    Run the following Python code to create the Data Asset: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_azure_blob_storage_using_pandas.py add_asset\"    Next steps     Prerequisites    Access to data in Azure Blob Storage    Import GX and instantiate a Data Context  Create a Data Source The following information is required when you create a Microsoft Azure Blob Storage Data Source:   name: The Data Source name. In the following examples, this is \"my_datasource\".   azure_options: Authentication settings.   Run the following Python code to define name and azure_options: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_azure_blob_storage_using_spark.py define_add_spark_abs_args\" 2. Run the following Python code to pass name and azure_options as parameters when you create your Data Source: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_azure_blob_storage_using_spark.py create_datasource\" :::tip Where did that connection string come from? In the previous example, the value for account_url is substituted for the contents of the AZURE_STORAGE_CONNECTION_STRING key you configured when you installed GX and set up your Azure Blob Storage dependencies. :::   Add data to the Data Source as a Data Asset    Run the following Python code to create the Data Asset: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_azure_blob_storage_using_spark.py add_asset\"    Next steps      GCS source data Connect to source data on GCS.   The following examples connect to .csv data. However, GX supports most of the Pandas read methods. Prerequisites   An installation of GX set up to work with GCS Access to data in a GCS bucket    Import GX and instantiate a Data Context  Create a Data Source The following information is required when you create a GCS Data Source:   name: The Data Source name. In the following examples, this is \"my_gcs_datasource\".   bucket_or_name: The GCS bucket or instance name.   gcs_options: Optional. Additional options for the Data Source. In the following examples, the default values are used.   Run the following Python code to define name, bucket_or_name, and gcs_options: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_gcs_using_pandas.py define_add_pandas_gcs_args\"   Run the following Python code to pass name, bucket_or_name, and gcs_options as parameters when you create your Data Source: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_gcs_using_pandas.py create_datasource\"   Add GCS data to the Data Source as a Data Asset Run the following Python code: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_gcs_using_pandas.py add_asset\"  Next steps  Related documentation For more information on Google Cloud and authentication, see the following:  gcloud CLI Tutorial GCS Python API Docs    Use Spark to connect to source data stored on GCS.  The following examples connect to .csv data. Prerequisites   An installation of GX set up to work with GCS Access to data on a GCS bucket    1. Import GX and instantiate a Data Context  Create a Data Source The following information is required when you create a GCS Data Source:   name: The Data Source name. In the following examples, this is \"my_gcs_datasource\".   bucket_or_name: The GCS bucket or instance name.   gcs_options: Optional. Additional options for the Data Source. In the following examples, the default values are used.   Run the following Python code to define name, bucket_or_name, and gcs_options: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_gcs_using_spark.py define_add_spark_gcs_args\"   Run the following Python code to pass name, bucket_or_name, and gcs_options as parameters when you create your Data Source: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_gcs_using_spark.py create_datasource\"   Add GCS data to the Data Source as a Data Asset Run the following Python code: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_data_on_gcs_using_spark.py add_asset\" :::info Optional parameters: header and infer_schema In the previous example there are two optional parameters.  If the file does not have a header line, the header parameter can be left out as it will default to false.  If you do not want GX to infer the schema of your file, you can exclude the infer_schema parameter as it also defaults to false. :::  Next steps  Related documentation For more information on Google Cloud and authentication, see the following:  gcloud CLI Tutorial GCS Python API Docs      Filesystem source data Connect to source data on a filesystem.    Prerequisites   Access to source data stored in a filesystem    Import the GX module and instantiate a Data Context  Specify a file to read into a Data Asset Run the following Python code to read the data in individual files directly into a Validator with Pandas: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_quickly_connect_to_a_single_file_with_pandas.py get_validator\"  Create Data Source (Optional) Modify the following code to connect to your Data Source. If you don't have data available for testing, you can use the NYC taxi data. The NYC taxi data is open source, and it is updated every month. An individual record in the data corresponds to one taxi trip. :::caution Do not include sensitive information such as credentials in the configuration when you connect to your Data Source. This information appears as plain text in the database. If you must include credentials or a full connection string, GX recommends using a config variables file. ::: ```python title=\"Jupyter Notebook\" Give your Datasource a name datasource_name = None datasource = context.sources.add_pandas(datasource_name) Give your first Asset a name asset_name = None path_to_data = None to use sample data uncomment next line path_to_data = \"https://raw.githubusercontent.com/great-expectations/gx_tutorials/main/data/yellow_tripdata_sample_2019-01.csv\" asset = datasource.add_csv_asset(asset_name, filepath_or_buffer=path_to_data) Build batch request batch_request = asset.build_batch_request() ``` Next steps     Prerequisites   Access to source data stored in a filesystem    Import the GX module and instantiate a Data Context  Create a Data Source The following information is required when you create a Filesystem Data Source:   name: The Data Source name.   base_directory: The path to the folder containing the files the Data Source connects to.   Run the following Python code to define name and base_directory and store the information in the Python variables datasource_name and path_to_folder_containing_csv_files: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_one_or_more_files_using_pandas.py define_add_pandas_filesystem_args\"      Run the following Python code to pass name and base_directory as parameters when you create your Data Source: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_one_or_more_files_using_pandas.py create_datasource\"    Add a Data Asset to the Data Source A Data Asset requires the following information to be defined:   name: The Data Asset name. Helpful when you define multiple Data Assets in the same Data Source.   batching_regex: A regular expression that matches the files to be included in the Data Asset.      Run the following Python code to define name and batching_regex and store the information in the Python variables asset_name and batching_regex: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_one_or_more_files_using_pandas.py define_add_csv_asset_args\"   Run the following Python code to pass name and batching_regex as parameters when you create your Data Asset: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_one_or_more_files_using_pandas.py add_asset\"    Add additional files as Data Assets (Optional)  Next steps  Related documentation For more information on Pandas read_* methods, see the Pandas Input/output documentation.    Prerequisites   Access to source data stored in a filesystem    Import the GX module and instantiate a Data Context  Create a Data Source The following information is required when you create a Filesystem Data Source:   name: The Data Source name.   base_directory: The path to the folder containing the files the Data Source connects to.   Run the following Python code to define name and base_directory and store the information in the Python variables datasource_name and path_to_folder_containing_csv_files: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_one_or_more_files_using_spark.py define_add_spark_filesystem_args\"    Run the following Python code to pass name and base_directory as parameters when you create your Data Source: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_one_or_more_files_using_spark.py create_datasource\"    Add a Data Asset to the Data Source A Data Asset requires the following information to be defined:   name: The Data Asset name. Helpful when you define multiple Data Assets in the same Data Source.   batching_regex: A regular expression that matches the files to be included in the Data Asset.      Run the following Python code to define name and batching_regex and store the information in the Python variables asset_name and batching_regex: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_one_or_more_files_using_spark.py define_add_csv_asset_args\" In addition, the argument header informs the Spark DataFrame reader that the files contain a header column, while the argument infer_schema instructs the Spark DataFrame reader to make a best effort to determine the schema of the columns automatically.   Run the following Python code to pass name and batching_regex and the optional header and infer_schema arguments as parameters when you create your Data Asset: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_one_or_more_files_using_spark.py add_asset\"   Add additional files as Data Assets (Optional)  Next steps      Related documentation For more information about storing credentials for use with GX, see How to configure credentials.", "\"Connect to in-memory source data\"": " sidebar_label: \"Connect to in-memory source data\" title: \"Connect to in-memory source data\" id: connect_in_memory_data description: Connect to source data stored in-memory. toc_min_heading_level: 2 toc_max_heading_level: 2  import Prerequisites from '/docs/components/_prerequisites.jsx' import ImportGxAndInstantiateADataContext from '/docs/components/setup/data_context/_import_gx_and_instantiate_a_data_context.md' import AfterCreateInMemoryDataAsset from '/docs/components/connect_to_data/next_steps/_after_create_in_memory_data_asset.md' import TabItem from '@theme/TabItem'; import Tabs from '@theme/Tabs'; Use the information provided here to connect to an in-memory pandas or Spark DataFrame. Great Expectations (GX) uses the term source data when referring to data in its original format, and the term source data system when referring to the storage location for source data.   pandas pandas can read many types of data into its DataFrame class, but the following examples use data originating in a parquet file. Prerequisites   Access to data that can be read into a Pandas DataFrame    Import the Great Expectations module and instantiate a Data Context  Create a Data Source Run the following Python code to create a Pandas Data Source: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_in_memory_data_using_pandas.py datasource\" Read your source data into a Pandas DataFrame In the following example, a parquet file is read into a Pandas DataFrame that will be used in subsequent code examples. Run the following Python code to create the Pandas DataFrame: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_in_memory_data_using_pandas.py dataframe\" Add a Data Asset to the Data Source The following information is required when you create a Pandas DataFrame Data Asset:   name: The Data Source name.   dataframe: The Pandas DataFrame containing the source data.   The DataFrame you created previously is the value you'll enter for dataframe parameter.     Run the following Python code to define the name parameter and store it as a Python variable: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_in_memory_data_using_pandas.py name\"   Run the following Python code to create the Data Asset: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_in_memory_data_using_pandas.py data_asset\" For dataframe Data Assets, the dataframe is always specified as the argument of one API method. For example: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_in_memory_data_using_pandas.py build_batch_request_with_dataframe\"   Next steps  Related documentation For more information on Pandas read methods, see the Pandas Input/Output documentation.   Spark Connect to in-memory source data using Spark.  Prerequisites   Access to data that can be read into a Spark An active Spark Context    Import the Great Expectations module and instantiate a Data Context  Create a Data Source Run the following Python code to create a Spark Data Source: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_in_memory_data_using_spark.py datasource\" Read your source data into a Spark DataFrame In the following example, you'll create a simple Spark DataFrame that is used in the following code examples. Run the following Python code to create the Spark DataFrame: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_in_memory_data_using_spark.py dataframe\" Add a Data Asset to the Datasource The following information is required when you create a Spark DataFrame Data Asset:   name: The Datasource name.   dataframe: The Spark DataFrame containing the source data.   The DataFrame you created previously is the value you'll enter for dataframe parameter.     Run the following Python code to define the name parameter and store it as a Python variable: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_in_memory_data_using_spark.py name\"   Run the following Python code to create the Data Asset: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_in_memory_data_using_spark.py data_asset\" For dataframe Data Assets, the dataframe is always specified as the argument of one API method. For example: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_connect_to_in_memory_data_using_spark.py build_batch_request_with_dataframe\"   Next steps  Related documentation For more information on Spark read methods, see the Spark Input/Output documentation.  ", "How to initialize a Filesystem Data Context in Python": " title: How to initialize a Filesystem Data Context in Python tag: [how-to, setup] keywords: [Great Expectations, Data Context, Filesystem]  import TechnicalTag from '/docs/term_tags/_tag.mdx'; import Prerequisites from '/docs/components/_prerequisites.jsx'  import GxImport from '/docs/components/setup/python_environment/_gx_import.md'  import DataContextVerifyContents from '/docs/components/setup/data_context/_data_context_verify_contents.md' A Data Context is required in almost all Python scripts utilizing GX, and when using the CLI. Use the information provided here to use Python code to initialize, instantiate, and verify the contents of a Filesystem Data Context. Prerequisites   Steps 1. Import Great Expectations  2. Determine the folder to initialize the Data Context in For purposes of this example, we will assume that we have an empty folder to initialize our Filesystem Data Context in: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_initialize_a_filesystem_data_context_in_python.py path_to_empty_folder\" 3. Create a GX context We will provide our empty folder's path to the GX library's FileDataContext.create(...) method as the project_root_dir parameter.  Because we are providing a path to an empty folder FileDataContext.create(...) will initialize a Filesystem Data Context at that location. For convenience, the FileDataContext.create(...) method will then instantiate and return the newly initialized Data Context, which we can keep in a Python variable. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_initialize_a_filesystem_data_context_in_python.py initialize_filesystem_data_context\" :::info What if the folder is not empty? If the project_root_dir provided to the FileDataContext.create(...) method points to a folder that does not already have a Data Context present, the FileDataContext.create(...) method will initialize a Filesystem Data Context at that location even if other files and folders are present.  This allows you to easily initialize a Filesystem Data Context in a folder that contains your source data or other project related contents. If a Data Context already exists at the provided project_root_dir, the FileDataContext.create(...) method will not re-initialize it.  Instead, FileDataContext.create(...) will simply instantiate and return the existing Data Context as is. ::: 4. Verify the content of the returned Data Context  Next steps For guidance on further customizing your Data Context's configurations for Metadata Stores and Data Docs, please see: - How to configure an Expectation Store on a filesystem - How to configure a Validation Result Store on a filesystem - How to configure and use a Metric Store - How to host and share Data Docs on a filesystem If you are content with the default configuration of your Data Context, you can move on to connecting GX to your source data: - How to configure a Pandas Data Source - How to configure a Spark Data Source - How to configure a SQL Data Source Additional information Related guides To initialize and instantiate a temporary Data Context, see: How to instantiate an Ephemeral Data Context.  ", "How to instantiate a specific Filesystem Data Context": " title: How to instantiate a specific Filesystem Data Context tag: [how-to, setup] keywords: [Great Expectations, Data Context, Filesystem]  import TechnicalTag from '/docs/term_tags/_tag.mdx'; import Prerequisites from '/docs/components/_prerequisites.jsx'  import GxImport from '/docs/components/setup/python_environment/_gx_import.md'  import DataContextVerifyContents from '/docs/components/setup/data_context/_data_context_verify_contents.md' A Data Context contains the configurations for Expectations, Metadata Stores, Data Docs, Checkpoints, and all things related to working with Great Expectations.   If you are using GX for multiple projects you may wish to utilize a different Data Context for each one.  This guide will demonstrate how to instantiate a specific Filesystem Data Context so that you can switch between sets of previously defined GX configurations. Prerequisites   A previously initialized Filesystem Data Context. See How to initialize a Filesystem Data Context in Python.   Steps 1. Import Great Expectations  2. Specify a folder containing a previously initialized Filesystem Data Context Each Filesystem Data Context has a root folder in which it was initialized.  This root folder will be used to indicate which specific Filesystem Data Context should be instantiated. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_instantiate_a_specific_filesystem_data_context.py path_to_context_root_folder\" 2. Run GX's get_context(...) method We provide our Filesystem Data Context's root folder path to the GX library's get_context(...) method as the context_root_dir parameter.  Because we are providing a path to an existing Data Context, the get_context(...) method will instantiate and return the Data Context at that location. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_instantiate_a_specific_filesystem_data_context.py get_filesystem_data_context\" :::info What if the folder does not contain a Data Context? If the context_root_dir provided to the get_context(...) method points to a folder that does not already have a Data Context present, the get_context(...) method will initialize a new Filesystem Data Context at that location. The get_context(...) method will then instantiate and return the newly initialized Data Context. ::: 3. Verify the content of the returned Data Context  Next steps For guidance on further customizing your Data Context's configurations for Metadata Stores and Data Docs, please see: - How to configure an Expectation Store on a filesystem - How to configure a Validation Result Store on a filesystem - How to configure and use a Metric Store - How to host and share Data Docs on a filesystem If you are content with the default configuration of your Data Context, you can move on to connecting GX to your source data: - How to configure a Pandas Data Source - How to configure a Spark Data Source - How to configure a SQL Data Source Additional information Related guides To initialize and instantiate a temporary Data Context, see: Instantiate an Ephemeral Data Context.  ", "How to quickly instantiate a Data Context": " title: How to quickly instantiate a Data Context tag: [how-to, setup] keywords: [Great Expectations, Data Context, Filesystem]  import TechnicalTag from '/docs/term_tags/_tag.mdx'; import Prerequisites from '/docs/components/_prerequisites.jsx'  import GxImport from '/docs/components/setup/python_environment/_gx_import.md'  import DataContextVerifyContents from '/docs/components/setup/data_context/_data_context_verify_contents.md' import AdmonitionConvertToFileContext from '/docs/components/setup/data_context/_admonition_convert_to_file_context.md' A Data Context contains the configurations for Expectations, Metadata Stores, Data Docs, Checkpoints, and all things related to working with Great Expectations.  This guide will demonstrate how to instantiate an existing Filesystem Data Context so that you can continue working with previously defined GX configurations. Prerequisites   Steps 1. Import Great Expectations  2. Run GX's get_context(...) method To quickly acquire a Data Context, we can use the get_context(...) method without any defined parameters. python title=\"Python code\" context = gx.get_context() This functions as a convenience method for initializing, instantiating, and returning a Data Context.  In the absence of parameters defining its behaviour, calling get_context() will return either a Cloud Data Context, a Filesystem Data Context, or an Ephemeral Data Context depending on what type of Data Context has previously been initialized with your GX install. If you have GX Cloud configured on your system, get_context() will instantiate and return a Cloud Data Context. Otherwise, get_context() will attempt to instantiate and return the last accessed Filesystem Data Context. Finally, if a previously initialized Filesystem Data Context cannot be found, get_context() will initialize, instantiate, and return a temporary in-memory Ephemeral Data Context. :::info Saving the contents of an Ephemeral Data Context for future use  ::: 3. Verify the content of the returned Data Context  Next steps For guidance on further customizing your Data Context's configurations for Metadata Stores and Data Docs, please see: - How to configure an Expectation Store on a filesystem - How to configure a Validation Result Store on a filesystem - How to configure and use a Metric Store - How to host and share Data Docs on a filesystem If you are content with the default configuration of your Data Context, you can move on to connecting GX to your source data: - How to configure a Pandas Data Source - How to configure a Spark Data Source - How to configure a SQL Data Source Additional information Related guides  To initialize and instantiate a temporary Data Context, see Instantiate an Ephemeral Data Context   ", "\"Instantiate a Data Context\"": " sidebar_label: \"Instantiate a Data Context\" title: \"Instantiate a Data Context\" id: instantiate_data_context description: Create and configure a Great Expectations Data Context. toc_min_heading_level: 2 toc_max_heading_level: 2  import TechnicalTag from '/docs/term_tags/_tag.mdx'; import Prerequisites from '/docs/components/_prerequisites.jsx' import GxImport from '/docs/components/setup/python_environment/_gx_import.md' import DataContextVerifyContents from '/docs/components/setup/data_context/_data_context_verify_contents.md' import AdmonitionConvertToFileContext from '/docs/components/setup/data_context/_admonition_convert_to_file_context.md' import TabItem from '@theme/TabItem'; import Tabs from '@theme/Tabs'; A Data Context contains the configurations for Expectations, Metadata Stores, Data Docs, Checkpoints, and all things related to working with Great Expectations (GX).  Use the information provided here to instantiate a Data Context so that you can continue working with previously defined GX configurations.   Existing Filesystem Instantiate an existing Filesystem Data Context so that you can continue working with previously defined GX configurations. Prerequisites   Import GX  Run the get_context(...) method To quickly acquire a Data Context, use the get_context(...) method without any defined parameters: python title=\"Python code\" context = gx.get_context() This functions as a convenience method for initializing, instantiating, and returning a Data Context.  In the absence of parameters defining its behavior, calling get_context() returns a Cloud Data Context, a Filesystem Data Context, or an Ephemeral Data Context depending on what type of Data Context has previously been initialized with your GX install. If you have GX Cloud configured on your system, get_context() instantiates and returns a Cloud Data Context. Otherwise, get_context()  instantiates and returns the last accessed Filesystem Data Context. If a previously initialized Filesystem Data Context cannot be found, get_context() initializes, instantiates, and returns a temporary in-memory Ephemeral Data Context. :::info Saving the contents of an Ephemeral Data Context for future use  ::: Verify Data Context content    Python A Data Context is required in almost all Python scripts utilizing GX. Use Python code to initialize, instantiate, and verify the contents of a Filesystem Data Context. Prerequisites   Import GX  Determine the folder to initialize the Data Context in Run the following command to initialize your Filesystem Data Context in an empty folder: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_initialize_a_filesystem_data_context_in_python.py path_to_empty_folder\" Create a context You provide the path for your empty folder to the GX library's FileDataContext.create(...) method as the project_root_dir parameter.  Because you are providing a path to an empty folder, FileDataContext.create(...) initializes a Filesystem Data Context in that location. For convenience, the FileDataContext.create(...) method instantiates and returns the newly initialized Data Context, which you can keep in a Python variable. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_initialize_a_filesystem_data_context_in_python.py initialize_filesystem_data_context\" :::info What if the folder is not empty? If the project_root_dir provided to the FileDataContext.create(...) method points to a folder that does not already have a Data Context present, the FileDataContext.create(...) method initializes a Filesystem Data Context in that location even if other files and folders are present.  This allows you to initialize a Filesystem Data Context in a folder that contains your source data or other project related contents. If a Data Context already exists in project_root_dir, the FileDataContext.create(...) method will not re-initialize it.  Instead, FileDataContext.create(...) instantiates and returns the existing Data Context. ::: Verify the Data Context content    Specific If you're using GX for multiple projects, you might want to use a different Data Context for each project. Instantiate a specific Filesystem Data Context so that you can switch between sets of previously defined GX configurations. Prerequisites   A previously initialized Filesystem Data Context. See How to initialize a Filesystem Data Context in Python.   Import GX  Specify a folder containing a previously initialized Filesystem Data Context Each Filesystem Data Context has a root folder in which it was initialized.  This root folder identifies the specific Filesystem Data Context to instantiate. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_instantiate_a_specific_filesystem_data_context.py path_to_context_root_folder\" Run the get_context(...) method You provide the path for your empty folder to the GX library's get_context(...) method as the context_root_dir parameter. Because you are providing a path to an empty folder, the get_context(...) method instantiates and return the Data Context at that location. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_instantiate_a_specific_filesystem_data_context.py get_filesystem_data_context\" :::info What if the folder does not contain a Data Context? If the context_root_dir provided to the get_context(...) method points to a folder that does not already have a Data Context, the get_context(...) method initializes a new Filesystem Data Context in that location. The get_context(...) method instantiates and returns the newly initialized Data Context. ::: Verify the Data Context content    Ephemeral An Ephemeral Data Context is a temporary, in-memory Data Context.  They are ideal for doing data exploration and initial analysis when you do not want to save anything to an existing project, or for when you need to work in a hosted environment such as an EMR Spark Cluster. An Ephemeral Data Context does not persist beyond the current Python session. To keep the contents of your Ephemeral Data Context for future use, see How to convert an Ephemeral Data Context to a Filesystem Data Context. Prerequisites   A Great Expectations instance. See Setup: Overview.    Import classes To create your Data Context, you'll create a configuration that uses in-memory Metadata Stores.    Run the following command to import the DataContextConfig and the InMemoryStoreBackendDefaults classes: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_explicitly_instantiate_an_ephemeral_data_context.py import_data_context_config_with_in_memory_store_backend\"   Run the following command to import the EphemeralDataContext class: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_explicitly_instantiate_an_ephemeral_data_context.py import_ephemeral_data_context\"   Create the Data Context configuration Run the following command to create a Data Context configuration that specifies the use of in-memory Metadata Stores and pass in an instance of the InMemoryStoreBackendDefaults class as a parameter when initializing an instance of the DataContextConfig class: python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_explicitly_instantiate_an_ephemeral_data_context.py instantiate_data_context_config_with_in_memory_store_backend\" Instantiate an Ephemeral Data Context Run the following command to initialize the EphemeralDataContext class while passing in the DataContextConfig instance you created as the value of the project_config parameter. python name=\"tests/integration/docusaurus/connecting_to_your_data/fluent_datasources/how_to_explicitly_instantiate_an_ephemeral_data_context.py instantiate_ephemeral_data_context\" :::info Saving the contents of an Ephemeral Data Context for future use  ::: Connect GX to source data systems Now that you have an Ephemeral Data Context you can connect GX to your data. See the following topics:    Next steps To customize a Data Context configuration for Metadata Stores and Data Docs, see: - How to configure an Expectation Store on a filesystem - How to configure a Validation Result Store on a filesystem - How to configure and use a Metric Store - How to host and share Data Docs on a filesystem To connecting GX to your source data: - How to configure a Pandas Data Source - How to configure a Spark Data Source - How to configure a SQL Data Source Related documentation  To initialize and instantiate a temporary Data Context, see Instantiate an Ephemeral Data Context ", "\"Connect to a source data system\"": " sidebar_label: \"Connect to a source data system\" title: \"Connect to a source data system\" id: connect_gx_source_data_system description: Install and configure Great Expectations to access data stored on Amazon S3, Google Cloud Storage, Microsoft Azure Blob Storage, and SQL databases. toc_min_heading_level: 2 toc_max_heading_level: 2  import TechnicalTag from '/docs/term_tags/_tag.mdx'; import Prerequisites from '/docs/components/_prerequisites.jsx' import PrereqInstalledAwsCli from '/docs/components/prerequisites/_aws_installed_the_aws_cli.mdx' import PrereqAwsConfiguredCredentials from '/docs/components/prerequisites/_aws_configured_your_credentials.mdx' import AwsVerifyInstallation from '/docs/components/setup/dependencies/_aws_verify_installation.md' import AwsVerifyCredentialsConfiguration from '/docs/components/setup/dependencies/_aws_verify_installation.md' import PythonCheckVersion from '/docs/components/setup/python_environment/_python_check_version.mdx' import PythonCreateVenv from '/docs/components/setup/python_environment/_python_create_venv.md' import TipPythonOrPython3Executable from '/docs/components/setup/python_environment/_tip_python_or_python3_executable.md' import S3InstallDependencies from '/docs/components/setup/dependencies/_s3_install_dependencies.md' import GxVerifyInstallation from '/docs/components/setup/_gx_verify_installation.md' import LinksAfterInstallingGx from '/docs/components/setup/next_steps/_links_after_installing_gx.md' import PrereqGcpServiceAccount from '/docs/components/prerequisites/_gcp_service_account.md' import GcpVerifyCredentials from '/docs/components/setup/dependencies/_gcp_verify_credentials_configuration.md' import GcpInstallDependencies from '/docs/components/setup/dependencies/_gcp_install_dependencies.md' import PrereqAbsConfiguredAnAbsAccount from '/docs/components/prerequisites/_abs_configured_an_azure_storage_account_and_kept_connection_string.md' import AbsInstallDependencies from '/docs/components/setup/dependencies/_abs_install_dependencies.md' import AbsConfigureCredentialsInDataContext from '/docs/components/setup/dependencies/_abs_configure_credentials_in_data_context.md' import AbsFurtherConfiguration from '/docs/components/setup/next_steps/_links_for_adding_azure_blob_storage_configurations_to_data_context.md' import InstallDependencies from '/docs/components/setup/dependencies/_sql_install_dependencies.mdx' import TabItem from '@theme/TabItem'; import Tabs from '@theme/Tabs'; This is where you'll find information about creating your Great Expectations (GX) Python environment, installing GX locally, and how to configure the dependencies necessary to access source data stored on Amazon S3, Google Cloud Storage (GCS), Microsoft Azure Blob Storage, or SQL databases. GX uses the term source data when referring to data in its original format, and the term source data system when referring to the storage location for source data.   Amazon S3 Create your GX Python environment, install Great Expectations locally, and then configure the necessary dependencies to access data stored on Amazon S3. Prerequisites   The ability to install Python modules with pip     Ensure your AWS CLI version is the most recent  Ensure your AWS credentials are correctly configured  Check your Python version   Create a Python virtual environment  Install GX with optional dependencies for S3  Verify the GX has been installed correctly  Next steps Now that you have installed GX with the necessary dependencies for working with S3, you are ready to initialize your Data Context.  The Data Context will contain your configurations for GX components, as well as provide you with access to GX's Python API.    Microsoft Azure Blob Storage Create your GX Python environment, install Great Expectations locally, and then configure the necessary dependencies to access data stored on Microsoft Azure Blob Storage. Prerequisites   The ability to install Python modules with pip    Check your Python version   Create a Python virtual environment  Install GX with optional dependencies for Azure Blob Storage  Verify that GX has been installed correctly  Configure the config_variables.yml file with your Azure Storage credentials  Next steps    GCS Create your GX Python environment, install Great Expectations locally, and then configure the necessary dependencies to access data stored on GCS. Prerequisites   The ability to install Python modules with pip    Ensure your GCP credentials are correctly configured  Check your Python version   Create a Python virtual environment  Install optional dependencies  Verify that GX has been installed correctly  Next steps Now that you have installed GX with the necessary dependencies for working with GCS, you are ready to initialize your Data Context.  The Data Context will contain your configurations for GX components, as well as provide you with access to GX's Python API.    SQL databases Create your GX Python environment, install Great Expectations locally, and then configure the necessary dependencies to access data stored on SQL databases. Prerequisites   The ability to install Python modules with pip   Check your Python version   Create a Python virtual environment  Install GX with optional dependencies for SQL databases  :::caution Additional dependencies for some SQL dialects The above pip instruction will install GX with basic SQL support through SqlAlchemy.  However, certain SQL dialects require additional dependencies.  Depending on the SQL database type you will be working with, you may wish to use one of the following installation commands, instead:  AWS Athena: pip install 'great_expectations[athena]' BigQuery: pip install 'great_expectations[bigquery]' MSSQL: pip install 'great_expectations[mssql]' PostgreSQL: pip install 'great_expectations[postgresql]' Redshift: pip install 'great_expectations[redshift]' Snowflake: pip install 'great_expectations[snowflake]' Trino: pip install 'great_expectations[trino]'  ::: Verify that GX has been installed correctly  Set up credentials Different SQL dialects have different requirements for connection strings and methods of configuring credentials.  By default, GX allows you to define credentials as environment variables or as values in your Data Context (once you have initialized one). There may also be third party utilities for setting up credentials of a given SQL database type.  For more information on setting up credentials for a given source database, please reference the official documentation for that SQL dialect as well as our guide on [how to set up credentials(/docs/guides/setup/configuring_data_contexts/how_to_configure_credentials). Next steps Now that you have installed GX with the necessary dependencies for working with SQL databases, you are ready to initialize your Data Context.  The Data Context will contain your configurations for GX components, as well as provide you with access to GX's Python API.   ", "How to set up GX to work with data in Azure Blob Storage": " title: How to set up GX to work with data in Azure Blob Storage tag: [how-to, setup] keywords: [Great Expectations, Data Context, Filesystem, ABS, Azure Blob Storage]  How to set up Great Expectations to work with data in Azure Blob Storage import TechnicalTag from '/docs/term_tags/_tag.mdx'; import Prerequisites from '/docs/components/_prerequisites.jsx'  import PrereqAbsConfiguredAnAbsAccount from '/docs/components/prerequisites/_abs_configured_an_azure_storage_account_and_kept_connection_string.md'  import PythonCheckVersion from '/docs/components/setup/python_environment/_python_check_version.mdx'  import PythonCreateVenv from '/docs/components/setup/python_environment/_python_create_venv.md' import TipPythonOrPython3Executable from '/docs/components/setup/python_environment/_tip_python_or_python3_executable.md'  import AbsInstallDependencies from '/docs/components/setup/dependencies/_abs_install_dependencies.md'  import GxVerifyInstallation from '/docs/components/setup/_gx_verify_installation.md'  import AbsConfigureCredentialsInDataContext from '/docs/components/setup/dependencies/_abs_configure_credentials_in_data_context.md'  import AbsFurtherConfiguration from '/docs/components/setup/next_steps/_links_for_adding_azure_blob_storage_configurations_to_data_context.md' This guide will walk you through best practices for creating your GX Python environment and demonstrate how to locally install Great Expectations along with the necessary dependencies for working with data stored in Azure Blob Storage. Prerequisites   The ability to install Python modules with pip    Steps 1. Check your Python version   2. Create a Python virtual environment  3. Install GX with optional dependencies for Azure Blob Storage  4. Verify that GX has been installed correctly  5. Configure the config_variables.yml file with your Azure Storage credentials  Next steps ", "How to set up GX to work with data on AWS S3": " title: How to set up GX to work with data on AWS S3 tag: [how-to, setup] keywords: [Great Expectations, Data Context, Filesystem, Amazon Web Services S3]  How to set up Great Expectations to work with data on Amazon Web Services S3 import TechnicalTag from '/docs/term_tags/_tag.mdx'; import Prerequisites from '/docs/components/_prerequisites.jsx'  import PrereqInstalledAwsCli from '/docs/components/prerequisites/_aws_installed_the_aws_cli.mdx' import PrereqAwsConfiguredCredentials from '/docs/components/prerequisites/_aws_configured_your_credentials.mdx'  import AwsVerifyInstallation from '/docs/components/setup/dependencies/_aws_verify_installation.md'  import AwsVerifyCredentialsConfiguration from '/docs/components/setup/dependencies/_aws_verify_installation.md'  import PythonCheckVersion from '/docs/components/setup/python_environment/_python_check_version.mdx'  import PythonCreateVenv from '/docs/components/setup/python_environment/_python_create_venv.md' import TipPythonOrPython3Executable from '/docs/components/setup/python_environment/_tip_python_or_python3_executable.md'  import S3InstallDependencies from '/docs/components/setup/dependencies/_s3_install_dependencies.md'  import GxVerifyInstallation from '/docs/components/setup/_gx_verify_installation.md'  import LinksAfterInstallingGx from '/docs/components/setup/next_steps/_links_after_installing_gx.md' This guide will walk you through best practices for creating your GX Python environment and demonstrate how to locally install Great Expectations along with the necessary dependencies for working with data stored in Amazon Web Services S3 storage. Prerequisites   The ability to install Python modules with pip     Steps 1. Ensure your AWS CLI version is the most recent  2. Ensure your AWS credentials are correctly configured  3. Check your Python version   4. Create a Python virtual environment  4. Install GX with optional dependencies for S3  5. Verify that GX has been installed correctly  Next steps Now that you have installed GX with the necessary dependencies for working with S3, you are ready to initialize your Data Context.  The Data Context will contain your configurations for GX components, as well as provide you with access to GX's Python API. ", "How to set up GX to work with data on GCS": " title: How to set up GX to work with data on GCS tag: [how-to, setup] keywords: [Great Expectations, Data Context, Filesystem, GCS, Google Cloud Storage]  How to set up Great Expectations to work with data on Google Cloud Storage import TechnicalTag from '/docs/term_tags/_tag.mdx'; import Prerequisites from '/docs/components/_prerequisites.jsx'  import PrereqGcpServiceAccount from '/docs/components/prerequisites/_gcp_service_account.md'  import GcpVerifyCredentials from '/docs/components/setup/dependencies/_gcp_verify_credentials_configuration.md'  import AwsVerifyCredentialsConfiguration from '/docs/components/setup/dependencies/_aws_verify_installation.md'  import PythonCheckVersion from '/docs/components/setup/python_environment/_python_check_version.mdx'  import PythonCreateVenv from '/docs/components/setup/python_environment/_python_create_venv.md' import TipPythonOrPython3Executable from '/docs/components/setup/python_environment/_tip_python_or_python3_executable.md'  import GcpInstallDependencies from '/docs/components/setup/dependencies/_gcp_install_dependencies.md'  import GxVerifyInstallation from '/docs/components/setup/_gx_verify_installation.md'  import LinksAfterInstallingGx from '/docs/components/setup/next_steps/_links_after_installing_gx.md' This guide will walk you through best practices for creating your GX Python environment and demonstrate how to locally install Great Expectations along with the necessary dependencies for working with data stored on Google Cloud Storage. Prerequisites   The ability to install Python modules with pip    Steps 1. Ensure your GCP credentials are correctly configured  2. Check your Python version   3. Create a Python virtual environment  4. Install GX with optional dependencies for GCS  5. Verify that GX has been installed correctly  Next steps Now that you have installed GX with the necessary dependencies for working with GCS, you are ready to initialize your Data Context.  The Data Context will contain your configurations for GX components, as well as provide you with access to GX's Python API. ", "Connect to a SQL database": " title: Connect to a SQL database tag: [how-to, setup] keywords: [Great Expectations, SQL]  How to set up Great Expectations to work with general SQL databases import TechnicalTag from '/docs/term_tags/_tag.mdx'; import Prerequisites from '/docs/components/_prerequisites.jsx'   import PythonCheckVersion from '/docs/components/setup/python_environment/_python_check_version.mdx'  import PythonCreateVenv from '/docs/components/setup/python_environment/_python_create_venv.md' import TipPythonOrPython3Executable from '/docs/components/setup/python_environment/_tip_python_or_python3_executable.md'  import InstallDependencies from '/docs/components/setup/dependencies/_sql_install_dependencies.mdx'  import GxVerifyInstallation from '/docs/components/setup/_gx_verify_installation.md'  import LinksAfterInstallingGx from '/docs/components/setup/next_steps/_links_after_installing_gx.md' This guide will walk you through best practices for creating your GX Python environment and demonstrate how to locally install Great Expectations along with the necessary dependencies for working with SQL databases. Prerequisites   The ability to install Python modules with pip   Steps 1. Check your Python version   2. Create a Python virtual environment  3. Install GX with optional dependencies for SQL databases  :::caution Additional dependencies for some SQL dialects The above pip instruction will install GX with basic SQL support through SqlAlchemy.  However, certain SQL dialects require additional dependencies.  Depending on the SQL database type you will be working with, you may wish to use one of the following installation commands, instead:  AWS Athena: pip install 'great_expectations[athena]' BigQuery: pip install 'great_expectations[bigquery]' MSSQL: pip install 'great_expectations[mssql]' PostgreSQL: pip install 'great_expectations[postgresql]' Redshift: pip install 'great_expectations[redshift]' Snowflake: pip install 'great_expectations[snowflake]' Trino: pip install 'great_expectations[trino]'  ::: 4. Verify that GX has been installed correctly  5. Setting up credentials Different SQL dialects have different requirements for connection strings and methods of configuring credentials.  By default, GX allows you to define credentials as environment variables or as values in your Data Context (once you have initialized one). There may also be third party utilities for setting up credentials of a given SQL database type.  For more information on setting up credentials for a given source database, please reference the official documentation for that SQL dialect as well as our guide on [how to set up credentials(/docs/guides/setup/configuring_data_contexts/how_to_configure_credentials). Next steps Now that you have installed GX with the necessary dependencies for working with SQL databases, you are ready to initialize your Data Context.  The Data Context will contain your configurations for GX components, as well as provide you with access to GX's Python API. "}