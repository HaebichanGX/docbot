{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655f34ae-5b9e-4f83-8376-5b9c571041e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa86349-e2df-4122-8d9c-db384c2f8cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run helper_functions.ipynb\n",
    "from variables import *\n",
    "\n",
    "all_file_mapping = process_md_files(DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3692604-d241-4efc-817a-b76f1e0c724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_databricks_data(cluster_id: str, query: str, profile: str) -> pd.DataFrame:\n",
    "    # Code for connecting to Databricks, pulling data    \n",
    "\n",
    "    CLUSTER_ID = cluster_id\n",
    "    \n",
    "    config = Config(profile = profile, cluster_id = cluster_id)\n",
    "    spark = SparkSession.builder.sdkConfig(config).getOrCreate()\n",
    "\n",
    "    df = spark.sql(query)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b02c81f-5517-44f7-995b-bf3375b0cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_save_databricks_data(spark_df: pd.DataFrame, filter_id_list: list, save_df:bool, save_path:str) -> None:\n",
    "    df = spark_df.toPandas()\n",
    "   \n",
    "    filtered_df = df[~df.id.isin(filter_id_list)]\n",
    "\n",
    "    filtered_df['rownum'] = filtered_df.groupby('text').cumcount() +1\n",
    "\n",
    "    filtered_df['reply_is_devrel'] = filtered_df['reply_is_devrel'].astype(bool)\n",
    "    filtered_df['reply_has_devrel_arrow_up'] = filtered_df['reply_has_devrel_arrow_up'].astype(bool)\n",
    "    \n",
    "    # even though we responded to it and likely gave them an answer, it took several discussions, which can mean it's a complicated question that we don't want the LLM to train on anyway\n",
    "    # this only gives us situations where we dev rel responded and we gave it a checkmark. Disregard every other messages that follow.\n",
    "    dev_rel_responded_multiple_times_id = filtered_df[(filtered_df.reply_is_devrel == True) & (filtered_df.rownum > 1)].id.unique()\n",
    "\n",
    "    filtered_more_df = filtered_df[~filtered_df.id.isin(dev_rel_responded_multiple_times_id)]\n",
    "\n",
    "    # then get rid of messages where they are written by community users and has not received up check mark, meaning useless text\n",
    "    final_df = filtered_more_df[~((filtered_more_df.reply_has_devrel_arrow_up == False) & (filtered_more_df.reply_is_devrel == False))]\n",
    "    \n",
    "    if save_df: \n",
    "        final_df.to_csv(save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb42aee-bd14-4c4b-9898-56a78c727a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_vectorstore(path) -> None:\n",
    "    document_objects = extract_document_objects(path, all_file_mapping)  \n",
    "    vectorstore = Chroma.from_documents(documents=document_objects, embedding=OpenAIEmbeddings(), persist_directory = \"./chroma_db\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e9c574-da26-4803-a95a-30d6b915992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_databricks_vectorstore_wrapper():\n",
    "    spark_df = get_databricks_data(cluster_id = CLUSTER_ID, query = query, profile = PROFILE)\n",
    "    filter_and_save_databricks_data(spark_df, filtered_id_list, save_df = True, save_path = DATABRICKS_SAVE_PATH)\n",
    "    create_new_vectorstore(DATABRICKS_SAVE_PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e22b5f2-4600-440a-b288-6c80e1b090f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to handle the question submission and display the results\n",
    "def on_question_submit(b):\n",
    "    question = text_box.value\n",
    "    score_threshold = float(slider.value)\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": score_threshold})\n",
    "\n",
    "    # Get the retrieved_docs\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    # Clear the previous output\n",
    "    output.clear_output()\n",
    "\n",
    "    # Check if there are relevant documents or not and display the output\n",
    "    with output:\n",
    "        if len(retrieved_docs) == 0:\n",
    "            print(\"No relevant documents found.\")\n",
    "        else:\n",
    "            print(\"Relevant Docs\\n\\n\")\n",
    "            for index, doc in enumerate(retrieved_docs):\n",
    "                index += 1\n",
    "                print(str(index) + \":\", doc.page_content)\n",
    "                print('\\n')\n",
    "                print('----------------------------------------------')\n",
    "                print('\\n')\n",
    "\n",
    "# Define the function to handle the \"Generate Response\" button click and display the result\n",
    "def generate_response(b):\n",
    "    question = text_box.value\n",
    "    score_threshold = float(slider.value)\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": score_threshold})\n",
    "\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    # Clear the previous output\n",
    "    output.clear_output()\n",
    "\n",
    "    # Check if there are relevant documents or not and display the output\n",
    "    with output:\n",
    "        if len(retrieved_docs) == 0:\n",
    "            print(f\"No relevant documents found with the query: {question}\")\n",
    "        else:        \n",
    "            # Create the ChatOpenAI model and the RetrievalQA chain\n",
    "            llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "            qa_chain = RetrievalQA.from_chain_type(llm, retriever= retriever, return_source_documents=True)    \n",
    "            result = qa_chain({\"query\": question})\n",
    "            # Display the result    \n",
    "            print(result['result'])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ac166b-c5c8-4ba2-8412-69b75dae3a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'HEROKU_APP_CONTEXT' in os.environ:\n",
    "\n",
    "    # Get the document_objects\n",
    "    document_objects = extract_document_objects(DATABRICKS_SAVE_PATH, all_file_mapping)\n",
    "\n",
    "    # Get the vectorstore saved previously\n",
    "    vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=OpenAIEmbeddings())\n",
    "    \n",
    "    # Create the widgets\n",
    "    text_box = widgets.Text(\n",
    "        description='Write your question:',\n",
    "        layout=widgets.Layout(width='50%'),\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    doc_display_text_box = widgets.Text(\n",
    "        value= str(len(document_objects)),\n",
    "        placeholder='Type something',\n",
    "        description='Number of Docs in Repository:',\n",
    "        disabled=True,\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    slider = widgets.FloatSlider(\n",
    "        value=0.7,\n",
    "        min=0.0,\n",
    "        max=1.0,\n",
    "        step=0.01,\n",
    "        description='Score Threshold:',\n",
    "        layout=widgets.Layout(width='50%'),\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    submit_button = widgets.Button(description='Source Document', )\n",
    "    submit_button.on_click(on_question_submit)\n",
    "    \n",
    "    generate_button = widgets.Button(description='Generate Response')\n",
    "    generate_button.on_click(generate_response)\n",
    "    \n",
    "    # Create an HBox to place the buttons side by side\n",
    "    buttons_box = widgets.HBox([submit_button, generate_button])\n",
    "    \n",
    "    # Create the output widget for displaying the results\n",
    "    output = widgets.Output(\n",
    "        layout=widgets.Layout(width='90%', height='300px', border='1px solid gray', overflow = 'auto')\n",
    "    )\n",
    "    \n",
    "    title_html = '<h1><b>GX DocBot (Beta)</b></h1>'\n",
    "    subtitle_html = '<h5>This application is for retrieving relevant docs related to question or generating a response.</h5>'\n",
    "    subtitle_html_2 = '<h5>The app utilizes our public-facing docs + Dev Rel slack Q and A data from slack community channel.</h5>'\n",
    "    subtitle_html_3 = '<h6>Score Threshold determines how relevant docs are to question. If question does not meet threshold, app will not return an answer.</h6>'\n",
    "    \n",
    "    title_widget = HTML(title_html)\n",
    "    subtitle_widget = HTML(subtitle_html)\n",
    "    subtitle_widget_2 = HTML(subtitle_html_2)\n",
    "    subtitle_widget_3 = HTML(subtitle_html_3)\n",
    "    \n",
    "    # Display the widgets and output\n",
    "    display(title_widget)\n",
    "    display(subtitle_widget)\n",
    "    display(subtitle_widget_2)\n",
    "    display(subtitle_widget_3)\n",
    "    display(HTML(\"<br>\"))\n",
    "    display(text_box)\n",
    "    display(slider)\n",
    "    display(doc_display_text_box)\n",
    "    display(buttons_box) \n",
    "    display(output)\n",
    "\n",
    "else:      \n",
    "    run_databricks_vectorstore_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0ce8dd-eca6-47d2-badc-ddbdebc3c083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f19a5f5-7bc9-49d7-9a2b-b766ce458472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "myenv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
